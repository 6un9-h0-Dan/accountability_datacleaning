---
title: "Wyoming Campaign Expenditures Data Diary"
subtitle: "Wyoming Expenditures July 31"
author: "Yanqi Xu"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  zipcode, # clean & database
  refinr, # cluster and merge
  knitr, # knit documents
  glue, # combine strings
  scales, #format strings
  here, # relative storage
  fs # search storage 
)
```

```{r fix_fun, echo=FALSE}
# fix conflict
here <- here::here
# custom utility functions
"%out%" <- Negate("%in%")
print_all <- function(df) df %>% print(n = nrow(.)) 
# source functions
source(here("R", "code", "normalize_geo.R"))
source(here("R", "code", "all_files_new.R"))
source(here("R", "code", "glimpse_fun.R"))
# load data
data("zipcode")
zipcode <-
  as_tibble(zipcode) %>% 
  select(city, state, zip) %>% 
  mutate(city = normalize_city(city))
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Describe _where_ the data is coming from. [Link to the data download][03] page if possible.

Describe the data set that is going to be cleaned. A file name, age, and unit of observation.

[03]: https://www.wycampaignfinance.gov/WYCFWebApplication/GSF_SystemConfiguration/SearchExpenditures.aspx "source"

### About

More information about the Wyoming Campaign Finance Information Systems can be found here https://www.wycampaignfinance.gov/WYCFWebApplication/Reports/FormationReportsViewer.aspx?docType=3.

### Variables

`variable_name`:

> Directly quote the definition given for variables of interest.

## Import

### Download

Download raw, **immutable** data file. Go to https://www.wycampaignfinance.gov/WYCFWebApplication/GSF_SystemConfiguration/SearchExpenditures.aspx, leave the fields blank, and click the "All" tab and hit "Search". After the table is populated, click "Export"

```{r raw_dir}
# create a directory for the raw data
raw_dir <- here("wy", "expends", "data", "raw")
dir_create(raw_dir)
```


### Read

```{r read_many, echo=FALSE}
wy <- 
  dir_ls(path = raw_dir) %>% 
  map(
    read_delim,
    delim = ",",
    escape_double = FALSE,
    escape_backslash = FALSE,
    col_types = cols(
      .default = col_character(),
      Date = col_date("%m/%d/%Y"),
      Amount = col_double()
    )  
    ) %>% 
  bind_rows() %>% 
  distinct() %>% 
  select(-starts_with("X")) %>% 
  clean_names() %>% 
  mutate_if(is_character, str_to_upper) %>% 
  map_if(is_character, str_replace_all, "\\\"", "\'") %>% 
  as_tibble()
```

## Explore

There are `nrow(wy)` records of `length(wy)` variables in the full database.

```{r glimpse}
head(wy)
tail(wy)
glimpse(wy)
```

### Distinct

The variables range in their degree of distinctness.


```{r n_distinct}
wy %>% glimpse_fun(n_distinct)
```

We can explore the distribution of the least distinct values with `ggplot2::geom_bar()`.

```{r plot_bar, echo=FALSE}
ggplot(data = wy) +
  geom_bar(aes(filer_type)) 
```

Or, filter the data and explore the most frequent discrete data.

```{r plot_bar2, echo=FALSE}
wy %>% 
  group_by(payee)  %>% 
  summarize(total_spent = sum(amount)) %>% 
  arrange(desc(total_spent)) %>% 
  head(10) %>% 
  ggplot(aes(x=payee, y=total_spent,)) + 
  geom_col() +
    labs(title = "Wyoming Campaign Expenditures",
       caption = "Source: Wyoming Secretary of State") +
  scale_y_continuous(labels = scales::dollar) +
  coord_flip() +
  theme_minimal()
```

### Missing

The variables also vary in their degree of values that are `NA` (missing).

```{r count_na}
wy %>% glimpse_fun(count_na)
```

We will flag any records with missing values in the key variables used to identify an expenditure.
There are `r sum(wy$na_flag)` columns in city_state_zip that are NAs
```{r na_flag}
wy <- wy %>% mutate(na_flag = is.na(city_state_zip))
```

### Duplicates
There are no duplicates
```{r get_dupes, collapse=TRUE}
wy_dupes <- get_dupes(wy)
```

### Ranges

#### Amounts

```{r}
summary(wy$amount)
```

See how the campaign expenditures were distributed

```{r}
wy %>% 
  ggplot(aes(x = amount)) + 
  geom_histogram() +
  scale_x_continuous(
    trans = "log10", labels = dollar)
```

Distribution of expenses by filer
```{r box_plot_by_type, echo=FALSE}

wy %>% 
  ggplot(
    mapping = aes(
      x = filer_type, 
      y = amount
    )
  ) +
  geom_boxplot(
    mapping  = aes(fill = filer_type), 
    varwidth = TRUE,
    outlier.alpha = 0.01
  ) +
  scale_fill_brewer(
    type    = "qual",
    palette = "Set1",
    guide   = FALSE
  ) +
  scale_y_continuous(
    trans = "log10",
    labels = dollar
  ) +
  theme(axis.text.x = element_text(angle = 15, hjust = 1)) +
  labs(
    title = "Wyoming Expenditure Amount Ranges",
    x     = "Expenditure Type",
    y     = "Amount",
    caption = "Source: Wyoming Secretary of State"
  )
```

### Dates
The dates seem to be reasonable, with records dating back to `r summary(wy$date)[1]` till `r summary(wy$date)[-1]` 
```{r}
summary(wy$date)
```

### Year

Add a `year` variable from `date` after `col_date()` using `lubridate::year()`.

```{r echo=FALSE}
is_even <- function(n){
  n %% 2 == 0
}
```

```{r add_year}
wy <- wy %>% mutate(year = year(date), on_year = is_even(year))
```

```{r year_count_bar, echo=FALSE}

wy %>% 
  count(on_year, year) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(aes(fill=on_year)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  labs(
    title = "Wyoming Expenditure Counts per Year",
    caption = "Source: Wyoming Secretary of State",
    x = "Year",
    y = "Count"
  )
  
```

```{r amount_year_bar, echo=FALSE}
wy %>% 
  group_by(year, on_year) %>% 
  summarize(mean = mean(amount)) %>% 
  ggplot(aes(x = year, y = mean)) +
  geom_col(aes(fill = on_year)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Wyoming Expenditure Mean Amount per Year",
    caption = "Source: Wyoming Secretary of State",
    x = "Year",
    y = "Amount"
  ) 
```

```{r amount_month_line}
wy %>% 
  mutate(month = month(date)) %>% 
  group_by(on_year, month) %>% 
  summarize(mean = mean(amount)) %>% 
  ggplot(aes(month, mean)) +
  geom_line(aes(color = on_year), size = 2) +
  scale_y_continuous(labels = dollar) +
  scale_x_continuous(labels = month.abb, breaks = 1:12) +
  scale_color_brewer(
    type = "qual",
    palette = "Dark2"
  ) +
  labs(
    title = "Wyoming Expenditure Amount by Month",
    caption = "Source: Wyoming Secretary of State",
    color = "Election Year",
    x = "Month",
    y = "Amount"
  )
```
## Wrangle
### Indexing
```{r}
wy <- tibble::rowid_to_column(wy, "id")
```

The lengths of city_state_zip column differ, and regular expressions can be used to separate the components.

The original data the city, state, and ZIP all in one column. The following code seperates them. 

### Zipcode
First, we'll extract any numbers whose lengths range from 1 to 5 and normalize them under "zip_clean". 

```{r}
wy <- wy %>% 
  mutate(
    zip_clean = city_state_zip %>% 
      str_extract("\\d{1,5}") %>% 
      normalize_zip(na_rep = TRUE))
sample(wy$zip_clean, 10)
```


### State

In this regex, state is considered to consist of two upper-case letters following a space, or two upper-case letters with a trailing space at the end.

```{r separate state}
wy <- wy %>% 
  mutate( state_clean =
            trimws(str_extract(wy$city_state_zip, "\\s([A-Z]{2})\\s|^([A-Z]{2})\\s$")))
count_na(wy$state_clean)
```

```{r normalize_state, collapse=TRUE}
wy <- wy %>% mutate(state_clean = normalize_state(state_clean))
```

### City

Cleaning city values is the most complicated. This process involves four steps:

1. Prepare raw city values by removing invalid data and reducing inconsistencies
1. Match prepared city values with the _actual_ city name of that record's ZIP code
1. swap prepared city values with the ZIP code match _if_ only 1 edit is needed
1. Refine swapped city values with key collision and n-gram fingerprints

#### Prep
``` {r}
wy <- wy %>% 
  mutate(
    city_clean = str_match(wy$city_state_zip,"(.+),")[,2]) 

wy <- wy %>% mutate(city_clean=ifelse(is.na(city_clean)==TRUE, 
               str_extract(city_state_zip, "^[A-Z]{2,}\\S$"),paste(city_clean)))

count_na(wy$city_clean)
## wy_ZIP <- wy %>% str_split(wy$city_state_zip, "\\s\\d{5}")
```

`r sum(!is.na(wy$city_clean))` cities were found.
```{r prep_city}
wy <- wy %>% mutate(city_prep = normalize_city(city_clean))
n_distinct(wy$city_clean)
```

#### Match

```{r match_dist}
wy <- wy %>%
  left_join(
    y = zipcode,
    by = c(
      "zip_clean" = "zip",
      "state_clean" = "state"
    )
  ) %>%
  rename(city_match = city) 

```

#### Swap

To replace city names with expected city names from zipcode when the two variables are no more than two characters different
```{r }

wy <- wy %>% 
  mutate(
    match_dist = stringdist(city_prep, city_match),
    city_swap = if_else(condition = is.na(city_match) == FALSE,
                        if_else(
      condition = match_dist <= 2,
      true = city_match,
      false = city_prep
    ),
      false = city_prep
  ))

summary(wy$match_dist)
sum(wy$match_dist == 1, na.rm = TRUE)
n_distinct(wy$city_swap)
```

#### Refine

```{r valid_city}
valid_city <- unique(zipcode$city)
```
Use the OpenRefine algorithms to cluster similar values and merge them together. This can be done using the refinr::key_collision_merge() and refinr::n_gram_merge() functions on our prepared and swapped city data.
```{r view_refine}
wy_refined <- wy %>%
  filter(match_dist != 1) %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge(dict = valid_city) %>% 
      n_gram_merge(numgram = 1),
    refined = (city_swap != city_refine)
  ) %>% 
  filter(refined) %>% 
  select(
    id,
    state_clean,
    zip_clean,
    city_clean,
    city_prep,
    city_match,
    match_dist,
    city_swap,
    city_refine,
  ) %>% 
  rename(
    state = state_clean,
    zip = zip_clean,
    city_raw = city_clean
  )

wy_refined %>% 
  count(city_swap, city_refine) %>% 
  arrange(desc(n))
```
##### Review Refined Cities


```{r}
refined_values <- unique(wy_refined$city_refine)
count_refined <- tibble(
  city_refine = refined_values, 
  refine_count = NA
)

for (i in seq_along(refined_values)) {
  count_refined$refine_count[i] <- sum(str_detect(wy$city_swap, refined_values[i]), na.rm = TRUE)
}

swap_values <- unique(wy_refined$city_swap)
count_swap <- tibble(
  city_swap = swap_values, 
  swap_count = NA
)

for (i in seq_along(swap_values)) {
  count_swap$swap_count[i] <- sum(str_detect(wy$city_swap, swap_values[i]), na.rm = TRUE)
}

wy_refined %>% 
  left_join(count_swap) %>% 
  left_join(count_refined) %>%
  select(
    id,
    city_match,
    city_swap,
    city_refine,
    swap_count,
    refine_count
  ) %>% 
  mutate(diff_count = refine_count - swap_count) %>%
  mutate(refine_dist = stringdist(city_swap, city_refine)) %>%
  distinct() %>%
  arrange(city_refine) %>% 
  print_all()
```

Manually change the city_refine fields due to overcorrection.


```{r}
wy_refined$city_refine <- wy_refined$city_refine %>% 
  str_replace("^SUNNYVALE$", "SUN VALLEY") %>% 
  str_replace("^DAVENPORT$", "PONTE VEDRA BEACH") %>% 
  str_replace("^AUBURN$", "CASPER") %>% 
  str_replace("^LAR$", "LARAMIE")

refined_table <- wy_refined %>% 
  select(id, city_refine)
```
  

#### Merge

```{r join_refine}
wy <- wy %>% 
  left_join(refined_table, by ="id") %>% 
  mutate(city = coalesce(city_refine, city_swap))
```

Each step of the cleaning process reduces the number of distinct city values.

## Conclude

1. There are `r nrow(wy)` records in the database
1. There are `r sum(wy$dupe_flag)` records with duplicate filer, recipient, date, _and_ amount
(flagged with `dupe_flag`)
1. The ranges for dates and amounts are reasonable
1. Consistency has been improved with `stringr` package and custom `normalize_*()` functions.
1. The five-digit `zip_clean` variable has been created with `zipcode::clean.zipcode()`
1. The `year` variable has been created with `lubridate::year()`
1. There are `r sum(is.na(wy$filer_name))` records with missing `name` values and `r sum(is.na(wy$date))`
records with missing `date` values (both flagged with the `na_flag`)

## Export

```{r write_clean}
clean_dir <- here("wy", "expends", "data", "processed")
dir_create(clean_dir)
wy %>% 
  select(
    -city_state_zip,
    -city_prep,
    -on_year,
    -city_match,
    -city_clean,
    -match_dist,
    -city_swap,
    -city_refine
  ) %>% 
  write_csv(
    path = glue("{clean_dir}/wy_expends_clean.csv"),
    na = ""
  )
```

