---
title: "Arkansas Contributions"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 3
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
set.seed(5)
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
fs::dir_create(here::here("ar", "contribs", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  magrittr, # pipe opperators
  janitor, # dataframe clean
  batman, # convert logical
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  rvest, # read html pages
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

The data is obtained from the [Arkansas Secretary of State][sos] in the form
of biannual CSV files.

> This page provides comma separated value (CSV) downloads of contribution,
expenditure, and loan data for each reporting year in a zipped file format.
These files can be downloaded and imported into other applications (Microsoft
Excel, Microsoft Access, etc.)
>
> This data is extracted from the Arkansas Campaign Finance database as it
existed as of 02/03/2020 10:37 AM.

[sos]: https://financial-disclosures.sos.arkansas.gov/index.html#/index
[key]: https://financial-disclosures.sos.arkansas.gov/CFISAR_Service/Template/KeyDownloads/Contributions%20and%20Loans%20File%20Layout%20Key.pdf"

## Import

### Download

```{r raw_dir}
raw_dir <- dir_create(here("ar", "contribs", "data", "raw"))
```

```{r raw_get, eval=FALSE}
# does not work
raw_url <- "https://financial-disclosures.sos.arkansas.gov/modules/partials/public/dataDownload.html"
raw_page <- httr::GET(raw_url, query = list(v = "20180912.0"))
raw_page$cookies %>% 
  read_html() %>% 
  html_node(".md-table") %>% 
  html_nodes("a") %>% 
  html_attr("href") %>% 
  download.file(destfile = path(raw_dir, basename(.)))
```

```{r raw_list}
raw_files <- dir_ls(raw_dir)
```

### Read

```{r read_raw}
arc <- map_df(
  raw_files,
  read_delim,
  delim = ",",
  escape_backslash = FALSE,
  escape_double = FALSE,
  col_types = cols(
    .default = col_character(),
    `Receipt Amount` = col_double(),
    `Receipt Date` = col_date("%m/%d/%Y  %H:%M:%S %p"),
    `Filed Date` = col_date("%m/%d/%Y  %H:%M:%S %p")
  )
)

arc <- arc %>% 
  remove_empty("cols") %>% 
  clean_names("snake") %>% 
  rename_all(str_remove, "^receipt_") %>% 
  rename_all(str_remove, "_name$") %>% 
  mutate_if(is_binary, to_logical)
```

## Explore

```{r glimpse}
head(arc)
tail(arc)
glimpse(sample_n(arc, 20))
```

### Missing

```{r glimpse_na}
col_stats(arc, count_na)
```

```{r flag_na}
arc <- arc %>% flag_na(last, date, amount)
arc$na_flag[arc$source_type == "Non-itemized"] <- FALSE
sum(arc$na_flag)
mean(arc$na_flag)
```

### Duplicates

```{r flag_dupes, warning=TRUE}
arc <- flag_dupes(arc, everything())
```

### Categorical

```{r glimpse_distinct}
col_stats(arc, n_distinct)
```

```{r source_bar}
explore_plot(
  data = filter(arc, !is.na(source_type)),
  var = source_type,
  title = "Arkansas Contribution Source Types",
)
```

```{r committee_bar}
explore_plot(
  data = filter(arc, !is.na(committee_type)),
  var = committee_type,
  title = "Arkansas Contribution Recipient Types",
)
```

### Amounts

The vast majority of contributions are less than $100 with nearly
`r percent(mean(arc$amount < 1))` being less than $1.

```{r summary_amount}
summary(arc$amount)
mean(arc$amount < 1)
```

```{r amount_histogram, echo=FALSE}
arc %>%
  ggplot(aes(amount)) +
  geom_histogram(fill = dark2["purple"]) +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "State Data Amount Distribution",
    subtitle = glue("from {min(arc$date)} to {max(arc$date)}"),
    caption = "Source: State Agency Name",
    x = "Amount",
    y = "Count"
  )
```

### Dates

```{r date_year}
arc <- mutate(arc, year = year(date))
```

```{r date_range}
min(arc$date)
max(arc$date)
sum(arc$date > today())
```

## Wrangle

### Address

```{r address_norm}
arc <- arc %>% 
  # combine street addr
  unite(
    col = address_full,
    starts_with("address"),
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  # normalize combined addr
  mutate(
    address_norm = normal_address(
      address = address_full,
      abbs = usps_street,
      na = invalid_city,
      na_rep = TRUE
    )
  ) %>% 
  select(-address_full)
```

```{r address_view}
arc %>% 
  select(contains("address")) %>% 
  distinct() %>% 
  sample_frac()
```

### ZIP

```{r zip_norm}
arc <- arc %>% 
  mutate(
    zip_norm = normal_zip(
      zip = zip,
      na_rep = TRUE
    )
  )
```

```{r zip_progress}
progress_table(
  arc$zip,
  arc$zip_norm,
  compare = valid_zip
)
```

### State

```{r state_norm}
arc <- arc %>% 
  mutate(
    state_norm = normal_state(
      state = state,
      abbreviate = TRUE,
      na_rep = TRUE,
      valid = valid_state
    )
  )
```

```{r state_view}
arc %>% 
  filter(state != state_norm) %>% 
  count(state, sort = TRUE)
```

```{r state_progress}
progress_table(
  arc$state,
  arc$state_norm,
  compare = valid_state
)
```

### City

The `campfin::normal_city()` function first forces consistent capitalization,
removes punctuation, and expands common abbreviations.

```{r city_norm}
arc <- arc %>% 
  mutate(
    city_norm = normal_city(
      city = city, 
      abbs = usps_city,
      states = c("AR", "DC", "ARKANSAS"),
      na = invalid_city,
      na_rep = TRUE
    )
  )
```

We can further reduce these inconsistencies by comparing our normalized value
to the _expected_ value for that record's (normalized) state and ZIP code. Using
`campfin::is_abbrev()` and `campfin::str_dist()`, we can test whether the
expected value is either an abbreviation for or within one character of our
normalized value.

```{r city_swap}
arc <- arc %>% 
  rename(city_raw = city) %>% 
  left_join(
    y = zipcodes,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(city_norm, city_match),
    match_dist = str_dist(city_norm, city_match),
    city_swap = if_else(
      condition = match_abb | match_dist == 1,
      true = city_match,
      false = city_norm
    )
  ) %>% 
  select(
    -city_match,
    -match_dist,
    -match_abb
  )
```

```{r city_progress, echo=FALSE}
many_city <- c(valid_city, extra_city)
progress <- progress_table(
  str_to_upper(arc$city_raw),
  arc$city_norm,
  arc$city_swap,
  compare = many_city
) %>% mutate(stage = as_factor(stage))
```

```{r progress_print, echo=FALSE}
kable(
  x = progress, 
  digits = 3,
  col.names = c("Stage", "Prop in", "N distinct", "Prop NA", "N out", "N diff")
)
```

```{r progress_bar, echo=FALSE}
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = RColorBrewer::brewer.pal(3, "Dark2")[3]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Michigan City Normalization Progress",
    x = "Stage",
    y = "Percent Valid"
  )
```

```{r distinct_bar, echo=FALSE}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Michigan City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Percent Valid",
    fill = "Valid"
  )
```

## Conclude

1. There are `r comma(nrow(arc))` records in the database.
1. There are `r sum(arc$dupe_flag)` duplicate records in the database.
1. The range of `date` is good, but a number of `amount` are less than $1.
1. There are `r sum(arc$na_flag)` records missing the contributor or date.
1. Consistency in goegraphic data has been improved with `campfin::normal_*()`.
1. The 5-digit `zip_norm` variable has been made with `campfin::normal_zip()`.
1. The 4-digit `year` variable has been made with `lubridate::year()`.

## Export

```{r clean_dir}
proc_dir <- dir_create(here("ar", "contribs", "data", "processed"))
```

```{r clean_rename}
arc <- arc %>% 
  select(
    -city_norm,
    city_norm = city_swap
  ) %>% 
  rename_all(
    str_replace, "norm", "clean"
  )
```

```{r clean_glimpse}
glimpse(arc)
```

```{r clean_write}
write_csv(
  x = arc,
  path = path(proc_dir, "ar_contribs_clean.csv"),
  na = ""
)
```

