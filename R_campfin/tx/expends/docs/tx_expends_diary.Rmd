---
title: "State Data"
author: "First Last"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
set.seed(seed = 05)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("kiernann/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  snakecase, # change string case
  RSelenium, # remote browser
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # text analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  batman, # rep(NA, 8) Batman!
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  httr, # http query
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Data is obtained from the [Texas Ethics Commission (TEC)][03]. According to [a TEC brochure][04],

> tatutory duties of the Ethics Commission are in Chapter 571 of the Government Code.  The agency
is responsible for administering these  laws:  (1)  Title  15,  Election  Code, concerning
political  contributions  and expenditures, and political advertising...

> The Ethics Commission serves as a repository of required disclosure statements for state
officials,  candidates,political  committees, lobbyists, and certain district and county judicial
officers.

Data is ontained from the [Campaign Finance section of the TEC website][05]. An entire database can
be downloaded as [a ZIP file][06]. The contents of that ZIP and the layout of the files within are
outlined in the [`CFS-ReadMe.txt` file][07].

> This zip package contains detailed information from campaign finance reports filed electronically
with the Texas Ethics Commission beginning July 1, 2000. Flat File Architecture Record Listing --
Generated 06/11/2016 12:38:08 PM

```{r read_key}
readme <- read_lines(file = "https://www.ethics.state.tx.us/data/search/cf/CFS-ReadMe.txt")
```

At the top of this file is a table of contents.

```{r print_key, results='asis', echo=FALSE}
readme[seq(13, 47, 2)][-2] %>% 
  str_trim() %>% 
  str_split(pattern = "\\s{2,}") %>%
  map(str_c, collapse = "|") %>%
  str_trim("both") %>% 
  str_c(collapse = "\n") %>%
  read_delim(delim = "|") %>%
  mutate(
    `File Name(s)` = glue("`{`File Name(s)`}`"),
    `File Contents` = str_trunc(`File Contents`, width = 30)
  ) %>% 
  kable("markdown")
```

From this table, we know the ExpendData record (`contribs_##.csv`) contains the data we want.

> Expenditures - Schedules F/G/H/I - Expenditures from special pre-election (formerly Telegram)
reports are stored in the file `expn_t`. They are kept separate from the expends file to avoid
creating duplicates, because they are supposed to be re-reported on the next regular campaign
finance report.

```{r print_expend_data, results='asis', echo=FALSE}
readme[497:535] %>%
  str_remove("#") %>% 
  str_trim() %>% 
  extract(c(-2, -35)) %>% 
  str_split(pattern = "\\s{2,}") %>%
  map(str_c, collapse = "|") %>%
  str_trim("both") %>% 
  str_c(collapse = "\n") %>%
  read_delim(delim = "|") %>% 
  separate(
    col = `Len Description`,
    sep = " ",
    into = c("Len", "Description"),
    extra = "merge",
    convert = TRUE
  ) %>%
  mutate_at(vars(1), str_remove_all, "\\d+\\s") %>% 
  mutate(`Field Name` = glue("`{`Field Name`}`")) %>% 
  mutate_all(str_replace_na) %>% 
  mutate_all(str_remove_all, "NA") %>% 
  kable("markdown")
```

The ExpendCategory record is a small table explaing the expenditure category codes used.

```{r print_expend_cats, results='asis', echo=FALSE}
readme[543:547][-2] %>% 
  str_remove("#") %>% 
  str_remove("\\d+") %>% 
  str_trim("both") %>% 
  str_split(pattern = "\\s{2,}") %>%
  map(str_c, collapse = "|") %>%
  str_trim("both") %>% 
  str_c(collapse = "\n") %>%
  read_delim(delim = "|") %>% 
  remove_empty("cols") %>% 
  separate(
    col = Mask,
    into = c("Len", "Description"),
    sep = " ",
    extra = "merge",
    convert = TRUE
  ) %>% 
  kable("markdown")
```

[03]: https://www.ethics.state.tx.us/search/cf/
[04]: https://www.ethics.state.tx.us/data/about/Bethic.pdf
[05]: https://www.ethics.state.tx.us/search/cf/
[06]: https://www.ethics.state.tx.us/data/search/cf/TEC_CF_CSV.zip
[07]: https://www.ethics.state.tx.us/data/search/cf/CFS-ReadMe.txt

### Download

```{r raw_paths}
raw_dir <- here("tx", "expends", "data", "raw")
dir_create(raw_dir)

zip_url <- "https://www.ethics.state.tx.us/data/search/cf/TEC_CF_CSV.zip"
zip_file <- str_c(raw_dir, basename(zip_url), sep = "/")
```

The ZIP file is fairly large, check the file size before downloading.

```{r head_raw, collapse=TRUE}
zip_head <- HEAD(zip_url)
zip_size <- as.numeric(headers(zip_head)["content-length"])
number_bytes(zip_size)
```

If the file hasn't been downloaded yet, do so now.

```{r download_raw}
if (!all_files_new(raw_dir, "*.zip$")) {
  download.file(
    url = zip_url, 
    destfile = zip_file
  )
}
```

### Unzip

There are `r nrow(unzip(zip_file, list = T))` CSV files inside the ZIP archive.

```{r list_zip, echo=FALSE}
zip_contents <- 
  unzip(zip_file, list = TRUE) %>% 
  as_tibble() %>% 
  clean_names() %>% 
  mutate(
    length = number_bytes(length),
    date = as_date(date)
  )

zip_contents %>% 
  filter(name %>% str_detect("exp"))

zip_expends <- zip_contents$name[str_which(zip_contents$name, "exp")]
```

If the files haven't been extracted, we can do so now.

```{r unzip_zip}
unzip(
  zipfile = zip_file,
  files = zip_expends,
  exdir = raw_dir
)
```

```{r rm_objects, echo=FALSE}
rm(zip_contents, readme, zip_expends, zip_size, zip_head)
```

### Read 

The TEC provides a helpful [record layout key][08] describing the structure of their flat files.
We can use the details in this key to properly read the files into R.

> The CSV file contains comma-delimited records –one line per record. Each record consists of
fields separated by commas.The following characters constitute the permitted list. The space
characterand commaarenotin this list. `! @ # $ % * -_ + : ; . / 0-9 A-Z a-z`

> If a raw data field contains any character other than these permitted characters, then the field
is surrounded by double-quotesin the CSV. Space is notin the above list–meaning that data
containing spaces will be double-quoted. Raw field data containing double-quotes will have doubled
double-quotes in the CSV encoding.In both raw dataand CSV encoding, new lines are represented with
the escape notation `\n`.

We can use this information as the arguments to `vroom::vroom()` and read all 8  files at once into
a single data frame.

[08]: https://www.ethics.state.tx.us/data/search/cf/CampaignFinanceCSVFileFormat.pdf

```{r read_csv, collapse=TRUE}
tx <- vroom(
  file = dir_ls(raw_dir, glob = "*\\d+.csv"),
  .name_repair = make_clean_names,
  na = c("", "NA", "N/A", "UNKNOWN"),
  delim = ",",
  col_names = TRUE,
  escape_double = TRUE,
  escape_backslash = FALSE,
  id = "file",
  locale = locale(tz = "US/Central"),
  col_types = cols(
    .default = col_character(),
    receivedDt = col_date("%Y%m%d"),
    expendDt = col_date("%Y%m%d"),
    expendAmount = col_double()
  )
)

tx <- tx %>%
  filter(expend_dt > "2008-01-01") %>% 
  mutate(
    capital_livingexp_flag = capital_livingexp_flag %>% 
      str_remove_all("X") %>%
      str_remove_all(",") %>%
      na_if("")
  ) %>% 
  # turn Y/N to T/F
  mutate_if(is_binary, to_logical) %>% 
  # shorten file var
  mutate(file = basename(file)) %>%
  # move to end of file
  select(everything(), file)
```

## Explore

```{r glimpse}
head(tx)
tail(tx)
glimpse(sample_n(tx, 10))
```

### Missing

```{r glimpse_na}
glimpse_fun(tx, count_na)
```

We can use `campfin::flag_na()` to create a new `na_flag` variable to identify any record missing
one of the values needed to identify the transaction.

```{r flag_na, collapse=TRUE}
tx <- tx %>%
  mutate(
    payee_name = coalesce(
      payee_name_last, 
      payee_name_organization
    )
  ) %>% 
  flag_na(
    filer_name,
    expend_dt,
    expend_amount,
    payee_name
  ) %>% 
  select(-payee_name)

sum(tx$na_flag)
mean(tx$na_flag)
```

### Duplicates

```{r flag_dupes, collapse=TRUE, eval=TRUE}
tx <- flag_dupes(tx, -expend_info_id)

sum(tx$dupe_flag)
mean(tx$dupe_flag)
```

### Categorical

```{r glimpse_distinct}
glimpse_fun(tx, n_distinct)
```

```{r filer_bar, echo=FALSE}
explore_plot(
  data = tx,
  var = filer_type_cd,
  palette = "Dark2",
  title = "Texas Expenditure Filer Types (Code)",
  subtitle = "from 2000 to 2019",
  caption = "Source: Texas Ethics Commission"
)
```

```{r category_bar, echo=FALSE}
explore_plot(
  data = drop_na(tx, expend_cat_cd),
  var = expend_cat_cd,
  palette = "Dark2",
  title = "Texas Expenditure Categories",
  subtitle = "from 2000 to 2019, without NA",
  caption = "Source: Texas Ethics Commission"
)
```

### Continuous

#### Amounts

```{r summary_amount}
summary(tx$expend_amount)
```

```{r amount_histogram, echo=FALSE}
tx %>%
  ggplot(aes(expend_amount)) +
  geom_histogram(bins = 30) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Texas Expenditure Amount Distribution",
    subtitle = "from 2000 to 2019",
    caption = "Source: Texas Ethics Commission"
  )
```

```{r amount_violin_what, echo=FALSE}
tx %>% 
  filter(
    expend_cat_cd %in% drop_na(count(tx, expend_cat_cd, sort = T))$expend_cat_cd[1:8],
    expend_amount > 1, 
    expend_amount < 1000000
  ) %>% 
  ggplot(
    mapping = aes(
      x = reorder(expend_cat_cd, X = expend_amount, FUN = median, na.rm = TRUE), 
      y = expend_amount
    )
  ) +
  geom_violin(
    draw_quantiles = c(0.25, 0.50, 0.75),
    scale = "width",
    trim = TRUE,
    na.rm = TRUE,
    mapping = aes(fill = expend_cat_cd)
  ) +
  scale_y_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  labs(
    title = "Texas Expenditure Amount Distribution",
    subtitle = "from 2000 to 2019, for most common categories",
    caption = "Source: Texas Ethics Commission",
    x = "",
    y = "Expenditure Amount"
  )
```

```{r amount_violin_who, echo=FALSE}
tx %>% 
  filter(
    filer_type_cd %in% drop_na(count(tx, filer_type_cd, sort = T))$filer_type_cd[1:8],
    expend_amount > 1, 
    expend_amount < 1000000
  ) %>% 
  ggplot(
    mapping = aes(
      x = reorder(filer_type_cd, X = expend_amount, FUN = median, na.rm = TRUE), 
      y = expend_amount
    )
  ) +
  geom_violin(
    draw_quantiles = c(0.25, 0.50, 0.75),
    scale = "width",
    trim = TRUE,
    na.rm = TRUE,
    mapping = aes(fill = filer_type_cd)
  ) +
  scale_y_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  labs(
    title = "Texas Expenditure Amount Distribution",
    subtitle = "from 2000 to 2019, for most filer types",
    caption = "Source: Texas Ethics Commission",
    x = "",
    y = "Expenditure Amount"
  )
```
  
#### Dates

To better explore and search the database, we will create a `expend_yr` variable from `expend_dt`
using `lubridate::year()`

```{r add_year}
tx <- mutate(tx, expend_yr = year(expend_dt))
```

The date range is fairly clean, with `r sum(tx$expend_dt > today(), na.rm = TRUE)` values after
`r today()` and only `r sum(tx$expend_yr < 2000, na.rm = TRUE)` before the year 2000.

```{r date_range, collapse=TRUE}
min(tx$expend_dt, na.rm = TRUE)
sum(tx$expend_yr < 2000, na.rm = TRUE)
max(tx$expend_dt, na.rm = TRUE)
sum(tx$expend_dt > today(), na.rm = TRUE)
```

We can see that the few expenditures in 1994 and 1999 seem to be outliers, with the vast majority
of expenditures coming from 2000 through 2019. We will flag these records.

```{r count_year}
count(tx, expend_yr, sort = FALSE)
```

```{r flag_dates, collapse=TRUE}
tx <- mutate(tx, date_flag = is_less_than(expend_yr, 2000))
sum(tx$date_flag, na.rm = TRUE)
```

## Wrangle

We can use the `campfin::normal_*()` functions to perform some basic and _confident_ programatic
text normalization to the geographic data for payees. This helps improve the searchability of the
database and more confidently links records.

### Address

```{r normal_address, collapse=TRUE, eval=TRUE}
# need version 0.8.99.9
packageVersion("tidyr")
tx <- tx %>% 
  # combine street addr
  unite(
    col = payee_street_addr_comb,
    starts_with("payee_street_addr"),
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  # normalize combined addr
  mutate(
    address_norm = normal_address(
      address = payee_street_addr_comb,
      add_abbs = NULL,
      na_rep = TRUE
    )
  )
```

```{r view_address_norm, eval=TRUE}
select(tx, payee_street_addr_comb, address_norm)
```

### ZIP

```{r count_zip_pre, collapse=TRUE}
n_distinct(tx$payee_street_postal_code)
prop_in(tx$payee_street_postal_code, geo$zip, na.rm = TRUE)
length(setdiff(tx$payee_street_postal_code, geo$zip))
```

```{r normal_zip}
tx <- tx %>% 
  mutate(
    zip_norm = normal_zip(
      zip = payee_street_postal_code,
      na_rep = TRUE
    )
  )
```

```{r count_zip_post, collapse=TRUE}
n_distinct(tx$zip_norm)
prop_in(tx$zip_norm, geo$zip, na.rm = TRUE)
length(setdiff(tx$zip_norm, geo$zip))
```

### State

```{r count_state_pre, collapse=TRUE}
n_distinct(tx$payee_street_state_cd)
prop_in(tx$payee_street_state_cd, geo$state, na.rm = TRUE)
length(setdiff(tx$payee_street_state_cd, geo$state))
```

```{r normal_state}
tx <- tx %>% 
  mutate(
    state_norm = normal_state(
      state = payee_street_state_cd,
      abbreviate = FALSE,
      na_rep = TRUE
    )
  )
```

```{r count_state_post, collapse=TRUE}
n_distinct(tx$state_norm)
prop_in(tx$state_norm, geo$state, na.rm = TRUE)
setdiff(tx$state_norm, geo$state)
tx$state_norm[which(tx$state_norm %out% geo$state)] <- NA
```

### City

```{r count_city_pre, collapse=TRUE}
n_distinct(tx$payee_street_city)
prop_in(str_to_upper(tx$payee_street_city), geo$city, na.rm = TRUE)
length(setdiff(str_to_upper(tx$payee_street_city), geo$city))
```

#### Normalize

```{r normal_city}
tx <- tx %>% 
  mutate(
    city_norm = normal_city(
      city = payee_street_city, 
      geo_abbs = usps_city,
      st_abbs = c("TX", "DC", "TEXAS"),
      na = na_city,
      na_rep = TRUE
    )
  )
```

```{r count_city_post_norm, collapse=TRUE}
n_distinct(tx$city_norm)
prop_in(tx$city_norm, geo$city, na.rm = TRUE)
length(setdiff(tx$city_norm, geo$city))
```

#### Swap

```{r swap_city}
tx <- tx %>% 
  left_join(
    y = geo,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_dist = stringdist(city_norm, city_match),
    city_swap = if_else(
      condition = equals(match_dist, 1),
      true = city_match,
      false = city_norm
    )
  )
```

```{r count_city_post_swap, collapse=TRUE}
n_distinct(tx$city_swap)
prop_in(tx$city_swap, geo$city, na.rm = TRUE)
length(setdiff(tx$city_swap, geo$city))
```

#### Refine

## Conclude

1. There are `r nrow(tx)` records in the database.
1. There are `r sum(tx$dupe_flag)` duplicate records in the database.
1. The range and distribution of `expend_amount` and `expend_dt` seem reasonable.
1. There are `r sum(tx$na_flag)` records missing either recipient or date.
1. Consistency in goegraphic data has been improved with `campfin::normal_*()`.
1. The 5-digit `zip_norm` variable has been created with `campfin::normal_zip()`.
1. The 4-digit `expend_yr` variable has been created with `lubridate::year()`.

## Export

```{r proc_dir}
proc_dir <- here("tx", "expends", "data", "processed")
dir_create(proc_dir)
```

```{r write_proc}
tx %>% 
  write_csv(
    path = glue("{proc_dir}/tx_expends_clean.csv"),
    na = ""
  )
```

