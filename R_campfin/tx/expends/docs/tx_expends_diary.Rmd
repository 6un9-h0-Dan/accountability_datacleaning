---
title: "State Data"
author: "First Last"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
set.seed(seed = 05)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("kiernann/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  snakecase, # change string case
  RSelenium, # remote browser
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # text analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  batman, # rep(NA, 8) Batman!
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  httr, # http query
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Data is obtained from the [Texas Ethics Commission (TEC)][03]. According to [a TEC brochure][04],

> tatutory duties of the Ethics Commission are in Chapter 571 of the Government Code.  The agency
is responsible for administering these  laws:  (1)  Title  15,  Election  Code, concerning
political  contributions  and expenditures, and political advertising...

> The Ethics Commission serves as a repository of required disclosure statements for state
officials,  candidates,political  committees, lobbyists, and certain district and county judicial
officers.

Data is ontained from the [Campaign Finance section of the TEC website][05]. An entire database can
be downloaded as [a ZIP file][06]. The contents of that ZIP and the layout of the files within are
outlined in the [`CFS-ReadMe.txt` file][07].

> This zip package contains detailed information from campaign finance reports filed electronically
with the Texas Ethics Commission beginning July 1, 2000. Flat File Architecture Record Listing --
Generated 06/11/2016 12:38:08 PM

```{r read_key}
readme <- read_lines(file = "https://www.ethics.state.tx.us/data/search/cf/CFS-ReadMe.txt")
```

At the top of this file is a table of contents.

```{r print_key, results='asis', echo=FALSE}
readme[seq(13, 47, 2)][-2] %>% 
  str_trim() %>% 
  str_split(pattern = "\\s{2,}") %>%
  map(str_c, collapse = "|") %>%
  str_trim("both") %>% 
  str_c(collapse = "\n") %>%
  read_delim(delim = "|") %>%
  mutate(
    `File Name(s)` = glue("`{`File Name(s)`}`"),
    `File Contents` = str_trunc(`File Contents`, width = 30)
  ) %>% 
  kable("markdown")
```

From this table, we know the ExpendData record (`contribs_##.csv`) contains the data we want.

> Expenditures - Schedules F/G/H/I - Expenditures from special pre-election (formerly Telegram)
reports are stored in the file `expn_t`. They are kept separate from the expends file to avoid
creating duplicates, because they are supposed to be re-reported on the next regular campaign
finance report.

```{r print_expend_data, results='asis', echo=FALSE}
readme[497:535] %>%
  str_remove("#") %>% 
  str_trim() %>% 
  extract(c(-2, -35)) %>% 
  str_split(pattern = "\\s{2,}") %>%
  map(str_c, collapse = "|") %>%
  str_trim("both") %>% 
  str_c(collapse = "\n") %>%
  read_delim(delim = "|") %>% 
  separate(
    col = `Len Description`,
    sep = " ",
    into = c("Len", "Description"),
    extra = "merge",
    convert = TRUE
  ) %>%
  mutate_at(vars(1), str_remove_all, "\\d+\\s") %>% 
  mutate(`Field Name` = glue("`{`Field Name`}`")) %>% 
  mutate_all(str_replace_na) %>% 
  mutate_all(str_remove_all, "NA") %>% 
  kable("markdown")
```

The ExpendCategory record is a small table explaing the expenditure category codes used.

```{r print_expend_cats, results='asis', echo=FALSE}
readme[543:547][-2] %>% 
  str_remove("#") %>% 
  str_remove("\\d+") %>% 
  str_trim("both") %>% 
  str_split(pattern = "\\s{2,}") %>%
  map(str_c, collapse = "|") %>%
  str_trim("both") %>% 
  str_c(collapse = "\n") %>%
  read_delim(delim = "|") %>% 
  remove_empty("cols") %>% 
  separate(
    col = Mask,
    into = c("Len", "Description"),
    sep = " ",
    extra = "merge",
    convert = TRUE
  ) %>% 
  kable("markdown")
```

[03]: https://www.ethics.state.tx.us/search/cf/
[04]: https://www.ethics.state.tx.us/data/about/Bethic.pdf
[05]: https://www.ethics.state.tx.us/search/cf/
[06]: https://www.ethics.state.tx.us/data/search/cf/TEC_CF_CSV.zip
[07]: https://www.ethics.state.tx.us/data/search/cf/CFS-ReadMe.txt

### Download

```{r raw_paths}
raw_dir <- here("tx", "expends", "data", "raw")
dir_create(raw_dir)
zip_url <- "https://www.ethics.state.tx.us/data/search/cf/TEC_CF_CSV.zip"
zip_file <- str_c(raw_dir, basename(zip_url), sep = "/")
```

The ZIP file is fairly large, check the file size before downloading.

```{r head_raw, collapse=TRUE}
zip_head <- HEAD(zip_url)
zip_size <- as.numeric(headers(zip_head)["content-length"])
number_bytes(zip_size)
```

If the file hasn't been downloaded yet, do so now.

```{r download_raw}
if (!all_files_new(raw_dir, "*.zip$")) {
  download.file(
    url = zip_url, 
    destfile = zip_file
  )
}
```

### Unzip

There are `r nrow(unzip(zip_file, list = T))` CSV files inside the ZIP archive.

```{r list_zip, echo=FALSE}
zip_contents <- 
  unzip(zip_file, list = TRUE) %>% 
  as_tibble() %>% 
  clean_names() %>% 
  mutate(
    length = number_bytes(length),
    date = as_date(date)
  )

zip_contents %>% 
  filter(name %>% str_detect("exp"))

zip_expends <- zip_contents$name[str_which(zip_contents$name, "exp")]
```

If the files haven't been extracted, we can do so now.

```{r unzip_zip}
if (!all_files_new(raw_dir, "exp*.csv$")) {
  unzip(
    zipfile = zip_file,
    files = zip_expends,
    exdir = raw_dir
  )
}
```

### Read 

The TEC provides a helpful [record layout key][08] describing the structure of their flat files.
We can use the details in this key to properly read the files into R.

> The CSV file contains comma-delimited records –one line per record. Each record consists of
fields separated by commas.The following characters constitute the permitted list. The space
characterand commaarenotin this list. `! @ # $ % * -_ + : ; . / 0-9 A-Z a-z`

> If a raw data field contains any character other than these permitted characters, then the field
is surrounded by double-quotesin the CSV. Space is notin the above list–meaning that data
containing spaces will be double-quoted. Raw field data containing double-quotes will have doubled
double-quotes in the CSV encoding.In both raw dataand CSV encoding, new lines are represented with
the escape notation `\n`.

[08]: https://www.ethics.state.tx.us/data/search/cf/CampaignFinanceCSVFileFormat.pdf

```{r read_csv, collapse=TRUE}
tx <- vroom(
  file = dir_ls(raw_dir, glob = "*0*.csv"),
  .name_repair = make_clean_names,
  na = c("", "NA", "N/A", "UNKNOWN"),
  delim = ",",
  col_names = TRUE,
  escape_double = TRUE,
  escape_backslash = TRUE,
  col_types = cols(
    .default = col_character(),
    receivedDt = col_date("%Y%m%d"),
    expendDt = col_date("%Y%m%d"),
    expendAmount = col_double()
  )
)

# 3,223,841
problems(tx)
```

## Explore

```{r glimpse}
head(tx)
tail(tx)
glimpse(sample_n(tx, 10))
```

