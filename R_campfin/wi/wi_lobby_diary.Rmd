---
title: "Data Diary"
subtitle: "Wisconsin Lobbying"
author: "Yanqi Xu"
date: "`r format(Sys.time())`"
output:
   github_document:    
    df_print: tibble
    # fig_caption: yes
    # highlight: tango
    # keep_md: yes
    # max.print: 32
    # toc: yes
    # toc_float: no
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  warning = FALSE
)
options(width = 99)
```

## Project


The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.


Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:


1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved


## Objectives


This document describes the process used to complete the following objectives:


1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction


## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  knitr, # knit documents
  glue, # combine strings
  scales, #format strings
  here, # relative storage
  fs, # search storage 
  vroom, #read deliminated files
  readxl #read excel files
)
```

```{r fix_fun, echo=FALSE, collapse = TRUE}
# fix conflict
here <- here::here
print_all <- function(df) df %>% print(n = nrow(.)) 
```
This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.


The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.


```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

## Download
[WIS. STAT. § 13.64] [05] regulates lobbyists registration as such.  

> “Lobbyist" means an individual who is employed by a principal, or contracts for or receives economic consideration, other than reimbursement for actual expenses, from a principal and whose duties include lobbying on behalf of the principal. If an individual's duties on behalf of a principal are not limited exclusively to lobbying, the individual is a lobbyist only if he or she makes lobbying communications on each of at least 5 days within a reporting period.An applicant for a license to act as a lobbyist may obtain an application from and file the application with the commission. The registration shall expire on December 31 of each even-numbered year. 

> Every principal who makes expenditures or incurs obligations in an aggregate amount exceeding $500 in a calendar year for the purpose of engaging in lobbying which is not exempt under s. 13.621 shall, within 10 days after exceeding $500, cause to be filed with the commission a registration statement specifying the principal's name, business address, the general areas of legislative and administrative action which the principal is attempting to influence, the names of any agencies in which the principal seeks to influence administrative action, and information sufficient to identify the nature and interest of the principal.The registration shall expire on December 31 of each even-numbered year.

According to[WIS. STAT. § 13.62(10)] [03],

> Attempting to influence legislative or administrative action;
• by oral or written communication;
• with any elective state official, agency official or legislative employee; and
• includes time spent in preparation for such communications and appearances at public hearings or
meetings or service on a committee in which such preparation or communication occurs. 
Does not include seeking a contract or grant, or quasi-judicial decisions.

[03]: http://docs.legis.wisconsin.gov/statutes/statutes/13/III/62/10

The [State of Wisconsin Ethics Commission][04] makes available directories of lobbyists, principals (otehrwise known as lobbying organizations, any person who employs a lobbyist) and state agency officials (legislative liaisons) that lobbying for their agency. The Accountability Project obtained the Wisconsin lobbying registration data from the Wisconsin Ethics Commission from 2013 to 2019. In an email communication, an ethics specialist with the commission wrote:
>The WithdrawnDate column exists if the lobbyists authorization was ever removed. A “null” entry here means that the authorization was active from the AuthorizedDate until the end of the session (December 31 of the even numbered year).

Set the download directory first.
```{r raw_dir}
# create a directory for the raw data
raw_dir <- here("wi", "lobbyists", "data", "raw")

principals <- file.path(raw_dir,"principals")
lobbyists <- file.path(raw_dir, "lobbyists")

dir_create(c(principals,lobbyists))
```

[04]:https://lobbying.wi.gov/Directories/DirectoryOfLicensedLobbyists/2019REG
[05]: https://docs.legis.wisconsin.gov/statutes/statutes/13/III/64

## Reading

```{r download directories}
wi_url_lobbyists <- glue("https://lobbying.wi.gov/Reports/Report.aspx?ReportPath=/GAB/Lobbying/Public&ReportName=DirectoryOfLicensedLobbyistsFlat&SessionID={(2013:2020)[c(TRUE,FALSE)]}REG&SessionStartYear={(2013:2020)[c(TRUE,FALSE)]}&SessionEndYear={(2013:2020)[c(FALSE,TRUE)]}&OutputFormat=Excel")

wi_url_principals <- glue("https://lobbying.wi.gov/Reports/Report.aspx?ReportPath=/GAB/Lobbying/Public&ReportName=DirectoryOfRegisteredLobbyingOrganizationsFlatter&SessionID={(2013:2020)[c(TRUE,FALSE)]}REG&SessionStartYear={(2013:2020)[c(TRUE,FALSE)]}&SessionEndYear={(2013:2020)[c(FALSE,TRUE)]}&OutputFormat=Excel")
  if (!all_files_new(lobbyists)) {
    download.file(
      #this step execute strings as vairables.
      url = wi_url_lobbyists,
      destfile = glue("{lobbyists}/wi_lobbyists_{(2013:2020)[c(TRUE,FALSE)]}.xls"))
  }
  if (!all_files_new(principals)) {
    download.file(
      #this step execute strings as vairables.
      url = wi_url_principals,
      destfile = glue("{principals}/wi_principals_{(2013:2020)[c(TRUE,FALSE)]}.xls"))
  }
```

We'll first merge each dataset in each folder into a master dataset and then combine into one the three master datasets, `wi_principals` for principals,  and `wi_lobbyists` for lobbyists.

The excel files contain some empty rows or text in some cells that are not part of the dataset itself. There are usually 3 top rows with table title and metadata, but the invalid rows at the bottom vary in lengths. We'll get rid of these columns by skipping the first three rows and requiring at least 3 non-na cells, `name`, `organization` and `licensed date` to be a valid registration entry. 

```{r read excel}
wi_lobbyists <- list.files(lobbyists, pattern = ".xls", full.names = TRUE) %>% 
  map_dfr(read_xls, skip = 3, .name_repair = make_clean_names, .id = "session_id") %>% 
  filter(rowSums(!is.na(.))>=3) %>% 
        # cols(.default = col_character(),
        #                licensed_date = col_date(format = "%m/%d/%Y"),
        #                surrendered_date = col_date(format = "%m/%d/%Y")
    mutate(surrendered_date = excel_numeric_to_date(surrendered_date,date_system = "modern"),
           licensed_date = as.Date(licensed_date, format = "%m/%d/%Y"),
           session_id = case_when(session_id == "1" ~ "2013 REGULAR SESSION",
                                  session_id == "2" ~ "2015 REGULAR SESSION",
                                  session_id == "3" ~ "2017 REGULAR SESSION",
                                  session_id == "4" ~ "2019 REGULAR SESSION")) %>% 
    mutate_if(is_character, str_to_upper)

wi_principals <- list.files(principals, pattern = ".xls", full.names = TRUE) %>% 
  map_dfr(read_xls, skip = 3, .name_repair = make_clean_names) %>% 
  filter(rowSums(!is.na(.))>=3) %>% 
  # There are some empty rows imported as x and x_2. They were merged cells from the excel that had no content
  select(-c(x, x_2)) %>% 
  mutate_if(is_character, str_to_upper)

wi_lobby <- read_xlsx(glue("{raw_dir}/wi_lobbyists.xlsx"), sheet = 1) %>% 
  clean_names() %>% 
  mutate_if(is_character, str_to_upper) %>% 
  mutate(authorized_date = as.numeric(authorized_date))
  
wi_lobby$withdrawn_date <- wi_lobby$withdrawn_date %>% 
  na_if("NULL") %>% 
  as.numeric()
  
wi_lobby <-  wi_lobby %>% mutate(authorized_date = excel_numeric_to_date(authorized_date, date_system = "modern")) %>% 
  mutate(withdrawn_date = excel_numeric_to_date(withdrawn_date, date_system = "modern"))

```

## Joining
```{r name}
wi_lobbyists <- wi_lobbyists %>% 
  mutate(first_name = {str_match(lobbyist_name, ",\\s([^,]{1,20})$")}[,2] %>% str_remove(" \\(.*\\)"),
        last_name = str_remove(lobbyist_name, first_name) %>% str_remove("\\(.*\\)") %>% str_remove(",") %>% trimws(),
        first_name =  first_name %>% str_remove("\\s.*"))

wi_principals <- wi_principals %>% 
  mutate(principal_session_id = str_replace(principal_session_id, "REG", " REGULAR SESSION"))
```

## Duplicates

We'll use the `flag_dupes()` function to see if there are records identical to one another and flag the duplicates. A new variable `dupe_flag` will be created.

```{r flag dupe}
wi_lobby <- flag_dupes(wi_lobby, dplyr::everything())
wi_lobbyists <- flag_dupes(wi_lobbyists, dplyr::everything())
wi_principals <- flag_dupes(wi_principals, dplyr::everything())
```

## Missing

```{r count_na}
wi_lobby %>% glimpse_fun(count_na)
wi_principals %>% glimpse_fun(count_na)
wi_lobbyists %>% glimpse_fun(count_na)
```
Few values are missing from the lobbyists database.



### Phone
```{r normal phone}
wi_lobby <- wi_lobby %>% 
  mutate_at(.vars = vars(starts_with('phone')),
            .funs = list(norm = normal_address),
            add_abbs = usps_street,
            na_rep = TRUE)

                                        
wi_principals <- wi_principals %>% 
  mutate_at(.vars = vars(starts_with('principal_phone')),
                                .funs = list(norm = normal_address),
                                add_abbs = usps_street,
                                na_rep = TRUE)
```


### Year 
```{r separate active start/end year}

```

### CityStateZip
Separate the address, city, state and zip columns.
```{r separate columns}
wi_lobbyists <- wi_lobbyists %>% 
    separate(city_state_zip, ", ", into = c("city_raw", "state_zip"), remove = FALSE) %>% 
    add_column(state = NA, .after = "city_raw") %>% 
    add_column(zip = NA, .after = "state") %>% 
    mutate(
         state = str_match(city_state_zip, ",\\s([A-Z]{2})\\s")[,2],
         zip = str_remove(state_zip, state) %>% normal_zip()) %>% 
    select(-state_zip)
```
### Address
```{r normal address, echo=FALSE}
wi_lobby <- wi_lobbyists %>% 
  mutate(lobbyist_address = normal_address(address = address,
      add_abbs = usps_city,
      na_rep = TRUE))

wi_principals <- wi_principals %>% 
  mutate(principal_address_norm = normal_address(address = principal_mailing_address,
      add_abbs = usps_city,
      na_rep = TRUE))
```

### ZIP 
Running the following commands tells us the zipcode fields are mostly clean.
```{r client normal zip}
prop_in(wi_lobbyists$zip, valid_zip, na.rm = TRUE) %>% percent()

wi_principals <- wi_principals %>% 
  mutate(principal_ZIP5 = normal_zip(principal_mailing_zip))

prop_in(wi_principals$principal_ZIP5, valid_zip, na.rm = TRUE) %>% percent()
```

### State
Running the following commands tells us the state fields are mostly clean.
```{r clients clean state}
prop_in(wi_lobbyists$state, valid_state, na.rm = TRUE) %>% percent()

prop_in(wi_principals$principal_mailing_state, valid_state, na.rm = TRUE) %>% percent()
```
### City
The city field needs some work. We'll use the three-step cleaning method.
```{r}
prop_in(wi_lobbyists$city_raw, valid_city, na.rm = TRUE) %>% percent()

prop_in(wi_principals$principal_mailing_city, valid_city, na.rm = TRUE) %>% percent()

```

#### Prep
```{r prep_city, collapse = TRUE}
wi_lobbyists <- wi_lobbyists %>% mutate(city_norm = normal_city(city = city_raw,
                                            geo_abbs = usps_city,
                                            st_abbs = c(valid_state),
                                            na = invalid_city,
                                            na_rep = TRUE))
n_distinct(wi_lobbyists$city_raw)
n_distinct(wi_lobbyists$city_norm)

prop_in(wi_lobbyists$city_raw, valid_city, na.rm = TRUE)
prop_in(wi_lobbyists$city_norm, valid_city, na.rm = TRUE)



wi_principals <- wi_principals %>% 
  mutate(principal_city_norm = normal_city(city = principal_mailing_city,
                                            geo_abbs = usps_city,
                                            st_abbs = c(valid_state),
                                            na = invalid_city,
                                            na_rep = TRUE))

n_distinct(wi_principals$principal_mailing_city)
n_distinct(wi_principals$principal_city_norm)
```

#### Swap
Then, we will compare these normalized `city_norm` values to the _expected_ city value for that
vendor's ZIP code. If the [levenshtein distance][09] is less than 3, we can confidently swap these
two values.

[09]: https://en.wikipedia.org/wiki/Levenshtein_distance

```{r swap_city}
wi_lobbyists <- wi_lobbyists %>% 
  left_join(
    y = zipcodes,
    by = c(
      "state" = "state",
      "zip" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_dist = stringdist(city_norm, city_match),
    city_swap = if_else(condition = !is.na(city_match),
                        if_else(
      condition = match_dist <= 2,
      true = city_match,
      false = city_norm
    ),
      false = city_norm
  ))

prop_in(wi_lobbyists$city_swap, valid_city, na.rm = TRUE)
```


```{r Google Maps Places API}
wi_lobbyists_out <- wi_lobbyists %>% 
  filter(city_swap %out% valid_city) %>% 
  drop_na(city_swap) %>% 
  select(city_raw,
         state,
         city_norm,
         city_match,
         city_swap) %>% 
  distinct()

wi_lobbyists_out <- wi_lobbyists_out %>% 
  mutate(test_col = pmap_lgl(.l = list(city_raw, state), .f = check_city, key = api_key))
# For the 


```

This is a very fast way to increase the valid proportion to
`r percent(prop_in(wi_lobby$city_swap, valid_city, na.rm = TRUE))` and reduce the number of distinct
_invalid_ values from `r length(setdiff(wi_lobby$city_norm, valid_city))` to only
`r length(setdiff(wi_lobby$city_swap, valid_city))`

## Export

```{r writeean}
clean_dir <- here("wi", "lobbying", "data", "processed")
dir_create(clean_dir)
wi_lobbyists %>% 
  write_csv(
    path = glue("{clean_dir}/wi_lobbyists.csv"),
    na = ""
  )
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

