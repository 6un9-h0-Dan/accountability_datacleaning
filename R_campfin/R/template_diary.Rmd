---
title: "Data Diary"
subtitle: "State Data"
author: "First Last"
date: "` format(Sys.time())`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = FALSE,
  echo = TRUE,
  dfrning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  lubridate, # datetime strings
  magrittr, # pipe opperators
  RSocrata, # read SODA APIs
  janitor, # dataframe clean
  zipcode, # clean & databse
  batman, # parse yes & no
  refinr, # cluster & merge
  rvest, # scrape website
  knitr, # knit documents
  here, # locate storage
  fs # search storage 
)
```

```{r fix_fun, echo=FALSE}
# fix conflict
here <- here::here
# custom utility functions
"%out%" <- Negate("%in%")
print_all <- function(df) df %>% print(n = nrow(.)) 
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory
of the more general, language-agnostic `irworkshop/accountability_datacleaning` 
[GitHub repository](https://github.com/irworkshop/accountability_datacleaning).

The `R_campfin` project uses the 
[RStudio projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)
feature and should be run as such. The project also uses the dynamic `here::here()` tool for file
paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

## Data

Describe _where_ the data is coming from (typically a state agency). 
[Link to the data download](https://example.com) page if possible.

Describe the data set that is going to be cleaned. A file name, age, and unit of observation.

### About

> If the publisher provides any information on the file, you can directly quote that here.

### Variables

Often the publisher will provide a dictionary to describe the variables in the data (and
potentially the key pairs between many relational tables). 
[Link to the dictionary](https://example.com).

`variable_name`:

> Directly quote the definition given for variables of interest.

## Download and Read

Describe the process used to download the file and read into into R as a data frame(s). The
advantage of using R and RMarkdown comes from the explict documentation of the steps taken to
obtain, load, and manipulate data. To that end, use R to explictly `download.file()` and `unzip()`
where needed. Pass filenames to `readr::read_delim()`, `RSocrata::read.socrata()`, or
`vroom::vroom()` to read data into R for processing.

```{r read_data}
if (file.exists(filename) & as_date(file.mtime(filename)) == today()) {
  df <- read_delim(
    file = filename, 
    delim = ",", 
    quote = "\"", 
    escape_backslash = FALSE,
    escape_double = TRUE, 
    col_names = TRUE, 
    col_types = NULL,
    locale = default_locale(), 
    na = c("", "NA"), 
    quoted_na = TRUE,
    comment = "", 
    trim_ws = FALSE, 
    skip = 0, 
    n_max = Inf,
    guess_max = min(1000, n_max), 
    progress = show_progress(),
    skip_empty_rows = TRUE)
}
```

Save a copy of the raw data in a `data/raw` directory. Not only does this safely backup the
original data, it increases the speed at which you can re-run data cleaning.

```{r write_raw}
dir_create(here("df_type", "data", "raw"))
write_delim(
  x = df,
  path = df_filename,
  delim = ",",
  na = "",
  col_names = TRUE,
  quote_escape = "double"
)
```

## Explore

In this section, we will explore the distribution of variables and the dimensions of the data
frame. We want to explore the distinctness of each discrete variable, the ranges continous
variables, any missing variables, and identify duplicate rows.

There are `nrow(df)` records of `length(df)` variables in the full database.

```{r glimpse}
sample_frac(df)
glimpse(sample_frac(df))
```

### Distinct

The variables range in their degree of distinctness.

```{r n_distinct}
df %>% 
  map(n_distinct) %>% 
  unlist() %>% 
  enframe(name = "variable", value = "n_distinct") %>% 
  mutate(prop_distinct = round(n_distinct / nrow(df), 4)) %>%
  print(n = length(df))
```

We can explore the distribution of the least distinct values with `ggplot2::geom_bar()`.

```{r plot_var1_bar, echo=FALSE}
ggplot(data = df) + 
  geom_bar(mapping = aes(var))
```

Or, filter the data and explore the most frequent discrete data.

```{r plot_var2_bar, echo=FALSE}
df %>% 
  count(var2) %>% 
  arrange(desc(n)) %>% 
  slice(1:10) %>% 
  ggplot(mapping = aes(x = var)) + 
  geom_bar(mapping = aes(color = var))
```

### Missing

The variables also vary in their degree of values that are `NA` (missing).

```{r count_na}
df %>% 
  map(function(var) sum(is.na(var))) %>% 
  unlist() %>% 
  enframe(name = "variable", value = "n_na") %>% 
  mutate(prop_na = n_na / nrow(df)) %>% 
  print(n = length(df))
```

We will flag any records with missing values in the key variables used to identify an expenditure.

```{r na_flag}
df <- df %>% 
  mutate(na_flag = is.na(date) | is.na(name))

df %>% 
  filter(na_flag) %>%
  sample_frac() %>% 
  select(
    na_flag,
    date,
    name,
    amount
    )
```

### Duplicates

Use `janitor::get_dupes()` to can create a table of records with duplicate filer, recipient,
date, _and_ amount values.

```{r get_dupes, collapse=TRUE}
df_dupes <- df %>% 
  get_dupes(
    name,
    date,
    amount
    ) %>% 
  mutate(dupe_flag = TRUE) %>%
  select(id, dupe_flag)

nrow(df_dupes)
```

Then join the table of duplicate records back onto the original dataset, with a new variable
marking each duplicate row.

```{r flag_dupes, message=FALSE, dfrning=FALSE}
df <- df %>% 
  left_join(df_dupes, by = "id") %>% 
  mutate(dupe_flag = !is.na(dupe_flag))
```

```{r count_dupes}
df %>% 
  filter(dupe_flag) %>% 
  select(
    id,
    name,
    date,
    amount
  )
```

Explore what kinds of records are being duplicated with a graph or table.

### Ranges

The range of continuous variables will need to be checked for data integrity. Typically this
includes `amount` and `date` at the very least.

We can check the distribution of continuous variables with `ggplot2::geom_histogram()`

#### Transaction Amounts

Below are the smallest and largest expenditures.

```{r glimpse_min_max}
glimpse(df %>% filter(amount == min(amount, na.rm = T)))
glimpse(df %>% filter(amount == max(amount, na.rm = T)))
```

### Transaction Dates

```{r date_future, collapse=TRUE}
max(df$date, na.rm = TRUE)
sum(df$date > today(), na.rm = T)
```

```{r date_past, collapse=TRUE}
min(df$date, na.rm = TRUE)
sum(year(df$date) < 2007, na.rm = T)
```

```{r plot_exp_year, echo=FALSE}
df %>% 
  count(year = year(date)) %>% 
  ggplot(aes(year, n)) +
  geom_col() +
  coord_cartesian(xlim = c(2000, 2020)) +
  geom_vline(xintercept = 2008)
```

To better track expenditures in the TAP database, we will create a `expenditure_year` variable from
the previously parsed `date` using `lubridate::year()`.

```{r add_year}
df <- df %>% mutate(year = year(date))
```

## Clean

### Address

The `ddress` variable should be minimally cleaned by removing punctuation and fixing white-space.

```{r}
df <- df %>% 
  mutate(
    address_clean = address %>% 
      str_to_upper() %>% 
      # remove punct and numbers
      str_replace("-", " ") %>% 
      str_remove_all("[:punct:]") %>% 
      str_trim() %>% 
      str_squish() %>% 
      na_if("") %>% 
      na_if("NA")
  )
```

### Zipcode

Use the `zipcodes::clean.zipcodes()` function to strips the ZIP+4 digits and adds leading zeroes to
three or four digit strings. We will also make some common invalid zips `NA`.

```{r zipcode_data, echo=FALSE}
data("zipcode")
source(here("R", "prep_city.R"))
zipcode <-
  as_tibble(zipcode) %>% 
  select(city, state, zip) %>% 
  mutate(city = prep_city(city))
```

```{r clean_zipcodes}
df <- df %>% 
  mutate(zip_clean = zip %>% 
           clean.zipcodes() %>% 
           na_if("00000") %>% 
           na_if("11111") %>% 
           na_if("99999")
  )

df$zip_clean[which(nchar(df$zip_clean) != 5)] <- NA
```

### State

Using comprehensive list of state abbreviations in the Zipcodes database, we can isolate invalid
`state` values and manually correct them.

```{r valid_state, collapse=TRUE}
valid_state <- c(unique(zipcode$state), "AB", "BC", "MB", "NB", "NL", "NS", "ON", "PE", "QC", "SK")
length(valid_state)
setdiff(valid_state, state.abb)
```

```{r view_states}
df %>% 
  filter(state %out% valid_state) %>% 
  filter(!is.na(state)) %>% 
  select(
    id,
    address,
    city,
    state,
    zip
  ) %>% 
  print_all()
```

If the `state` abbreviation is invalid, use the `city` and `zip` values for that record to manually
correct the abbreviation where possible. If it can't be manually corrected, make the value `NA`.

```{r}
df$state_clean <- df$state %>% 
  str_replace("DG", "DF") %>% # add match
  str_remove("[:punct:]") %>% 
  na_if("")   %>% # empty
  na_if("RE") %>% # requested
  na_if("99") %>% # df zip
```

### City

Cleaning city values is the most complicated. This process involves four steps:

1. Prepare raw city values by removing invalid data and reducing inconsistencies
1. Match prepared city values with the _actual_ city name of that record's ZIP code
1. Sdfp prepared city values with the ZIP code match _if_ only 1 edit is needed
1. Refine sdfpped city values with key collision and n-gram fingerprints

#### Prep City

The first step in cleaning city names is to reduce inconsistencies. The custom `city_prep()`
function found in the `/` sub-directory of this root project performs the bulk of our preparation.
In short, the function (1) removes punctuation, numbers, and state abbreviations, (2) expands
directional and geographic abbreviations, and (3) catches common `NA` strings.

```{r prep_city}
source(here("R", "prep_city.R"))
df <- df %>% 
  mutate(
    city_prep = prep_city(
      cities = city,
      na = read_lines(here("R", "na_city.csv")),
      abbs = c("DF", "OR", "ID", "DC", "BC")
    )
  )
```

#### Match and Sdfp City

The second step will be to compare the new `city_prep` value to the _actual_ city value for that
record's `zip_clean` value. If the `city_prep` is very similar to the expected city name for that
ZIP code, we can make that change.

```{r match_dist}
df <- df %>%
  left_join(zipcode, by = c("state_clean" = "state", "zip_clean" = "zip")) %>%
  rename(city_match = city) %>%
  mutate(
    match_dist  = stringdist(city_prep, city_match),
    city_sdfp = if_else(match_dist == 1, city_match, city_prep)
  )
```

#### Refine City

Now that we've prepared out city values and made the most obvious changes, we can use the
OpenRefine algorithms to cluster similar values and merge them together. This can be done using the
`efinr::key_collision_merge()` and `efinr::n_gram_merge()` functions on our prepared and sdfpped
city data.

```{r valid_city}
valid_city <- c(unique(zipcode$city), "COMMON SUBURB")
```

We will create a new table with these refined values.

```{r view_refine}
df_refined <- df %>%
  filter(state_clean == "df") %>% 
  filter(match_dist != 1) %>% 
  mutate(
    city_refine = city_sdfp %>% 
      key_collision_merge(dict = valid_city) %>% 
      n_gram_merge(numgram = 1),
    refined = (city_sdfp != city_refine)
  ) %>% 
  filter(refined) %>% 
  select(
    id,
    state_clean,
    zip_clean,
    city,
    city_prep,
    city_match,
    match_dist,
    city_sdfp,
    city_refine,
  ) %>% 
  rename(
    state = state_clean,
    zip = zip_clean,
    city_raw = city
  )
```

#### Review Refined City

```{r most_refined}
df_refined %>% 
  count(city_sdfp, city_refine) %>% 
  arrange(desc(n))
```

The key to the refine algorithms is clustering rare values with their more common similar values.
We can count how often the original value appears and compare it to the frequency of the refined
value.

```{r count_refined}
refined_values <- unique(df_refined$city_refine)
count_refined <- tibble(
  city_refine = refined_values, 
  refine_count = NA
)

for (i in seq_along(refined_values)) {
  count_refined$refine_count[i] <- sum(str_detect(df$city_sdfp, refined_values[i]), na.rm = TRUE)
}

sdfp_values <- unique(df_refined$city_sdfp)
count_sdfp <- tibble(
  city_sdfp = sdfp_values, 
  sdfp_count = NA
)

for (i in seq_along(sdfp_values)) {
  count_sdfp$sdfp_count[i] <- sum(str_detect(df$city_sdfp, sdfp_values[i]), na.rm = TRUE)
}
```

The least frequent refined values are the ones of which we should be most suspicious. The more
frequent a refined value appears compared to it's original value, the more confident the algorithm
can be in making the change. We should manually check any refined value with a small count.
Furthermore, some refined values appear _less_ often than their original value because their
fingerprint matches an entry in our dictionary (e.g., "Lake Forrest" vs "Forrest Lake").

```{r distinct_refined}
df_refined %>% 
  left_join(count_sdfp) %>% 
  left_join(count_refined) %>%
  select(
    city_match,
    city_sdfp,
    city_refine,
    sdfp_count,
    refine_count
  ) %>% 
  mutate(diff_count = refine_count - sdfp_count) %>%
  mutate(refine_dist = stringdist(city_sdfp, city_refine)) %>%
  distinct() %>%
  arrange(city_refine) %>% 
  print_all()
```

We can finally manually correct the last of these values.

```{r refine_fix}
df_refined$city_refine <- df_refined$city_refine %>% 
  str_replace("^BAD FIX$", "GOOD FIX") %>% 
  str_replace("^BAD FIX$", "ORIGINAL") %>% 
  na_if("INTERNET COMPANY")

refine_table <- df_refined %>% 
  select(id, city_refine)
```

#### Merge Refined City

Then, we match these refined values to the original data. Use the refined value where possible,
otherwise use the sdfpped city value (which is the prepared value or real value).

```{r join_refine}
df <- df %>% 
  left_join(refine_table, by = "id") %>% 
  mutate(city_clean = coalesce(city_refine, city_sdfp))
```

Each step of the cleaning process reduces the number of distinct city values.

```{r city_steps_distinct, collapse=TRUE}
n_distinct(df$city)
n_distinct(df$city_prep)
n_distinct(df$city_sdfp)
n_distinct(df$city_clean)
sum(df$city != df$city_clean, na.rm = TRUE)
sample(setdiff(df$city, df$city_clean), 25)
```

## Conclude

1. There are `nrow(df)` records in the database
1. There are `sum(df$dupe_flag)` records with duplicate filer, recipient, date, _and_ amount
(flagged with `dupe_flag`)
1. The ranges for dates and amounts are reasonable
1. Consistency in strings has been fixed with `city_prep()` and the `stringr` package
1. The five-digit `zip_clean` variable has been created with `zipcode::clean.zipcode()`
1. The `expenditure_year` variable has been created with `lubridate::year()`
1. There are ` sum(is.na(df$name))` records with missing `name` values and ` sum(is.na(df$date))`
records with missing `date` values (both flagged with the `na_flag`)

## Write

```{r write_clean}
df %>% 
  select(
    -zip,
    -state,
    -city,
    -city_prep,
    -city_match,
    -match_dist,
    -city_sdfp,
    -city_refine
  ) %>% 
  write_csv(
    path = here("df_expends", "data", "raw", "df_expends_clean.csv"),
    na = ""
  )
```

