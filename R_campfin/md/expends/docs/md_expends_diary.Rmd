---
title: "Maryland Expenditures"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = FALSE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  # it's nice to un-collapse df print
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  RSelenium, # remote browser
  tidyverse, # data manipulation
  lubridate, # datetime strings
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Data is obtained from the [Maryland Campaign Reporting Information System][03] (CRIS).

As explained by this [CRIS help page][04]:

> ## General Information on Expenditures and Outstanding Obligations
>
> An ***expenditure*** is defined as a gift, transfer, disbursement, or promise of money or
valuable thing by or on behalf of a political committee to promote or assist in promoting the
success or defeat of a candidate, political party, or question at  an election.
> 
> Expenditures must be election related; that is, they must enhance the candidates election
chances, such that they would not have been incurred if there had been no candidacy. Furthermore,
expenditures, including loans, may not be for the personal use of the candidate or any other
individual.
> 
> An outstanding obligation is any unpaid debt that the committee has incurred at the end of a
reporting period.

[03]: https://campaignfinance.maryland.gov/Home/Logout
[04]: https://campaignfinance.maryland.gov/home/viewpage?title=View%20Expenditures%20/%20Outstanding%20Obligations&link=Public/ViewExpenses

## Import

### Download

The data must be exported from the results page of an [expenditure search][05]. We can automate
this process using the RSelenium package.

```{r create_raw_dir}
raw_dir <- here("md", "expends", "data", "raw")
dir_create(raw_dir)
```

```{r download_raw, warning=FALSE, error=FALSE, message=FALSE, collapse=TRUE, eval=FALSE}
remote_driver <- rsDriver(browser = "firefox")
remote_browser <- remote_driver$client
remote_browser$navigate("https://campaignfinance.maryland.gov/Public/ViewExpenses")
remote_browser$findElement("css", "#dtStartDate")$sendKeysToElement(list("01/01/2008"))
end_date <- format(today(), "%m/%d/%Y")
remote_browser$findElement("css", "#dtEndDate")$sendKeysToElement(list(end_date))
remote_browser$findElement("css", "#btnSearch")$clickElement()
remote_browser$findElement("css", "a.t-button:nth-child(1)")$clickElement()
remote_driver$server$stop()
```

```{r find_raw_file}
raw_file <- dir_ls(raw_dir)
```

[05]: https://campaignfinance.maryland.gov/Public/ViewExpenses

### Read

```{r read_raw}
md <- read_delim(
  file = raw_file,
  delim = ",",
  escape_double = FALSE,
  escape_backslash = FALSE,
  col_types = cols(
    .default = col_character(),
    `Expenditure Date` = col_date_usa(),
    `Amount($)` = col_double()
  )
)
```

Our search result above indicated that 503,676 records were available for download, yet we were
only able to correctly read `r comma(nrow(md))`. From `readr::problems()` we can see the types
of issues found when reading the file.

```{r read_problems}
md_probs <- problems(md)
nrow(md_probs)
distinct(select(md_probs, -row, -file))
```

These issues almost assuredly stem from extraneous or erroneous commas or quotation marks. We are
going to remove rows with these problems (for now).

```{r filter_bad_rows}
# identify bad rows
bad_rows <- unique(md_probs$row)
# bad rows make up small part
percent(length(bad_rows)/nrow(md))
# filter out bad rows and cols
md <- md %>% 
  extract(-bad_rows, ) %>% 
  remove_empty("cols") %>% 
  clean_names("snake")
```

## Wrangle 

To better understand the database, we will first have to perform some rudimentary wrangling.

### Separate

The `address` feild contains many not only the street address, but also the city, state, and ZIP
code. Each element of the address is separated by _two_ space characters (`\\s`). We can use the
[`tidyr::separate()`][sep] function to split this single column into four new columns. There are a
few rows containing more than four instances of double spaces; The first half of the address is the
most troublesome, sometimes containing a name or additional address information. The city, state,
and ZIP code are always the last 3 elements, so we can set `extra = "merge"` and `fill = "left"`
to push ensure our data is pushed to the rightmost column.

```{r sep_address}
md %>% separate(
  col    = address,
  into   = c("address1", "address2", "city_sep", "state_zip"),
  sep    = "\\s{2}",
  remove = FALSE,
  extra  = "merge",
  fill   = "left"
) -> md
```

Since we `fill = "left"`, any record without a secondary address unit (e.g., Apartment number)
will have their primary street address shifted into the new rightmost `address2` column. We do not
need the complex street address separated, so we can use [`tidyr::unite()`][unite] to combine them
into a single `address_sep` column.

```{r unite_address}
md %>% unite(
  address1, address2,
  col    = "address_sep",
  sep    = " ",
  remove = TRUE,
  na.rm  = TRUE
) -> md
```

Unlike the rest of the original `address` string, the state and ZIP code are separated by only _one_
space character. We can again use `tidyr::separate()`, this time only splitting the two elements
by the single space preceding the fist digit (`\\d`) of the ZIP code.

```{r separate_state_zip}
md %>% separate(
  col    = state_zip,
  into   = c("state_sep", "zip_sep"),
  sep    = "\\s{1,}(?=\\d)",
  remove = TRUE,
  extra  = "merge"
) -> md
```

Below, you can see how this makes wrangling these individual components much easier. Now we can
more easily search the database for a certain state or ZIP code.

```{r show_sep, echo=FALSE}
ex_sep <- sample_n(md, 10)
select(ex_sep, address)
select(ex_sep, ends_with("sep"))
rm(ex_sep)
```

[sep]: https://tidyr.tidyverse.org/reference/separate.html
[unite]: https://tidyr.tidyverse.org/reference/unite.html

### Normalize

Now that each geographic element in a separate column, we can use the `campfin::normal_*()` 
functions to improve the searchability of the database.

#### ZIP

Before each normalization, we should check the quality of our existing data so we have a benchmark
to compare against.

```{r zip_pre}
n_distinct(md$zip_sep)
prop_in(md$zip_sep, valid_zip)
prop_na(md$zip_sep)
count_out(md$zip_sep, valid_zip)
length(setdiff(md$zip_sep, valid_zip))
```

The `campfin::normal_zip()` function is used to form a valid 5-digit US ZIP code from messy data.
It removes the unnecessary ZIP+4 suffix, pads the left of short strings with zeroes, and removes
any values which are a single repeating digit (e.g., 00000, XXXXX).

```{r normal_zip}
md <- mutate(md, zip_norm = normal_zip(zip_sep, na_rep = TRUE))
```

```{r zip_post}
n_distinct(md$zip_norm)
prop_in(md$zip_norm, valid_zip)
prop_na(md$zip_norm)
count_out(md$zip_norm, valid_zip, na.rm = TRUE)
length(setdiff(md$zip_norm, valid_zip))
```

This process puts our percentage of valid `zip_norm` values well over 99%.

```{r zip_progress}
progress_table(
  raw = md$zip_sep,
  norm = md$zip_norm,
  compare = valid_zip
)
```

#### Address

We can perform similar normalization on the `address_sep` variable, although it's impractical to
compare these values against a fixed set of known valid addresses. The primary function of
`campfin::normal_address()` is to replace all USPS abbreviations with their full string equivalent.
This improves consistency in our database and makes addresses more searchable.

```{r normal_address}
md <- md %>% 
  mutate(
    address_norm = normal_address(
      address = address_sep,
      add_abbs = usps_street,
      na_rep = TRUE
    )
  )
```

```{r show_address_norm, echo=FALSE}
ex_add <- sample_n(md, 10)
select(ex_add, address_sep)
select(ex_add, address_norm)
rm(ex_add)
```

#### State

```{r state_pre}
n_distinct(md$state_sep)
prop_in(md$state_sep, valid_state)
prop_na(md$state_sep)
count_out(md$state_sep, valid_state)
length(setdiff(md$state_sep, valid_state))
```

The `campfin::normal_state()` function aims to form a valid 2-letter state abbreviation from our
`state_sep` variable.

```{r normal_state}
md <- md %>% 
  mutate(
    state_norm =
      normal_state(
      state = state_sep,
      abbreviate = TRUE,
      na_rep = TRUE
    )
  )
```

There are still a fair number of invalid `state_norm` values that we can try and fix more manually,
although they account for less than `r percent(prop_out(md$state_norm, valid_state))` of the total
values.

```{r state_post_norm}
n_distinct(md$state_norm)
prop_in(md$state_norm, valid_state)
prop_na(md$state_norm)
count_out(md$state_norm, valid_state)
length(setdiff(md$state_norm, valid_state))
```

We will create a list of all `state_norm` values not found in our comprehensive `valid_state` 
vector. Many of these are longer strings which contain a valid abbreviation or name.

```{r manual_abbrev_check}
bad_states <- md$state_norm[which(md$state_norm %out% valid_state)]
count_vec(bad_states)
```

We can abbreviate these full strings and attempt to extract a valid abbreviation from the end of
the invalid string.

```{r abbrev_bad_states}
bad_states <- abbrev_full(bad_states, full = valid_name, rep = valid_state)
fixed_states <- if_else(
  condition = str_extract(bad_states, "\\b[:upper:]{2}$") %in% valid_state,
  true = str_extract(bad_states, "\\b[:upper:]{2}$"),
  false = bad_states
)
```

```{r view_bad_state_fixes, echo=FALSE}
tibble(
  orig = md$state_norm[which(md$state_norm %out% valid_state)],
  new = bad_states
) %>% 
  mutate(fixed = new %in% valid_state) %>%
  count(orig, new, fixed, sort = TRUE)
```

```{r}
md <- mutate(md, state_fixed = state_norm)
md$state_fixed[which(md$state_fixed %out% valid_state)] <- fixed_states
```

```{r state_post}
n_distinct(md$state_norm)
prop_in(md$state_norm, valid_state)
prop_na(md$state_norm)
count_out(md$state_norm, valid_state)
length(setdiff(md$state_norm, valid_state))
md$state_norm[which(md$state_norm %out% valid_state)] <- NA
```

#### City

```{r city_pre}
n_distinct(md$city_sep)
prop_in(md$city_sep, valid_city)
prop_in(str_to_upper(md$city_sep), valid_city)
prop_na(md$city_sep)
count_out(md$city_sep, valid_city)
length(setdiff(md$city_sep, valid_city))
```

##### Normalize

```{r normal_city}
md <- md %>% 
  mutate(
    city_norm = normal_city(
      city = city_sep,
      geo_abbs = usps_city,
      st_abbs = c("MD", "DC", "MARYLAND"),
      na = c(invalid_city, ""),
      na_rep = TRUE
    )
  )
```

```{r city_post_norm}
n_distinct(md$city_norm)
prop_in(md$city_norm, valid_city)
prop_in(str_to_upper(md$city_norm), valid_city)
prop_na(md$city_norm)
count_out(md$city_norm, valid_city)
length(setdiff(md$city_norm, valid_city))
```

##### Match

```{r match_city}
md <- md %>% 
  left_join(
    y = zipcodes,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city)
```

```{r prop_match_na}
prop_na(md$city_norm)
prop_na(md$city_match)
```

##### Compare

```{r compare_city}
md <- md %>% 
  mutate(
    match_dist = str_dist(city_norm, city_match),
    match_abb = is_abbrev(city_norm, city_match)
  )
```

```{r compare_stats}
summary(md$match_dist)
sum(md$match_abb, na.rm = TRUE)
```

```{r view_match_abbs}
md %>% 
  filter(match_abb) %>% 
  count(city_norm, city_match, sort = TRUE)
```

```{r view_match_dist}
md %>% 
  filter(match_dist == 1) %>% 
  count(city_norm, city_match, sort = TRUE)
```

##### Swap

```{r swap_city}
md <- md %>% 
  mutate(
    city_swap = if_else(
      condition = match_abb | match_dist == 1,
      true = city_match,
      false = city_norm
    )
  )
```

```{r city_post_swap}
n_distinct(md$city_swap)
prop_in(md$city_swap, valid_city)
prop_in(str_to_upper(md$city_swap), valid_city)
prop_na(md$city_swap)
count_out(md$city_swap, valid_city)
length(setdiff(md$city_swap, valid_city))
```

##### Refine

```{r refine_city}
good_refine <- md %>% 
  filter(state_norm == "MD") %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>%
  filter(city_refine != city_swap) %>% 
  inner_join(
    y = zipcodes,
    by = c(
      "city_refine" = "city",
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  )
```

```{r count_refine}
good_refine %>% 
  count(city_swap, city_refine, sort = TRUE)
```

```{r join_refine}
md <- md %>% 
  left_join(good_refine) %>% 
  mutate(city_refine = coalesce(city_refine, city_swap))
```

##### Evaluate

We can make very few manual changes to capture the last few big invalid values. Local city
abbreviations (e.g., SPFD) often need to be changed by hand.

```{r view_final_bad}
md %>%
  filter(city_refine %out% valid_city) %>% 
  count(state_norm, city_refine, sort = TRUE) %>% 
  drop_na(city_refine)
```

```{r progress_table, echo=FALSE}
progress_table <- tibble(
  stage = c("raw", "norm", "swap", "refine"),
  prop_good = c(
    prop_in(str_to_upper(md$city_sep), valid_city, na.rm = TRUE),
    prop_in(md$city_norm, valid_city, na.rm = TRUE),
    prop_in(md$city_swap, valid_city, na.rm = TRUE),
    prop_in(md$city_refine, valid_city, na.rm = TRUE)
  ),
  total_distinct = c(
    n_distinct(str_to_upper(md$city_sep)),
    n_distinct(md$city_norm),
    n_distinct(md$city_swap),
    n_distinct(md$city_refine)
  ),
  unique_bad = c(
    length(setdiff(str_to_upper(md$city_sep), valid_city)),
    length(setdiff(md$city_norm, valid_city)),
    length(setdiff(md$city_swap, valid_city)),
    length(setdiff(md$city_refine, valid_city))
  ),
  prop_na = c(
    prop_na(md$city_sep),
    prop_na(md$city_norm),
    prop_na(md$city_swap),
    prop_na(md$city_refine)
  )
)

diff_change <- progress_table$unique_bad[4]-progress_table$unique_bad[1]
prop_change <- diff_change/progress_table$unique_bad[1]
```

Still, our progress is significant without having to make a single manual or unconfident change.
The percent of valid cities increased from `r percent(progress_table$prop_good[1])` to 
`r percent(progress_table$prop_good[4])`. The number of total distinct city values decreased from
`r comma(progress_table$total_distinct[1])` to `r comma(progress_table$total_distinct[4])`. The
number of distinct invalid city names decreased from `r comma(progress_table$unique_bad[1])` to
only `r comma(progress_table$unique_bad[4])`, a change of `r percent(prop_change)`.

```{r print_progress, echo=FALSE}
progress_table %>% 
  mutate_at(vars(2, 5), scales::percent) %>% 
  kable(
    format = "markdown", 
    digits = 4,
    col.names = c(
      "Normalization Stage", 
      "Percent Valid",
      "Total Distinct", 
      "Unique Invalid", 
      "Prop NA"
    )
  )
```

```{r wrangle_bar_prop, echo=FALSE}
brewer_dark2 <- RColorBrewer::brewer.pal(8, "Dark2")
progress_table %>% 
  mutate(stage = as_factor(stage)) %>% 
  ggplot(aes(x = stage, y = prop_good)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = brewer_dark2[2]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Missouri Expenditures Payee City Progress",
    subtitle = "Percent of total values contained in pre-defined list of cities",
    caption = "Source: Missouri Ethics Commission",
    x = "Wrangling Stage",
    y = "Proportion Valid Cities"
  )
```

```{r wrangle_bar_distinct, echo=FALSE}
progress_table %>% 
  mutate(stage = as_factor(stage)) %>% 
  select(-prop_good, -prop_na) %>% 
  rename(
    All = total_distinct,
    Invalid = unique_bad
  ) %>% 
  gather(
    -stage,
    key = "key",
    value = "value"
  ) %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = key)) +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Missouri Expenditures Payee City Progress",
    subtitle = "Total distinct number of city values",
    caption = "Source: Missouri Ethics Commission",
    fill = "Distinct Values",
    x = "Wrangling Stage",
    y = "Number of Expenditures"
  )
```

## Explore

Now that the data is sufficiently wrangled and normalized, we should explore it
a little for consistency issues.

```{r glimpse}
head(md)
tail(md)
glimpse(sample_frac(md))
```

### Missing

There are a number of records missing one of the four key variables we need to
identify a unique transaction.

```{r glimpse_na}
glimpse_fun(md, count_na)
```

We will flag these records with a new `na_flag` variable using the
`campfin::flag_na()` function.

```{r flag_na}
md <- md %>% flag_na(expenditure_date, amount, payee_name, committee_name)
sum(md$na_flag)
mean(md$na_flag)
```

### Duplicates

There are also a number of records that are complete duplicates of another row.
It's possible that a campaign made multiple valid expenditures for the same
amount, to the same vendor, on the same day. However, we will flag these two
variables just to be safe. We can create a new `dupe_flag` variable using the
`campfin::flag_dupes()` function.

```{r flag_dupes}
md <- flag_dupes(md, everything())
sum(md$dupe_flag)
mean(md$dupe_flag)
```

### Categorical

```{r glimpse_distinct}
glimpse_fun(md, n_distinct)
```

```{r payee_type_bar, echo=FALSE}
explore_plot(
  data = drop_na(md, payee_type),
  var = payee_type,
  title = "Maryland Expenditure Count by Payee Type",
  caption = "Source: Maryland Campaign Reporting Information System",
  x = "Amount",
  y = "Count"
)
```

```{r expense_category_bar, echo=FALSE}
explore_plot(
  data = drop_na(md, expense_category),
  var = expense_category,
  title = "Maryland Expenditure Count by Expense Category",
  caption = "Source: Maryland Campaign Reporting Information System",
  x = "Amount",
  y = "Count"
) + theme(axis.text.x = element_text(angle = 10, vjust = 1, hjust = 1))
```

```{r expense_method_bar, echo=FALSE}
explore_plot(
  data = drop_na(md, expense_method),
  var = expense_method,
  title = "Maryland Expenditure Count by Expense Methods",
  caption = "Source: Maryland Campaign Reporting Information System",
  x = "Amount",
  y = "Count"
)
```

### Continuous

#### Amounts

```{r summary_amount}
summary(md$amount)
sum(md$amount <= 0)
sum(md$amount > 1000000)
```

```{r amount_histogram, echo=FALSE}
md %>%
  ggplot(aes(amount)) +
  geom_histogram(fill = brewer_dark2[1]) +
  geom_vline(xintercept = median(md$amount)) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Maryland Expenditure Amount Distribution",
    caption = "Source: Maryland Campaign Reporting Information System",
    x = "Amount",
    y = "Count"
  )
```

```{r}

```

#### Dates

```{r}
md <- mutate(md, year = year(date))
```

```{r date_range, collapse=TRUE}
min(md$date)
sum(md$year < 2000)
max(md$date)
sum(md$date > today())
```

## Conclude

1. There are `r nrow(md)` records in the database.
1. There are `r sum(md$dupe_flag)` duplicate records in the database.
1. The range and distribution of `amount` seems reasonable, and `date` has been cleaned by removing
`r sum(md$date_flag, na.rm = T)` values from the distance past or future.
1. There are `r sum(md$na_flag)` records missing either recipient or date.
1. Consistency in geographic data has been improved with `campfin::normal_*()`.
1. The 5-digit `zip_norm` variable has been created with `campfin::normal_zip()`.
1. The 4-digit `year_clean` variable has been created with `lubridate::year()`.

## Export

```{r create_proc_dir}
proc_dir <- here("in", "expends", "data", "processed")
dir_create(proc_dir)
```

```{r write_clean}
mo %>% 
  select(
    -city_norm,
    -city_swap,
    -city_match,
    -city_swap,
    -match_dist,
    -city_refine,
    -year
  ) %>% 
  write_csv(
    path = glue("{proc_dir}/mo_expends_clean.csv"),
    na = ""
  )
```
