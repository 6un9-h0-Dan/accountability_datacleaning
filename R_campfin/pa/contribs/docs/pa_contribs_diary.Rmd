---
title: "Data Diary"
subtitle: "Pennsylvania Contribution"
author: "Yanqi Xu"
date: "`r format(Sys.time())`"
output:
  html_document: 
    df_print: tibble
    fig_caption: yes
    highlight: tango
    keep_md: yes
    max.print: 32
    toc: yes
    toc_float: no
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  warning = FALSE
)
options(width = 99)
```


## Project


The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.


Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:


1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved


## Objectives


This document describes the process used to complete the following objectives:


1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction


## Packages


The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.


```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("kiernann/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  zipcode, # clean & database
  refinr, # cluster and merge
  knitr, # knit documents
  glue, # combine strings
  scales, #format strings
  here, # relative storage
  fs, # search storage 
  vroom #read deliminated files
)
```


```{r fix_fun, echo=FALSE, collapse = TRUE}
# fix conflict
here <- here::here
# custom utility functions
"%out%" <- Negate("%in%")
print_all <- function(df) df %>% print(n = nrow(.)) 
# load data
zipcode <- geo
```


This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.


The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.


```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```


[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"


## Import


Link to download:
[03]: https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx "source"


### Download


Download raw, **immutable** data file. Go to https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx. We'll download the files from 2015 to 2019 (file format: zip file) with the script.


```{r raw_dir}
# create a directory for the raw data
raw_dir <- here("pa", "contribs", "data", "raw")
dir_create(raw_dir)
```


Download all the file packages containing all campaign-finance-related files. 
```{r download to raw_dir, eval = FALSE}
#download the files into the directory
pa_exp_urls <- glue("https://www.dos.pa.gov//VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/{2015:2019}.zip")


if (!all_files_new(raw_dir)) {
  for (url in pa_exp_urls) {
    download.file(
      url = url,
      destfile = glue("{raw_dir}/{basename(url)}")
    )
  }
}
```


### Read
Read individual csv files from the downloaded zip files
```{r read_many, eval = FALSE}


zip_files <- dir_ls(raw_dir, glob = "*.zip")


if (all_files_new(path = raw_dir, glob = "*.txt")) {
  for (i in seq_along(zip_files)) {
    unzip(
      zipfile = zip_files[i],
      #Matches the csv files that starts with expense, and trim the "./ " from directory names
      files = grep("contrib.+", unzip(zip_files[i]), value = TRUE) %>% substring(3,),
      exdir = raw_dir
    )
  }
}
```
Read multiple csvs into R
```{r read multiple files}
#recursive set to true because 2016 and 2015 have subdirectories under "raw"
contrib_files <- list.files(raw_dir, pattern = ".txt", recursive = TRUE, full.names = TRUE)
#pa_lines <- list.files(raw_dir, pattern = ".txt", recursive = TRUE) %>% map(read_lines) %>% unlist()
pa_col_names <- c("FILERID", "EYEAR", "CYCLE", "SECTION", "CONTRIBUTOR", "ADDRESS1", "ADDRESS2", "CITY", "STATE", "ZIPCODE", "OCCUPATION", "ENAME", "EADDRESS1", "EADDRESS2", "ECITY", "ESTATE", "EZIPCODE", "CONTDATE1", "CONTAMT1", "CONTDATE2", "CONTAMT2", "CONTDATE3", "CONTAMT3", "CONTDESC")

pa <- contrib_files %>% 
  map(read_delim, delim = ",", escape_double = FALSE,
      escape_backslash = FALSE, col_names = pa_col_names, 
      col_types = cols(.default = col_character(),
                     CONTAMT1 = col_double())) %>% bind_rows()
```

### About

More information about the record layout can be found here https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/readme.txt.

### Encoding
```{r}
for (i in c(5:10)) {
  pa[[i]] <- iconv(pa[[i]], 'UTF-8', 'ASCII') %>% 
    toupper() %>% 
   str_replace("&AMP;", "&") 
}
```

Some columns are not read properly due to extraneous and unescaped commas or double quotes. The following lines of code fix that. 
### Indexing
```{r}
pa <- tibble::rowid_to_column(pa, "index")
```

### Repositioning
```{r misplaced columns}
to_reposit <- which(pa$CONTDATE2 != 0 & is.na(pa$CONTDATE2) == FALSE )

nudge <- function(df, index, cut_off_col) {
  for (i in index){
    col_posit <- match(cut_off_col, colnames(pa))
    df[i, col_posit] <- str_c(df[i, col_posit], df[i, col_posit+1], sep = " ")
      for (col in col_posit:ncol(df)-1) {
        df[i, col] <- df[i, col+1]
      } 
  }
  return(df)
}

pa <- nudge(pa, to_reposit[1:length(to_reposit)-1], "ENAME")
pa <- nudge(pa, to_reposit[length(to_reposit)], "CONTRIBUTOR")

```

## Explore

There are `nrow(pa)` records of `length(pa)` variables in the full database.

```{r glimpse}
head(pa)
tail(pa)
glimpse(pa)
```

### Distinct


The variables range in their degree of distinctness.


```{r n_distinct}
pa %>% glimpse_fun(n_distinct)
```


### Missing


The variables also vary in their degree of values that are `NA` (missing).


```{r count_na}
pa %>% glimpse_fun(count_na)
```


We will flag any records with missing values in the key variables used to identify an expenditure.
There are `r sum(pa$na_flag)` records missing `CONTRIBUTOR`, `CONTAMT1` AND `CONTDATE1`
```{r na_flag}
pa <- pa %>% flag_na(CONTRIBUTOR, CONTAMT1, CONTDATE1)
```


### Duplicates


```{r flag_dupes, eval = TRUE, collapse=TRUE}
pa <- flag_dupes(pa, dplyr::everything())
sum(pa$dupe_flag)
```

### Ranges

#### Amounts

```{r, collapse = TRUE}
summary(pa$CONTRIBUTOR)
sum(pa$CONTAMT1 < 0 , na.rm = TRUE)
```


See how the campaign expenditures were distributed


```{r amount distribution, eval = TRUE}
pa %>% 
  ggplot(aes(x = EXPAMT)) + 
  geom_histogram() +
  scale_x_continuous(
    trans = "log10", labels = dollar)
```

### Dates

```{r convert to date}
pa$CONTDATE1 <- as.Date(pa$CONTDATE1, "%Y%m%d")
```
Records in the PA contribution datasets date back to `r min(pa$CONTDATE1, na.rm = T)` till `r max(pa$CONTDATE1, na.rm = T)` 
```{r}
summary(pa$CONTDATE1)
```


### Year


Add a `year` variable from `date` after `col_date()` using `lubridate::year()`.


```{r add_year}
pa <- pa %>% mutate(year = year(CONTDATE1), on_year = is_even(year))
```


```{r year_count_bar, eval = TRUE, echo=FALSE}


pa %>% 
  filter( 2014 < year & year < 2020) %>% 
  count(on_year, year) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(aes(fill=on_year)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  labs(
    title = "Pennsylvania Contributions Counts per Year",
    caption = "Source: Pennsylvania Dept. of State",
    x = "Year",
    y = "Count"
  )
  
```


## Wrangle
The state column is now pretty clean, as all non-NA columns have two characters.


### Zipcode
The Zipcode column can range from 1 to 13 columns.


```{r collapse = TRUE}
table(nchar(pa$ZIPCODE))
```




```{r normalize zip, collapse = TRUE}
pa <- pa %>% 
  mutate(
    zip_clean = ZIPCODE %>% 
      normal_zip(na_rep = TRUE))
sample(pa$zip_clean, 10)
```




### State
View values in the STATE field is not a valid state abbreviation
```{r fix state abbrev, collapse = TRUE}
{pa$STATE[pa$STATE %out% zipcode$state]}[!is.na(pa$STATE[pa$STATE %out% zipcode$state])]
# Fix the entry where the state was read as "CHADDS FORD"
nudge <- which(nchar(pa$STATE)>2)
  for (column in c(9:17)){
    pa[[ nudge, column]] <- pa[[nudge, column+1]]
  }
```
These are expenditures in Canada, which we can leave in. 
### City


Cleaning city values is the most complicated. This process involves four steps:


1. Prepare raw city values by removing invalid data and reducing inconsistencies
1. Match prepared city values with the _actual_ city name of that record's ZIP code
1. swap prepared city values with the ZIP code match _if_ only 1 edit is needed
1. Refine swapped city values with key collision and n-gram fingerprints


#### Prep


`r sum(!is.na(pa$CITY))` distinct cities were in the original dataset in column 
```{r prep_city, collapse = TRUE}
pa <- pa %>% mutate(city_prep = normal_city(CITY))
n_distinct(pa$city_prep)
```


#### Match


```{r match_dist}
pa <- pa %>%
  left_join(
    y = zipcode,
    by = c(
      "zip_clean" = "zip",
      "STATE" = "state"
    )
  ) %>% 
  rename(city_match = city)
```


#### Swap


To replace city names with expected city names from zipcode when the two variables are no more than two characters different
```{r }
pa <- pa %>% 
  mutate(
    match_dist = stringdist(city_prep, city_match),
    city_swap = if_else(condition = is.na(city_match) == FALSE,
                        if_else(
      condition = match_dist <= 2,
      true = city_match,
      false = city_prep
    ),
      false = city_prep
  ))




summary(pa$match_dist)
sum(pa$match_dist == 1, na.rm = TRUE)
n_distinct(pa$city_swap)
```


#### Refine


```{r valid_city}
valid_city <- unique(zipcode$city)
```
Use the OpenRefine algorithms to cluster similar values and merge them together. This can be done using the refinr::key_collision_merge() and refinr::n_gram_merge() functions on our prepared and swapped city data.
```{r view_refine}
pa_refined <- pa %>%
  filter(match_dist != 1) %>% 
  filter(STATE =="PA") %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge(dict = valid_city) %>% 
      n_gram_merge(numgram = 2),
    refined = (city_swap != city_refine)
  ) %>% 
  filter(refined) %>% 
  select(
    index,
    FILERID, 
    CITY,
    city_prep,
    city_match,
    city_swap,
    match_dist,
    city_refine,
    STATE, 
    ZIPCODE,
    zip_clean
  )


pa_refined %>% 
  count(city_swap, city_refine) %>% 
  arrange(desc(n))
```


```{r}
refined_values <- unique(pa_refined$city_refine)
count_refined <- tibble(
  city_refine = refined_values, 
  refine_count = NA
)


for (i in seq_along(refined_values)) {
  count_refined$refine_count[i] <- sum(str_detect(pa$city_swap, refined_values[i]), na.rm = TRUE)
}


swap_values <- unique(pa_refined$city_swap)
count_swap <- tibble(
  city_swap = swap_values, 
  swap_count = NA
)


for (i in seq_along(swap_values)) {
  count_swap$swap_count[i] <- sum(str_detect(pa$city_swap, swap_values[i]), na.rm = TRUE)
}


pa_refined %>% 
  left_join(count_swap) %>% 
  left_join(count_refined) %>%
  select(
    FILERID,
    city_match,
    city_swap,
    city_refine,
    swap_count,
    refine_count
  ) %>% 
  mutate(diff_count = refine_count - swap_count) %>%
  mutate(refine_dist = stringdist(city_swap, city_refine)) %>%
  distinct() %>%
  arrange(city_refine) %>% 
  print_all()
```


Manually change the city_refine fields due to overcorrection.




```{r}
pa_refined$city_refine <- pa_refined$city_refine %>% 
  str_replace("^HUNTINGDON$", "NORTH HUNTINGDON") %>% 
  str_replace("^E FALLOWFIELD$", "FALLOWFIELD") %>% 
  str_replace("^WYO$", "WYOMISSNG") %>% 
  str_replace("^WG$", "WILLOW GROVE") %>% 
  str_replace("^S ABINGTON TOWNSHIP$", "NORTH ABINGTON TOWNSHIP") %>% 
  str_replace("^MT\\s", "MOUNT ") %>% 
  str_replace("^W\\sB$", "WILKES BARRE") 


refined_table <-pa_refined %>% 
  select(index, city_refine)
```
  


#### Merge 


```{r join_refine}
pa <- pa %>% 
  left_join(refined_table, by ="index") %>% 
  mutate(city = coalesce(city_refine, city_swap)) 


st_pattern <- str_c("\\s",unique(zipcode$state), "$", collapse = "|")


pa$city <- pa$city %>% 
  str_replace("\\sVLY$", " VALLEY") %>% 
  str_replace("\\sVLG$|\\sVL$|\\sVIL$", " VILLAGE") %>% 
  str_replace("\\sHLS$", " HILLS") %>% 
  str_replace("\\sTWP$|\\sTP$", " TOWNSHIP") %>% 
  str_replace("^ST\\s", "SAINT ") %>% 
  str_replace("\\sST\\s", " SAINT ") %>% 
  str_replace("^MT\\s", "MOUNT ") %>%  
  str_replace("^FT\\s", "FORT ") %>% 
  str_replace("^PHILA$", "PHILADELPHIA") %>% 
  str_replace("^W\\sB$|^WB$", "WILKES BARRE") %>% 
  str_replace("\\sHTS$|\\sHGTS$", " HEIGHTS") %>% 
  str_replace("\\sSQ$", " SQUARE") %>% 
  str_replace("\\sSPGS$|\\sSPR$|\\sSPRG$", "  SPRINGS") %>% 
  str_replace("\\sJCT$", " JUNCTION") %>% 
  str_replace("^E\\s", "EAST ") %>% 
  str_replace("^N\\s", "NORTH ") %>% 
  str_replace("^W\\s", "WEST ") %>% 
  str_remove(st_pattern) %>% 
  str_remove("^X+$")
```


```{r}
valid_city <- unique(c(valid_city, "MCMURRAY",
                "BRIGHTON TOWNSHIP",
                "HARDYSTON",
                "MONTVILLE",
                "MOLUNKUS TOWNSHIP",
                "MADISON TOWNSHIP",
                "BLOOMFIELD TOWNSHIP",
                "CASTLE PINES",
                "CEDAR FORT", 
                "COLLIER",
                "MOON TOWNSHIP",
                "YARDLEY",
                "ABBOTT PARK",
                "NORTH CHESTERFIELD",
                "OVERLAND PARK",
                "RESEARCH TRIANGLE PARK",
                "THE WOODLANDS",
                "COPLEY",
                "WYOMISSING",
                "FAIR OAKS RANCH",
                "BRANCHBURG",
                "PALM BEACH GARDENS",
                "NORTH HUNTINGDON",
                "SEHLBY TOWNSHIP"
                ))
pa_match_table <- pa %>% 
  filter(str_sub(pa$city, 1,1) == str_sub(pa$city_match, 1,1)) %>% 
  filter(city %out% valid_city)  %>% 
  mutate(string_dis = stringdist(city, city_match)) %>% 
  select (index, zip_clean, STATE, city, city_match, string_dis) %>% 
  distinct() %>% 
  add_count(city_match) %>% 
  rename("sec_city_match" = "city_match") %>% 
  select(index, sec_city_match)


pa_match_table$sec_city_match <- pa_match_table$sec_city_match %>% 
  str_replace("^CLEVELAND$", "CLEVELAND HEIGHTS") %>% 
  str_replace("CEDAR", "CONSHOHOCKEN")


pa <-pa %>% 
  left_join(pa_match_table, by = "index") %>% 
  mutate(city_final = coalesce(sec_city_match, city))
pa_out <- pa %>% filter(city_clean %out% valid_city) %>%  count(city_clean, CITY) %>% arrange(desc(n))

pa$city_final <- pa$city_ %>% 
  str_replace("\\sVLY$", " VALLEY") %>% 
  str_replace("\\sVLG$|\\sVL$|\\sVIL$", " VILLAGE") %>% 
  str_replace("\\sHLS$", " HILLS") %>% 
  str_replace("\\sTWP$|\\sTP$", " TOWNSHIP") %>% 
  str_replace("^ST\\s", "SAINT ") %>% 
  str_replace("\\sST\\s", " SAINT ") %>% 
  str_replace("^MT\\s", "MOUNT ") %>%  
  str_replace("^FT\\s", "FORT ") %>% 
  str_replace("^PHILA$", "PHILADELPHIA") %>% 
  str_replace("^W\\sB$|^WB$", "WILKES BARRE") %>% 
  str_replace("\\sHTS$|\\sHGTS$", " HEIGHTS") %>% 
  str_replace("\\sSQ$", " SQUARE") %>% 
  str_replace("\\sSPGS$|\\sSPR$|\\sSPRG$", "  SPRINGS") %>% 
  str_replace("\\sJCT$", " JUNCTION") %>% 
  str_replace("^E\\s", "EAST ") %>% 
  str_replace("^N\\s", "NORTH ") %>% 
  str_replace("^W\\s", "WEST ") %>% 
  str_remove(st_pattern) %>% 
  str_remove("^X+$")
```


Each process also increases the percent of valid city names.


```{r city_progress2, collapse=TRUE}
prop_in(pa$CITY, valid_city, na.rm = TRUE)
prop_in(pa$city_prep, valid_city, na.rm = TRUE)
prop_in(pa$city_swap, valid_city, na.rm = TRUE)
prop_in(pa$city, valid_city, na.rm = TRUE)
prop_in(pa$city_clean, valid_city, na.rm = TRUE)
```


Each step of the cleaning process reduces the number of distinct city values.
There are `r sum(!is.na(pa$CITY))` with `r n_distinct(pa$CITY)` distinct values, after the swap and refine processes, there are `r sum(!is.na(pa$city_clean))` entries with `r n_distinct(pa$city_clean)` distinct values. 


## Conclude


1. There are `r nrow(pa)` records in the database
1. There are `r sum(pa$dupe_flag)` records with suspected duplicate filerID, recipient, date, _and_ amount
(flagged with `dupe_flag`)
1. The ranges for dates and amounts are reasonable
1. Consistency has been improved with `stringr` package and custom `normal_*()` functions.
1. The five-digit `zip_clean` variable has been created with `zipcode::clean.zipcode()`
1. The `year` variable has been created with `lubridate::year()`
1. There are `r count_na(pa$CITY)` records with missing `city` values and `r count_na(pa$EXPNAME)` records with missing `payee` values (both flagged with the `na_flag`).


## Export


```{r write_clean}
clean_dir <- here("pa", "contribs", "data", "processed")
dir_create(clean_dir)
pa %>% 
  select(
    -city_prep,
    -on_year,
    -city_match,
    -city_clean,
    -match_dist,
    -city_swap,
    -city_refine
  ) %>% 
  write_csv(
    path = glue("{clean_dir}/pa_contribs_clean.csv"),
    na = ""
  )
```
