---
title: "Data Diary"
subtitle: "Pennsylvania Contribution"
author: "Yanqi Xu"
date: "`r format(Sys.time())`"
output:
   github_document:    
    df_print: tibble
    fig_caption: yes
    highlight: tango
    keep_md: yes
    max.print: 32
    toc: yes
    toc_float: no
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, results='asis'}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  warning = FALSE
)
options(width = 99)
```


## Project


The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.


Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:


1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved


## Objectives


This document describes the process used to complete the following objectives:


1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction


## Packages


The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.


```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  knitr, # knit documents
  glue, # combine strings
  scales, #format strings
  here, # relative storage
  fs, # search storage 
  vroom #read deliminated files
)
```


```{r fix_fun, echo=FALSE, collapse = TRUE}
# fix conflict
here <- here::here
print_all <- function(df) df %>% print(n = nrow(.)) 
```


This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.


The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.


```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```


[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"


## Import


[Link][03] to download

[03]: https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx "source"


### Download


Download raw, **immutable** data file. Go to https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx. We'll download the files from 2015 to 2019 (file format: zip file) with the script.


```{r raw_dir}
# create a directory for the raw data
raw_dir <- here("pa", "contribs", "data", "raw")
dir_create(raw_dir)
```


Download all the file packages containing all campaign-finance-related files. 
```{r download to raw_dir, eval = FALSE}
#download the files into the directory
pa_exp_urls <- glue("https://www.dos.pa.gov//VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/{2015:2019}.zip")

if (!all_files_new(raw_dir)) {
  for (url in pa_exp_urls) {
    download.file(
      url = url,
      destfile = glue("{raw_dir}/{basename(url)}")
    )
  }
}
```


### Read
Read individual csv files from the downloaded zip files
```{r read_many, eval = FALSE}

zip_files <- dir_ls(raw_dir, glob = "*.zip")

if (all_files_new(path = raw_dir, glob = "*.txt")) {
  for (i in seq_along(zip_files)) {
    unzip(
      zipfile = zip_files[i],
      #Matches the csv files that starts with expense, and trim the "./ " from directory names
      files = grep("contrib.+", unzip(zip_files[i]), value = TRUE) %>% substring(3,),
      exdir = raw_dir
    )
  }
}
```
Read multiple csvs into R
```{r read multiple files}
#recursive set to true because 2016 and 2015 have subdirectories under "raw"
contrib_files <- list.files(raw_dir, pattern = ".txt", recursive = TRUE, full.names = TRUE)
#pa_lines <- list.files(raw_dir, pattern = ".txt", recursive = TRUE) %>% map(read_lines) %>% unlist()
pa_col_names <- c("FILERID", "EYEAR", "CYCLE", "SECTION", "CONTRIBUTOR", "ADDRESS1", "ADDRESS2", "CITY", "STATE", "ZIPCODE", "OCCUPATION", "ENAME", "EADDRESS1", "EADDRESS2", "ECITY", "ESTATE", "EZIPCODE", "CONTDATE1", "CONTAMT1", "CONTDATE2", "CONTAMT2", "CONTDATE3", "CONTAMT3", "CONTDESC")

pa <- contrib_files %>% 
  map(read_delim, delim = ",", escape_double = FALSE,
      escape_backslash = FALSE, col_names = pa_col_names, 
      col_types = cols(.default = col_character(),
                       EYEAR = col_integer(),
                       CYCLE = col_integer(),
                     CONTAMT1 = col_double())) %>% 
  bind_rows() %>% 
  mutate_if(is_character, str_to_upper)
      
```

### About

More information about the record layout can be found here https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/readme.txt.

### Encoding
```{r}
for (i in c(5:10)) {
  pa[[i]] <- iconv(pa[[i]], 'UTF-8', 'ASCII') %>% 
    toupper() %>% 
   str_replace("&AMP;", "&") 
}
```

Some columns are not read properly due to extraneous and unescaped commas or double quotes. The following lines of code fix that. 
### Indexing
```{r}
pa <- tibble::rowid_to_column(pa, "index")
```

### Repositioning
```{r misplaced columns}
to_reposit <- which(pa$CONTDATE2 != 0 & is.na(pa$CONTDATE2) == FALSE )

nudge <- function(df, index, cut_off_col) {
  for (i in index){
    col_posit <- match(cut_off_col, colnames(pa))
    df[i, col_posit] <- str_c(df[i, col_posit], df[i, col_posit+1], sep = " ")
      for (col in {col_posit+1}:{ncol(df)-1}) {
        df[i, col] <- df[i, col+1]
      } 
  }
  return(df)
}

pa <- nudge(pa, to_reposit[1:length(to_reposit)-1], "ENAME")
pa <- nudge(pa, to_reposit[length(to_reposit)], "CONTRIBUTOR")

pa$CONTAMT1 <- as.numeric(pa$CONTAMT1)
```

## Explore

There are `nrow(pa)` records of `length(pa)` variables in the full database.

```{r glimpse}
head(pa)
tail(pa)
glimpse(pa)
```

### Distinct


The variables range in their degree of distinctness.`CONTDATE2`, `CONTAMT2`, `CONTDATE3", "CONTAMT3` only have one value 0.


```{r n_distinct}
pa %>% glimpse_fun(n_distinct)
```

### Missing

The variables also vary in their degree of values that are `NA` (missing).


```{r count_na}
pa %>% glimpse_fun(count_na)
```


We will flag any records with missing values in the key variables used to identify a contribution.
There are `r sum(pa$na_flag)` records missing `CONTRIBUTOR`, `CONTAMT1` AND `CONTDATE1`
```{r na_flag}
pa <- pa %>% flag_na(CONTRIBUTOR, CONTAMT1, CONTDATE1)
```


### Duplicates


```{r flag_dupes, eval = FALSE, collapse=TRUE}
pa <- flag_dupes(pa, dplyr::everything())
sum(pa$dupe_flag)
```

### Ranges

#### Amounts

```{r, collapse = TRUE}
summary(pa$CONTRIBUTOR)
sum(pa$CONTAMT1 < 0 , na.rm = TRUE)
```


See how the campaign contributions were distributed

```{r amount distribution, eval = TRUE}
pa %>% 
  ggplot(aes(x = CONTAMT1)) + 
  geom_histogram() +
  scale_x_continuous(
    trans = "log10", labels = dollar) +
  labs(title = "Pennsylvania Campaign Contribution Amount Distribution",
       caption = "Source: Pennsylvania Dept. of State")
```

#### Year

Add a `year` variable from `date` after `col_date()` using `lubridate::year()`.

```{r convert to date}
pa$CONTDATE1 <- as.Date(pa$CONTDATE1, "%Y%m%d")
```

```{r add_year}
pa <- pa %>% mutate(year = year(CONTDATE1), on_year = is_even(year))
```

#### Dates

Records in the PA contribution datasets date back to `r min(pa$CONTDATE1, na.rm = T)` till `r max(pa$CONTDATE1, na.rm = T)` 
```{r}
summary(pa$CONTDATE1)
```

The `CONTDATE2` and `CONTDATE3` variables should be blank
```{r year_flag}
pa <- pa %>% mutate(date_flag = year < 2000 | year > format(Sys.Date(), "%Y"), 
                    year_clean = ifelse(
                    date_flag, NA, year))

pa <- pa %>% mutate(date_clean = CONTDATE1) 
pa$date_clean[pa$date_flag] <- NA

summary(pa$date_clean)
```


```{r year_count_bar, eval = FALSE, echo=FALSE}
pa %>% 
  filter( 2014 < year & year < 2020) %>% 
  count(on_year, year) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(aes(fill=on_year)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  labs(
    title = "Pennsylvania Contributions Counts per Year",
    caption = "Source: Pennsylvania Dept. of State",
    x = "Year",
    y = "Count"
  )
  
```

## Wrangle
The state column is now pretty clean, as all non-NA columns have two characters.

### Zipcode
The Zipcode column can range from 1 to 13 columns.

```{r collapse = TRUE}
table(nchar(pa$ZIPCODE))
```

```{r normalize zip, collapse = TRUE}
pa <- pa %>% 
  mutate(
    zip_clean = ZIPCODE %>% 
      normal_zip(na_rep = TRUE))
sample(pa$zip_clean, 10)
  
## Same with EZIPCODE
pa <- pa %>% 
  mutate(
    employer_zip_clean = EZIPCODE %>% 
      normal_zip(na_rep = TRUE)
  )
  sample(pa$employer_zip_clean, 10)
```

### State
View values in the STATE field is not a valid state abbreviation
```{r fix state abbrev, collapse = TRUE}
{pa$STATE[pa$STATE %out% valid_state]}[!is.na(pa$STATE[pa$STATE %out% valid_state])]
{pa$ESTATE[pa$ESTATE %out% valid_state]}[!is.na(pa$ESTATE[pa$ESTATE %out% valid_state])]
```
These are contributions from people in Canada, which we can leave in. 

### City
Cleaning city values is the most complicated. This process involves four steps:

1. Prepare raw city values by removing invalid data and reducing inconsistencies
1. Match prepared city values with the _actual_ city name of that record's ZIP code
1. swap prepared city values with the ZIP code match _if_ only 1 edit is needed
1. Refine swapped city values with key collision and n-gram fingerprints

#### Prep

`r sum(!is.na(pa$CITY))` distinct cities were in the original dataset in column 
```{r prep_city, collapse = TRUE}
valid_place <- c(valid_city, extra_city) %>% unique()

pa <- pa %>% mutate(city_prep = normal_city(city = CITY,
                                            geo_abbs = usps_city,
                                            st_abbs = c(valid_state),
                                            na = invalid_city,
                                            na_rep = TRUE))
pa <- pa %>% mutate(employer_city_prep = normal_city(city = ECITY,
                                            geo_abbs = usps_city,
                                            st_abbs = c(valid_state),
                                            na = invalid_city,
                                            na_rep = TRUE))
n_distinct(pa$city_prep)
n_distinct(pa$employer_city_prep)
```

#### Match

```{r match_dist}
pa <- pa %>%
  left_join(
    y = zipcodes,
    by = c(
      "zip_clean" = "zip",
      "STATE" = "state"
    )
  ) %>% 
  rename(city_match = city)
```

#### Swap

To replace city names with expected city names from zipcode when the two variables are no more than two characters different
```{r }
pa <- pa %>% 
  mutate(
    match_dist = stringdist(city_prep, city_match),
    city_swap = if_else(condition = is.na(city_match) == FALSE,
                        if_else(
      condition = match_dist <= 2,
      true = city_match,
      false = city_prep
    ),
      false = city_prep
  ))

summary(pa$match_dist)
sum(pa$match_dist == 1, na.rm = TRUE)
n_distinct(pa$city_swap)
```

#### Refine

Use the OpenRefine algorithms to cluster similar values and merge them together. This can be done using the refinr::key_collision_merge() and refinr::n_gram_merge() functions on our prepared and swapped city data.
```{r view_refine}
pa_refined <- pa %>%
  filter(match_dist != 1) %>% 
  filter(STATE =="PA") %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge(dict = valid_place) %>% 
      n_gram_merge(numgram = 2),
    refined = (city_swap != city_refine)
  ) %>% 
  filter(refined) %>% 
  select(
    index,
    FILERID, 
    CITY,
    city_prep,
    city_match,
    city_swap,
    match_dist,
    city_refine,
    STATE, 
    ZIPCODE,
    zip_clean
  )

pa_refined %>% 
  count(city_swap, city_refine) %>% 
  arrange(desc(n))
```


```{r display refine value table, eval = FALSE, echo = FALSE}
refined_values <- unique(pa_refined$city_refine)
count_refined <- tibble(
  city_refine = refined_values, 
  refine_count = NA
)


for (i in seq_along(refined_values)) {
  count_refined$refine_count[i] <- sum(str_detect(pa$city_swap, refined_values[i]), na.rm = TRUE)
}


swap_values <- unique(pa_refined$city_swap)
count_swap <- tibble(
  city_swap = swap_values, 
  swap_count = NA
)


for (i in seq_along(swap_values)) {
  count_swap$swap_count[i] <- sum(str_detect(pa$city_swap, swap_values[i]), na.rm = TRUE)
}


pa_refined %>% 
  left_join(count_swap) %>% 
  left_join(count_refined) %>%
  select(
    FILERID,
    city_match,
    city_swap,
    city_refine,
    swap_count,
    refine_count
  ) %>% 
  mutate(diff_count = refine_count - swap_count) %>%
  mutate(refine_dist = stringdist(city_swap, city_refine)) %>%
  distinct() %>%
  arrange(city_refine) %>% 
  print_all()
```


Manually change the city_refine fields due to overcorrection.

```{r}
pa_refined$city_refine <- pa_refined$city_refine %>% 
  str_replace("SHAWNEE\\sON\\sDELA$", "SHAWNEE ON DELAWARE") %>% 
  str_replace("^SPRINGBROOK$", "SPRING BROOK") %>% 
  str_replace("^ALBA$", "BALA CYNWYD") %>% 
  str_replace("^BRADFORDWOODS$", "BRADFORD WOODS") 

refined_table <-pa_refined %>% 
  select(index, city_refine)
```

#### Merge 

```{r join_refine}
pa <- pa %>% 
  left_join(refined_table, by ="index") %>% 
  mutate(city = coalesce(city_refine, city_swap)) 

pa$city <- pa$city %>% 
  str_replace("\\sTWP$|\\sTP$", " TOWNSHIP") %>% 
  str_replace("\\sSQ$", " SQUARE") 

```


```{r}
pa_out <- pa %>% filter(city %out% valid_place) 

pa_city_lookup <- read_csv(file = here("pa", "contribs", "data", "raw", "pa_city_lookup_contrib.csv"), skip =1, col_names = c("city", "city_lookup", "changed"))

pa_out <- pa_out %>% select(index, CITY) %>% 
  inner_join(pa_city_lookup, by = c("CITY" = "city")) %>% 
  drop_na(CITY) %>% 
  select(index,city_lookup)

pa <- pa %>% left_join(pa_out, by = "index") %>% mutate(city_lkp = ifelse(pa$index %in% pa_out$index, city_lookup,city))

pa_match_table <- pa %>% 
  filter(str_sub(pa$city_swap, 1,1) == str_sub(pa$city_match, 1,1)) %>% 
  filter(city_lkp %out% valid_place)  %>% 
  mutate(string_dis = stringdist(city, city_match)) %>% 
  select (index, zip_clean, STATE, city, city_match, string_dis) %>% 
  distinct() %>% 
  add_count(city_match) %>% 
  rename("sec_city_match" = "city_match") %>% 
  filter(string_dis < 11) 

pa_match_table[pa_match_table$city == "HODGDON", "sec_city_match"] <- "HODGDON"
pa_match_table[pa_match_table$city == "WOLVERINE LAKE", "sec_city_match"] <- "WOLVERINE LAKE" 
pa_match_table[pa_match_table$city == "CROSS LANES", "sec_city_match"] <- "CROSS LANES"
pa_match_table[pa_match_table$city == "PONTE VEDRA", "sec_city_match"] <- "PONTE VEDRA BEACH"
pa_match_table[pa_match_table$city == "MOYLAN", "sec_city_match"] <- "MOYLAN"

pa_match_table[pa_match_table$city == "WYO", "sec_city_match"] <- "WYOMISSING"
pa_match_table[pa_match_table$city == "TEMPLE TERRACE", "sec_city_match"] <- "TEMPLE TERRACE"
pa_match_table[pa_match_table$city == "SPRINGTON", "sec_city_match"] <- "SPRINGTON"

pa <- pa_match_table %>% select(index, sec_city_match) %>% 
  right_join(pa, by = "index") %>% 
  mutate(city_clean = coalesce(sec_city_match, city_lkp))
```

#### Check
```{r check_city}
pa <- pa %>% 
  filter(city %out% valid_place) %>% 
  drop_na(city,STATE) %>% 
  count(city, STATE) %>% 
  mutate(check_city_flag = pmap_lgl(.l = list(city, STATE), .f = check_city, key = api_key))

pa_add_valid <- pa %>% filter(check_city_flag) %>% print_all() 
# Use the following command to paste to the extra_city Google sheet
# pa_add_valid$city %>% cat( sep = "\n")
valid_places <- unique(c(valid_places, pa$city[pa$check_city_flag]))
```

```{r fetch_city, eval=FALSE}
pa_filer <- pa_filer %>% mutate(city = city %>% 
  str_replace("^PHILA$", "PHILADELPHIA") %>% 
  str_replace("^PGH$", "PITSSBURGH"))

pa_filer_out <- pa_filer %>% filter(city %out% valid_places) %>% 
  drop_na(STATE,city) %>% 
  count(STATE, city) 

api_key <- Sys.getenv("GEOCODING_API")


pa_filer_out <- pa_filer_out %>% cbind(
  pmap_dfr(.l = list(pa_filer_out$city, pa_filer_out$STATE), .f = check_city, key = api_key, guess = T))
```


```{r generate progress}
progress <- progress_table(
  pa$city_raw,
  pa$city_norm,
  pa$city_swap,
  pa$city_clean,
  compare = valid_place
) %>% mutate(stage = as_factor(stage))
```

```{r print progress, echo=FALSE}
kable(progress, digits = 3)
```
You can see how the percentage of valid values increased with each stage.

```{r progress_bar, echo=FALSE}
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = RColorBrewer::brewer.pal(3, "Dark2")[3]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Massachusetts City Normalization Progress",
    x = "Stage",
    y = "Percent Valid"
  )
```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivilent.

```{r distinct_bar}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Pennsylvania Contributions City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Percent Valid",
    fill = "Valid"
  )
```

```{r clean address, echo = TRUE, execute = FALSE}
pa <- pa %>%   
  unite(
    ADDRESS1, ADDRESS2,
    col = address_clean,
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(address_clean = normal_address(
      address = address_clean,
      add_abbs = usps_city,
      na_rep = TRUE
    ))
```

```{r join filer table}
filer_dir <- here("pa", "expends", "data", "processed")
pa_filer <- read_csv(glue("{filer_dir}/pa_filers_clean.csv"), 
                     col_types = cols(.default = col_character())) %>% 
  rename_at(vars(contains("clean")), list(~str_c("filer_",.))) %>% 
  select(
    FILERID,
    EYEAR,
    CYCLE,
    FILERNAME,
    FILERTYPE,
    filer_state,
    ends_with("clean")
  )
```


Each step of the cleaning process reduces the number of distinct city values.
There are `r sum(!is.na(pa$CITY))` with `r n_distinct(pa$CITY)` distinct values, after the swap and refine processes, there are `r sum(!is.na(pa$city_clean))` entries with `r n_distinct(pa$city_clean)` distinct values. 

### Address
Finally, we will create a new variable named `address_clean` cleaned with the `normal_address` function. Make sure you're using a tidyr version that is greater than "0.8.3.9", where the na.rm argument of the `unite()` function is supported.
```{r normal address}
pa <- pa %>% unite(
    ADDRESS1, ADDRESS2,
    col = address_clean,
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(address_clean = normal_address(address = address_clean,
      add_abbs = usps_city,
      na_rep = TRUE))
```



## Conclude


1. There are `r nrow(pa)` records in the database
1. There are `r sum(pa$dupe_flag)` records with suspected duplicate filerID, recipient, date, _and_ amount
(flagged with `dupe_flag`)
1. The ranges for dates and amounts are reasonable
1. Consistency has been improved with `stringr` package and custom `normal_*()` functions.
1. The five-digit `zip_clean` variable has been created with `zipcode::clean.zipcode()`
1. The `year` variable has been created with `lubridate::year()`
1. There are `r count_na(pa$CITY)` records with missing `city` values and `r count_na(pa$EXPNAME)` records with missing `payee` values (both flagged with the `na_flag`).


## Export


```{r write_clean}
clean_dir <- here("pa", "contribs", "data", "processed")
dir_create(clean_dir)
pa %>% 
  select(
    -city_prep,
    -on_year,
    -match_dist,
    -city_swap,
    -city_refine,
    -city_match,
    -city,
    -city_lookup,
    -city_lkp,
    -sec_city_match
  ) %>% 
  rename(ZIP5 = zip_clean, employer_ZIP5 = employer_zip_clean) %>% 
  write_csv(
    path = glue("{clean_dir}/pa_contribs_clean.csv"),
    na = ""
  )
```
