---
title: "Data Diary"
subtitle: "Pennsylvania Contributions"
author: "Yanqi Xu"
date: "`r format(Sys.time())`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, results='asis'}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  warning = FALSE
)
options(width = 99)
```


## Project


The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.


Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:


1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved


## Objectives


This document describes the process used to complete the following objectives:


1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction


## Packages


The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.


```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  knitr, # knit documents
  glue, # combine strings
  scales, #format strings
  here, # relative storage
  fs, # search storage 
  vroom, #read deliminated files
  campfin #wrangle data for accountability project
)
```


```{r fix_fun, echo=FALSE, collapse = TRUE}
# fix conflict
here <- here::here
```


This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.


The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.


```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```


[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"


## Import


[Link][03] to download

[03]: https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx "source"


### Download


Download raw, **immutable** data file. Go to https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx. We'll download the files from 2015 to 2019 (file format: zip file) with the script.


```{r raw_dir}
# create a directory for the raw data
raw_dir <- here("pa", "contribs", "data", "raw")
dir_create(raw_dir)
```


Download all the file packages containing all campaign-finance-related files. 
```{r download to raw_dir, eval = FALSE}
#download the files into the directory
pa_exp_urls <- glue("https://www.dos.pa.gov//VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/{2015:2019}.zip")

if (!all_files_new(raw_dir)) {
  for (url in pa_exp_urls) {
    download.file(
      url = url,
      destfile = glue("{raw_dir}/{basename(url)}")
    )
  }
}
```


### Read
Read individual csv files from the downloaded zip files
```{r read_many, eval = FALSE}

zip_files <- dir_ls(raw_dir, glob = "*.zip")

if (all_files_new(path = raw_dir, glob = "*.txt")) {
  for (i in seq_along(zip_files)) {
    unzip(
      zipfile = zip_files[i],
      #Matches the csv files that starts with expense, and trim the "./ " from directory names
      files = grep("contrib.+", unzip(zip_files[i]), value = TRUE) %>% substring(3,),
      exdir = raw_dir
    )
  }
}
```
Read multiple csvs into R
```{r read multiple files, eval=FALSE}
#recursive set to true because 2016 and 2015 have subdirectories under "raw"
contrib_files <- list.files(raw_dir, pattern = ".txt", recursive = TRUE, full.names = TRUE)
#pa_lines <- list.files(raw_dir, pattern = ".txt", recursive = TRUE) %>% map(read_lines) %>% unlist()
pa_col_names <- c("FILERID", "EYEAR", "CYCLE", "SECTION", "CONTRIBUTOR", "ADDRESS1", "ADDRESS2", "CITY", "STATE", "ZIPCODE", "OCCUPATION", "ENAME", "EADDRESS1", "EADDRESS2", "ECITY", "ESTATE", "EZIPCODE", "CONTDATE1", "CONTAMT1", "CONTDATE2", "CONTAMT2", "CONTDATE3", "CONTAMT3", "CONTDESC")

pa <- contrib_files %>% 
  map(read_delim, delim = ",", escape_double = FALSE,
      escape_backslash = FALSE, col_names = pa_col_names, 
      col_types = cols(.default = col_character(),
                       EYEAR = col_integer(),
                       CYCLE = col_integer(),
                     CONTAMT1 = col_double())) %>% 
  bind_rows() %>% 
  mutate_if(is_character, str_to_upper)
      
```

### About

More information about the record layout can be found here https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/readme.txt.

### Encoding
```{r eval=FALSE}
for (i in c(5:10)) {
  pa[[i]] <- iconv(pa[[i]], 'UTF-8', 'ASCII') %>% 
    toupper() %>% 
   str_replace("&AMP;", "&") 
}
```

Some columns are not read properly due to extraneous and unescaped commas or double quotes. The following lines of code fix that. 
### Indexing
```{r, eval=FALSE}
pa <- tibble::rowid_to_column(pa, "index")
```

### Repositioning
```{r misplaced columns,eval=FALSE}
to_reposit <- which(pa$CONTDATE2 != 0 & is.na(pa$CONTDATE2) == FALSE )

nudge <- function(df, index, cut_off_col) {
  for (i in index){
    col_posit <- match(cut_off_col, colnames(pa))
    df[i, col_posit] <- str_c(df[i, col_posit], df[i, col_posit+1], sep = " ")
      for (col in {col_posit+1}:{ncol(df)-1}) {
        df[i, col] <- df[i, col+1]
      } 
  }
  return(df)
}

pa <- nudge(pa, to_reposit[1:length(to_reposit)-1], "ENAME")
pa <- nudge(pa, to_reposit[length(to_reposit)], "CONTRIBUTOR")

pa$CONTAMT1 <- as.numeric(pa$CONTAMT1)
```

## Explore

There are `nrow(pa)` records of `length(pa)` variables in the full database.

```{r glimpse, eval=FALSE}
head(pa)
tail(pa)
glimpse(pa)
```


```{r echo=FALSE}
pa <- read_csv(glue("{raw_dir}/pa_contribs_midway.csv"), 
col_types = cols(.default = col_character(),
                 CONTAMT1 = col_double(),
                 CONTDATE1 = col_date(),
                 dupe_flag = col_logical(),
                 na_flag = col_logical()
                 ))

pa_orig <-  pa %>% 
  select(2:25)
```

### Distinct

The variables range in their degree of distinctness.`CONTDATE2`,  `CONTDATE3`, only have one value 0.


```{r n_distinct, eval=FALSE}
pa %>% glimpse_fun(n_distinct)
```

```{r actual n_distinct, echo=FALSE, collapse=TRUE}
pa_orig %>% glimpse_fun(n_distinct)
```

### Missing

The variables also vary in their degree of values that are `NA` (missing).


```{r count_na, eval=FALSE}
pa %>% glimpse_fun(count_na)
```

```{r actual count_na, echo=FALSE}
pa_orig %>% glimpse_fun(count_na)
```

We will flag any records with missing values in the key variables used to identify a contribution.
There are `r sum(pa$na_flag)` records missing `CONTRIBUTOR`, `CONTAMT1` AND `CONTDATE1`
```{r na_flag, eval=FALSE}
pa <- pa %>% flag_na(CONTRIBUTOR, CONTAMT1, CONTDATE1)
```

### Duplicates


```{r flag_dupes, eval = FALSE, collapse=TRUE}
pa <- flag_dupes(pa, dplyr::everything())
```

```{r, collapse=TRUE}
sum(pa$dupe_flag)
```


### Ranges

#### Amounts

```{r, collapse = TRUE}
summary(pa$CONTRIBUTOR)
sum(pa$CONTAMT1 < 0 , na.rm = TRUE)
```


See how the campaign contributions were distributed

```{r amount distribution, eval = TRUE}
pa %>% 
  ggplot(aes(x = CONTAMT1)) + 
  geom_histogram() +
  scale_x_continuous(
    trans = "log10", labels = dollar) +
  labs(title = "Pennsylvania Campaign Contribution Amount Distribution",
       caption = "Source: Pennsylvania Dept. of State")
```

#### Year

Add a `year` variable from `date` after `col_date()` using `lubridate::year()`.

```{r convert to date, eval=FALSE}
pa$CONTDATE1 <- as.Date(pa$CONTDATE1, "%Y%m%d")
```

```{r add_year, eval=FALSE}
pa <- pa %>% mutate(year = year(CONTDATE1), on_year = is_even(year))
```

#### Dates

Records in the PA contribution datasets date back to `r min(pa$CONTDATE1, na.rm = T)` till `r max(pa$CONTDATE1, na.rm = T)` 
```{r}
summary(pa$CONTDATE1)
```

The `CONTDATE2` and `CONTDATE3` variables should be blank
```{r year_flag, eval=FALSE}
pa <- pa %>% mutate(date_flag = year < 2000 | year > format(Sys.Date(), "%Y"), 
                    year_clean = ifelse(
                    date_flag, NA, year))

pa <- pa %>% mutate(date_clean = CONTDATE1) 
pa$date_clean[pa$date_flag] <- NA

summary(pa$date_clean)
```


```{r year_count_bar, eval = TRUE, echo=FALSE}
pa %>% 
  filter( 2014 < year & year < 2020) %>% 
  count(on_year, year) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(aes(fill=on_year)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  labs(
    title = "Pennsylvania Contributions Counts per Year",
    caption = "Source: Pennsylvania Dept. of State",
    x = "Year",
    y = "Count"
  )
  
```

## Wrangle
In order to normalize column content and avoid irregular tabulation in the Accountability Project, we normalize column names with string functions from the `campfin` package.

```{r names}
pa <- pa %>% 
  mutate(contributor_clean = str_remove(CONTRIBUTOR, "^MRS\\s|^MR\\s|^MS\\s|^MISS\\s|^DR\\s"))
```


### State
View values in the STATE field is not a valid state abbreviation
```{r fix state abbrev, collapse = TRUE, eval=TRUE}
{pa$STATE[pa$STATE %out% valid_state]}[!is.na(pa$STATE[pa$STATE %out% valid_state])]
{pa$ESTATE[pa$ESTATE %out% valid_state]}[!is.na(pa$ESTATE[pa$ESTATE %out% valid_state])]
```
These are contributions from people in Canada, which we can leave in. The state column is now pretty clean, as all non-NA columns have two characters.


### Zipcode
The Zipcode column can range from 1 to 13 columns.

```{r collapse = TRUE}
table(nchar(pa$ZIPCODE))
```

```{r normalize zip, collapse = TRUE, eval=FALSE}
pa <- pa %>% 
  mutate(
    zip_clean = ZIPCODE %>% 
      normal_zip(na_rep = TRUE))
sample(pa$zip_clean, 10)
  
## Same as EZIPCODE
pa <- pa %>% 
  mutate(
    employer_zip_clean = EZIPCODE %>% 
      normal_zip(na_rep = TRUE)
  )
  sample(pa$employer_zip_clean, 10)
```


### City
Cleaning city values is the most complicated. This process involves the following steps:

1. Prepare raw city values by removing invalid data and reducing inconsistencies
1. Match prepared city values with the _actual_ city name of that record's ZIP code
1. swap prepared city values with the ZIP code match _if_ only 1 edit is needed
1. Refine swapped city values with key collision and n-gram fingerprints
1. Use the lookup table (inspected by data editor at IRW) to join and replace values outside of `valid_place`
1. Use a second swap to swap out cities that have the same zipcode as the corresponding valid city and whose string distance is no greater than 11. The process is also vetted by an IRW staff. 

Our goal is to correct misspelled city names to the point where the majority of the city names are within the `valid_place` vector. 
```{r}
valid_place <- c(valid_city, extra_city) %>% unique()
```


#### Prep

`r sum(!is.na(pa$CITY))` distinct cities were in the original dataset in column 
```{r prep_city, collapse = TRUE, eval=FALSE}
pa <- pa %>% mutate(city_prep = normal_city(city = CITY,
                                            geo_abbs = usps_city,
                                            st_abbs = c(valid_state),
                                            na = invalid_city,
                                            na_rep = TRUE))
pa <- pa %>% mutate(employer_city_prep = normal_city(city = ECITY,
                                            geo_abbs = usps_city,
                                            st_abbs = c(valid_state),
                                            na = invalid_city,
                                            na_rep = TRUE))
n_distinct(pa$city_prep)
n_distinct(pa$employer_city_prep)
```

#### Match

```{r match_dist, eval=FALSE}
pa <- pa %>%
  left_join(
    y = zipcodes,
    by = c(
      "zip_clean" = "zip",
      "STATE" = "state"
    )
  ) %>% 
  rename(city_match = city)
```

#### Swap

To replace city names with expected city names from zipcode when the two variables are no more than two characters different
```{r swap, eval=FALSE}
pa <- pa %>% 
  mutate(
    match_dist = stringdist(city_prep, city_match),
    city_swap = if_else(condition = is.na(city_match) == FALSE,
                        if_else(
      condition = match_dist <= 2,
      true = city_match,
      false = city_prep
    ),
      false = city_prep
  ))

summary(pa$match_dist)
sum(pa$match_dist == 1, na.rm = TRUE)
n_distinct(pa$city_swap)
```

#### Refine

Use the OpenRefine algorithms to cluster similar values and merge them together. This can be done using the refinr::key_collision_merge() and refinr::n_gram_merge() functions on our prepared and swapped city data.
```{r view_refine, eval=FALSE}
pa_refined <- pa %>%
  filter(match_dist != 1) %>% 
  filter(STATE =="PA") %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge(dict = valid_place) %>% 
      n_gram_merge(numgram = 2),
    refined = (city_swap != city_refine)
  ) %>% 
  filter(refined) %>% 
  select(
    index,
    FILERID, 
    CITY,
    city_prep,
    city_match,
    city_swap,
    match_dist,
    city_refine,
    STATE, 
    ZIPCODE,
    zip_clean
  )

pa_refined %>% 
  count(city_swap, city_refine) %>% 
  arrange(desc(n))
```


```{r display refine value table, eval = FALSE, echo = FALSE}
refined_values <- unique(pa_refined$city_refine)
count_refined <- tibble(
  city_refine = refined_values, 
  refine_count = NA
)


for (i in seq_along(refined_values)) {
  count_refined$refine_count[i] <- sum(str_detect(pa$city_swap, refined_values[i]), na.rm = TRUE)
}


swap_values <- unique(pa_refined$city_swap)
count_swap <- tibble(
  city_swap = swap_values, 
  swap_count = NA
)


for (i in seq_along(swap_values)) {
  count_swap$swap_count[i] <- sum(str_detect(pa$city_swap, swap_values[i]), na.rm = TRUE)
}


pa_refined %>% 
  left_join(count_swap) %>% 
  left_join(count_refined) %>%
  select(
    FILERID,
    city_match,
    city_swap,
    city_refine,
    swap_count,
    refine_count
  ) %>% 
  mutate(diff_count = refine_count - swap_count) %>%
  mutate(refine_dist = stringdist(city_swap, city_refine)) %>%
  distinct() %>%
  arrange(city_refine) %>% 
  print_all()
```


Manually change the city_refine fields due to overcorrection.

```{r, eval=FALSE}
pa_refined$city_refine <- pa_refined$city_refine %>% 
  str_replace("SHAWNEE\\sON\\sDELA$", "SHAWNEE ON DELAWARE") %>% 
  str_replace("^SPRINGBROOK$", "SPRING BROOK") %>% 
  str_replace("^ALBA$", "BALA CYNWYD") %>% 
  str_replace("^BRADFORDWOODS$", "BRADFORD WOODS") 

refined_table <-pa_refined %>% 
  select(index, city_refine)
```

#### Merge 

```{r join_refine, eval=FALSE}
pa <- pa %>% 
  left_join(refined_table, by ="index") %>% 
  mutate(city = coalesce(city_refine, city_swap)) 

pa$city <- pa$city %>% 
  str_replace("\\sTWP$|\\sTP$", " TOWNSHIP") %>% 
  str_replace("\\sSQ$", " SQUARE") 

```


```{r}
pa_out <- pa %>% filter(city %out% valid_place)

pa_city_lookup <- read_csv(file = here("pa", "contribs", "data", "raw", "pa_city_lookup_contrib.csv"), skip =1, col_names = c("city", "city_lookup", "changed"))

pa_out <- pa_out %>% select(index, CITY) %>% 
  inner_join(pa_city_lookup, by = c("CITY" = "city")) %>% 
  drop_na(CITY) %>% 
  select(index,city_lookup)

pa <- pa %>% left_join(pa_out, by = "index") %>% mutate(city_lkp = ifelse(pa$index %in% pa_out$index, city_lookup,city))

pa_match_table <- pa %>% 
  filter(str_sub(pa$city_swap, 1,1) == str_sub(pa$city_match, 1,1)) %>% 
  filter(city_lkp %out% valid_place)  %>% 
  mutate(string_dis = stringdist(city, city_match)) %>% 
  select (index, zip_clean, STATE, city, city_match, string_dis) %>% 
  distinct() %>% 
  add_count(city_match) %>% 
  rename("sec_city_match" = "city_match") %>% 
  filter(string_dis < 11) 

pa_match_table[pa_match_table$city == "HODGDON", "sec_city_match"] <- "HODGDON"
pa_match_table[pa_match_table$city == "WOLVERINE LAKE", "sec_city_match"] <- "WOLVERINE LAKE" 
pa_match_table[pa_match_table$city == "CROSS LANES", "sec_city_match"] <- "CROSS LANES"
pa_match_table[pa_match_table$city == "PONTE VEDRA", "sec_city_match"] <- "PONTE VEDRA BEACH"
pa_match_table[pa_match_table$city == "MOYLAN", "sec_city_match"] <- "MOYLAN"

pa_match_table[pa_match_table$city == "WYO", "sec_city_match"] <- "WYOMISSING"
pa_match_table[pa_match_table$city == "TEMPLE TERRACE", "sec_city_match"] <- "TEMPLE TERRACE"
pa_match_table[pa_match_table$city == "SPRINGTON", "sec_city_match"] <- "SPRINGTON"

pa <- pa_match_table %>% select(index, sec_city_match) %>% 
  right_join(pa, by = "index") %>% 
  mutate(city_tocheck = coalesce(sec_city_match, city_lkp))
```

#### Check
```{r check_city, eval=FALSE}
api_key <- Sys.getenv("GEOCODING_API")

pa_check <- pa %>% 
  filter(city_tocheck%out% valid_place) %>% 
  drop_na(city_tocheck,STATE) %>% 
  count(city_tocheck, STATE)

pa_check_result <- 
  pmap_dfr(.l = list(city = pa_check$city_tocheck, state = pa_check$STATE), .f = check_city, key = api_key, guess = T)


pa_check <- pa_check %>% 
  left_join(pa_check_result, by = c("city_tocheck" = "original_city", "STATE" = "original_state"))

pa_check <-  pa_check %>% select(-original_zip)

head(pa_check)
```


```{r write check_table to csv, echo=FALSE, eval=FALSE}
clean_dir <- here("pa", "contribs", "data", "processed")
pa_check %>% write_csv(
    path = glue("{clean_dir}/pa_check.csv"),
    na = ""
  )
```

```{r read saved check_table, echo=FALSE}
clean_dir <- here("pa", "contribs", "data", "processed")
pa_check <- read_csv(glue("{clean_dir}/pa_check.csv"))
head(pa_check)
```

Add the valid places to the `extra_city` spreadhseet.
```{r add to extra_city, eval=FALSE}
extra_city <- gs_title("extra_city")

extra_city <- extra_city %>% 
  gs_add_row(ws = 1, input = pa_check %>% filter(check_city_flag) %>% select(city_tocheck))
```

```{r}
pa_check <-  pa_check %>% 
  mutate(check_dist = stringdist(guess_place, city_tocheck),
    check_swap = if_else(condition = !is.na(guess_place),
                        if_else(
      condition = check_dist <= 2,
      true = guess_place,
      false = city_tocheck
    ),
      false = city_tocheck
  ))
```

Manually change the city_refine fields due to overcorrection.


```{r revert overcorrected refine}
pa <- pa_check %>% select(city_tocheck, STATE, check_swap) %>% 
  right_join(pa, by = c("city_tocheck","STATE"))
  
pa <-  pa %>% mutate(city_clean = coalesce(check_swap,city_tocheck))

pa$city_clean <- pa$city_clean %>% 
  str_replace("^DOWNTOWN$", "DAWNSTOWN") %>% 
  str_replace("^RESEARCH TRIANGLE PARK$", "RESEARCH TRIANGLE") %>% 
  na_if("^VIRTUAL$") %>% 
  na_if("^PROXY$")
```

```{r generate progress}
valid_place <-  c(valid_place,pa_check$city_tocheck[pa_check$check_city_flag]) %>% unique()

progress <- progress_table(
  pa$CITY,
  pa$city_prep,
  pa$city_swap,
  pa$city_lkp,
  pa$city_tocheck,
  pa$city_clean,
  compare = valid_place
) %>% mutate(stage = as_factor(stage))
```

```{r print progress, echo=FALSE}
kable(progress, digits = 3)
```
You can see how the percentage of valid values increased with each stage.

```{r progress_bar, echo=FALSE}
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = RColorBrewer::brewer.pal(3, "Dark2")[3]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Pennsylvania Contributions City Normalization Progress",
    x = "Stage",
    y = "Percent Valid"
  )
```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivilent.

```{r distinct_bar}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Pennsylvania Contributions City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Percent Valid",
    fill = "Valid"
  )
```

Each step of the cleaning process reduces the number of distinct city values.
There are `r sum(!is.na(pa$CITY))` with `r n_distinct(pa$CITY)` distinct values, after the swap and refine processes, there are `r sum(!is.na(pa$city_clean))` entries with `r n_distinct(pa$city_clean)` distinct values. 

### Address
Finally, we will create a new variable named `address_clean` cleaned with the `normal_address` function. Make sure you're using a tidyr version that is greater than "0.8.3.9", where the na.rm argument of the `unite()` function is supported.
```{r normal address}
pa <- pa %>% unite(
    ADDRESS1, ADDRESS2,
    col = address_clean,
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(address_clean = normal_address(address = address_clean,
      add_abbs = usps_city,
      na_rep = TRUE))
```


## Join with Filer 

We also need to pull up the processed filer table to join back to the `FILERID`, `EYEAR` and `CYCLE` field.

```{r join filer table}
filer_dir <- here("pa", "expends", "data", "processed")
pa_filer <- read_csv(glue("{filer_dir}/pa_filers_clean.csv"), 
                     col_types = cols(.default = col_character())) %>% 
  rename_at(vars(contains("clean")), list(~str_c("filer_",.))) %>% 
  select(
    FILERID,
    EYEAR,
    CYCLE,
    FILERNAME,
    FILERTYPE,
    filer_state,
    ends_with("clean")
  )
```

```{r join with filer}
pa <- pa %>% left_join(pa_filer, by = c("FILERID", "EYEAR", "CYCLE"))
```
## Conclude


1. There are `r nrow(pa)` records in the database
1. There are `r sum(pa$dupe_flag)` records with suspected duplicate filerID, recipient, date, _and_ amount
(flagged with `dupe_flag`)
1. The ranges for dates and amounts are reasonable
1. Consistency has been improved with `stringr` package and custom `normal_*()` functions.
1. The five-digit `zip_clean` variable has been created with `zipcode::clean.zipcode()`
1. The `year` variable has been created with `lubridate::year()`
1. There are `r count_na(pa$CITY)` records with missing `city` values and `r count_na(pa$EXPNAME)` records with missing `payee` values (both flagged with the `na_flag`).


## Export


```{r write_clean}
clean_dir <- here("pa", "contribs", "data", "processed")
dir_create(clean_dir)
pa %>% 
  select(
    -city_prep,
    -on_year,
    -match_dist,
    -city_swap,
    -city_refine,
    -city_match,
    -city,
    -city_lookup,
    -city_lkp,
    -sec_city_match,
    -check_swap,
    -CONTDATE2,
    -CONTDATE3
  ) %>% 
  rename(ZIP5 = zip_clean, employer_ZIP5 = employer_zip_clean) %>% 
  write_csv(
    path = glue("{clean_dir}/pa_contribs_clean.csv"),
    na = ""
  )
```
