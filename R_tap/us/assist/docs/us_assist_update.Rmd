---
title: "Federal Financial Assistance Update"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
if (!interactive()) {
  options(width = 99)
  set.seed(5)
}
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
fs::dir_create(here::here("docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give
journalists, policy professionals, activists, and the public at large a simple
way to search across huge volumes of public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each
dataset row as a transaction. For each transaction there should be (at least) 3
variables:

1. All **parties** to a transaction.
2. The **date** of the transaction.
3. The **amount** of money involved.

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for entirely duplicated records.
1. Check ranges of continuous variables.
1. Is there anything blank or missing?
1. Check for consistency issues.
1. Create a five-digit ZIP Code called `zip`.
1. Create a `year` field from the transaction date.
1. Make sure there is data on both parties to a transaction.

## Software

This data is processed using the free, open-source statistical computing
language R, which can be [installed from CRAN][cran] for various opperating
systems. For example, R can be installed from the apt package repository on
Ubuntu.

```bash
sudo apt update
sudo apt -y upgrade
sudo apt -y install r-base
```

[cran]: https://cran.r-project.org/

The following additional R packages are needed to collect, manipulate,
visualize, analyze, and communicate these results. The `pacman` package will
facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This
package contains functions custom made to help facilitate the processing of
campaign finance data.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  magrittr, # pipe operators
  gluedown, # print markdown
  janitor, # data frame clean
  refinr, # cluster and merge
  scales, # format strings
  readxl, # read excel
  aws.s3, # upload files
  knitr, # knit documents
  vroom, # read files fast
  furrr, # parallel map
  glue, # combine strings
  here, # relative storage
  pryr, # memory usage
  fs # search storage 
)
```

This document should be run as part of the `us_spending` project, which lives as
a sub-directory of the more general, language-agnostic [`irworkshop/tap`][tap]
GitHub repository.

The `us_spending` project uses the [RStudio projects][rproj] feature and should
be run as such. The project also uses the dynamic `here::here()` tool for file
paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::here()
```

[tap]: https://github.com/irworkshop/accountability_datacleaning
[rproj]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

## Data

Federal spending data is obtained from [USASpending.gov][usas], a site run by
the Department of the Treasury.

> [Many] sources of information support USAspending.gov, linking data from a
variety of government systems to improve transparency on federal spending for
the public. Data is uploaded directly from more than a hundred federal agencies'
financial systems. Data is also pulled or derived from other government
systems... In the end, more than 400 points of data are collected...

> Federal agencies submit contract, grant, loan, direct payment, and other award
data at least twice a month to be published on USAspending.gov. Federal agencies
upload data from their financial systems and link it to the award data
quarterly. This quarterly data must be certified by the agency's Senior
Accountable Official before it is displayed on USAspending.gov.

Flat text files containing all spending data can be found on the 
[Award Data Archive][arch].

[usas]: https://www.usaspending.gov/#/
[arch]: https://www.usaspending.gov/#/download_center/award_data_archive

> Welcome to the Award Data Archive, which features major agencies’ award
transaction data for full fiscal years. They’re a great way to get a view into
broad spending trends and, best of all, the files are already prepared — you can
access them instantaneously.

In this document, we are only going to update the most recent file to ensure the
data in on the TAP website is up to date. Instead of downloading multiple years
ZIP archives, we will only download the current year and the delta file.

Archives can be obtained for individual agencies or for _all_ agencies.

## Download

We first need to construct both the URLs and local paths to the archive files.

```{r full_path}
zip_dir <- dir_create(here("us", "assist", "data", "zip"))
base_url <- "https://files.usaspending.gov/award_data_archive/"
fin_files <- "FY2020_All_Assistance_Full_20201008.zip"
fin_urls <- str_c(base_url, fin_files)
fin_zips <- path(zip_dir, fin_files)
```

```{r echo=FALSE}
head(fin_urls, 3)
path.abbrev(fin_zips)
```

We also need to add the records for spending and corrections made since this
file was last updated. This is information is crucial, as it contains the most
recent data. This information can be found in the "delta" file released
alongside the "full" spending files.

> New files are uploaded by the 15th of each month. Check the Data As Of column
to see the last time files were generated. Full files feature data for the
fiscal year up until the date the file was prepared, and delta files feature
only new, modified, and deleted data since the date the last month's files were
generated. The `correction_delete_ind` column in the delta files indicates
whether a record has been modified (C), deleted (D), or added (blank). To
download data prior to FY 2008, visit our Custom Award Data page.

```{r delta_path}
delta_file <- "FY(All)_All_Assistance_Delta_20201008.zip"
delta_url <- str_c(base_url, delta_file)
delta_zip <- path(zip_dir, delta_file)
```

These files are large, so we might want to check their size before downloading.

```{r raw_sizes}
(fin_size <- tibble(
  url = basename(fin_urls),
  size = as_fs_bytes(map_dbl(fin_urls, url_file_size))
))
```

```{r speed_test, eval=FALSE}
# remotes::install_github("hrbrmstr/speedtest")
if (require(speedtest)) {
  config <- speedtest::spd_config()
  servers <- speedtest::spd_servers(config = config)
  closest_servers <- speedtest::spd_closest_servers(servers, config = config)
  speed <- speedtest::spd_download_test(closest_servers[1, ], config = config)
  # use median results
  speed[, 11:15]
  # minutes to download
  ((sum(fin_size$size)/1e+6) / (speed$median/8))/60
}
```

If the archive files have not been downloaded, we can do so now. 

```{r raw_download}
if (!all(file_exists(c(fin_zips, delta_zip)))) {
  download.file(fin_urls, fin_zips)
  download.file(delta_url, delta_zip)
}
```

## Extract

We can extract the text files from the annual archives into a new directory.

```{r extract, results='hide'}
raw_dir <- dir_create(here("us", "assist", "data", "raw"))
if (length(dir_ls(raw_dir)) == 0) {
  unzip(fin_zips, exdir = raw_dir)
  unzip(delta_zip, exdir = raw_dir)
}
```

```{r paths}
fin_paths <- dir_ls(raw_dir, regexp = "FY\\d+.*csv")
delta_paths <- dir_ls(raw_dir, regexp = "FY\\(All\\).*csv")
```

```{r dir_info}
(raw_info <- dir_info(raw_dir) %>% 
  select(path, size, modification_time) %>% 
  mutate(across(path, path.abbrev)))
```

## Layout

The USA Spending website also provides a comprehensive data dictionary which
covers the many variables in this file.

```{r dictionary}
dict_file <- path(here("us", "assist", "data"), "dict.xlsx")
if (!file_exists(dict_file)) {
  download.file(
    url = "https://files.usaspending.gov/docs/Data_Dictionary_Crosswalk.xlsx",
    destfile = dict_file
  )
}
dict <- read_excel(
  path = dict_file, 
  range = "A2:L414",
  na = "N/A",
  sheet = "Public",
  .name_repair = make_clean_names
)

usa_names <- names(vroom(fin_paths[which.min(file_size(fin_paths))], n_max = 0))
# get cols from hhs data
mean(usa_names %in% dict$award_element)
dict %>% 
  filter(award_element %in% usa_names) %>% 
  select(award_element, definition) %>% 
  mutate_at(vars(definition), str_replace_all, "\"", "\'") %>% 
  arrange(match(award_element, usa_names)) %>% 
  head(10) %>% 
  mutate_at(vars(definition), str_trunc, 75) %>% 
  kable()
```

```{r include=FALSE}
rm(dict, usa_names, dict_file)
```

## Read

Due to the sheer size and number of files in question, we can't read them all at
once into a single data file for exploration and wrangling. 

```{r fin_size}
length(fin_paths)
# total file sizes
sum(file_size(fin_paths))
# avail local memory
as_fs_bytes(str_extract(system("free", intern = TRUE)[2], "\\d+"))
```

What we will instead do is read each file individually and perform the type of
exploratory analysis we need to ensure the data is well structured and normal.
This will be done with a lengthy `for` loop and appending the checks to a new
text file on disk.

We can append the checks from the new updated files onto the checks from the
entire database (going back to 2008) and separate them with a comment line.

```{r check_comment}
spend_path <- here("us", "assist", "spend_check.csv")
write_lines("###### updates", spend_path, append = TRUE)
```

We are not going to use the delta file to correct, delete, and update the
original transactions. We are instead going to upload the separately so that
the changed versions appear alongside the original in all search results. We
will tag all records with the file they originate from.

```{r fin_read}
# track progress in text file
prog_path <- file_create(here("us", "assist", "read_prog.txt"))
for (f in c(fin_paths, delta_paths)) {
  prog_files <- read_lines(prog_path)
  n <- str_remove(basename(f), "_All_Assistance_(Full|Delta)_\\d+")
  if (f %in% prog_files) {
    message(paste(n, "already done"))
    next()
  } else {
    message(paste(n, "starting"))
  }
  # read contracts ------------------------------------------------------------
  usc <- vroom(
    file = f,
    delim = ",",
    guess_max = 0,
    escape_backslash = FALSE,
    escape_double = FALSE,
    progress = FALSE,
    id = "file",
    num_threads = 1,
    col_types = cols(
      .default = col_character(),
      action_date_fiscal_year = col_integer(),
      action_date = col_date(),
      federal_action_obligation = col_double()
    )
  )
  usc <- select(
    .data = usc,
    key = assistance_award_unique_key,
    piid = award_id_fain,
    fiscal = action_date_fiscal_year,
    date = action_date,
    amount = federal_action_obligation,
    agency = awarding_agency_name,
    sub_id = awarding_sub_agency_code,
    sub_agency = awarding_sub_agency_name,
    office = awarding_office_name,
    rec_id = recipient_duns,
    address1 = recipient_address_line_1,
    address2 = recipient_address_line_2,
    city = recipient_city_name,
    state = recipient_state_code,
    zip = recipient_zip_code,
    place = primary_place_of_performance_zip_4,
    type = assistance_type_code,
    desc = award_description,
    file,
    everything()
  )
  # tweak cols ---------------------------------------------------------------
  usc <- mutate( # create single recip col
    .data = usc,
    .after = "rec_id", 
    file = basename(file),
    rec_name = coalesce(
      recipient_name,
      recipient_parent_name
    )
  )
  usc <- flag_na(usc, date, amount, sub_agency, rec_name) # flag missing vals
  usc <- mutate_at(usc, vars("zip", "place"), str_sub, end = 5) # trim zip
  usc <- mutate(usc, year = year(date), .after = "fiscal") # add calendar year
  flush_memory()
  # if delta remove rows
  if ("correction_delete_ind" %in% names(usc)) {
    usc <- rename(usc, change = correction_delete_ind)
    usc <- relocate(usc, change, .after = "file")
    usc <- filter(usc, change != "D" | is.na(change))
  }
  # save checks --------------------------------------------------------------
  if (n_distinct(usc$fiscal) > 1) {
    fy <- NA_character_
  } else {
    fy <- unique(usc$fiscal)
  }
  check <- tibble(
    file = n,
    nrow = nrow(usc),
    ncol = ncol(usc),
    types = n_distinct(usc$type),
    fiscal = fy,
    start = min(usc$date, na.rm = TRUE),
    end = max(usc$date, na.rm = TRUE),
    miss = sum(usc$na_flag, na.rm = TRUE),
    zero = sum(usc$amount <= 0, na.rm = TRUE),
    city = round(prop_in(usc$city, c(valid_city, extra_city)), 4),
    state = round(prop_in(usc$state, valid_state), 4),
    zip = round(prop_in(usc$zip, valid_zip), 4)
  )
  message(paste(n, "checking done"))
  vroom_write(x = usc, path = f, delim = ",", na = "") # save manipulated file
  write_csv(check[1, ], spend_path, append = TRUE) # save the checks as line 
  write_lines(f, prog_path, append = TRUE) # save the file as line
  # reset for next
  rm(usc, check) 
  flush_memory(n = 10)
  p <- paste(match(f, fin_paths), length(fin_paths), sep = "/")
  message(paste(n, "writing done:", p, file_size(f)))
  # beepr::beep("fanfare")
  Sys.sleep(30)
}
```

## Check

In the end, `r length(dir_ls(raw_dir))` files were read and checked.

```{r check_files}
all_paths <- dir_ls(raw_dir)
length(all_paths)
sum(file_size(all_paths))
```

Now we can read the `spend_check.csv` text file to see the statistics saved
from each file.

```{r check_read, echo=FALSE}
all_checks <- read_csv(
  file = here("us", "assist", "spend_check.csv"),
  comment = "#",
  col_names = c(
    "file", "nrow", "ncol", "types", "fiscal", "start", 
    "end", "missing", "zero", "city", "state", "zip"
  )
)
```

We can `summarise()` across all files to find the typical statistic across all
raw data.

```{r check_summarise}
all_checks %>% 
  group_by(file) %>%
  arrange(desc(nrow)) %>% 
  slice(1) %>% 
  ungroup() %>% 
  summarise(
    nrow = sum(nrow),
    ncol = mean(ncol),
    type = mean(types),
    start = min(start),
    end = max(end),
    missing = sum(missing)/sum(nrow),
    zero = sum(zero)/sum(nrow),
    city = mean(city),
    state = mean(state),
    zip = mean(zip)
  )
```

```{r year_bar}
per_day <- all_checks %>% 
  group_by(file) %>% 
  arrange(desc(nrow)) %>% 
  slice(1) %>% 
  group_by(fiscal) %>% 
  summarise(nrow = sum(nrow)) %>% 
  mutate(per = nrow/365)
per_day$per[13] <- per_day$nrow[13]/yday(today())
per_day %>% 
  ggplot(aes(fiscal, per)) + 
  geom_col(fill = dark2["purple"]) +
  scale_y_continuous(labels = comma) +
  labs(
    title = "US Spending Transactions per day by year",
    subtitle = glue("As of {today()}"),
    x = "Fiscal Year",
    y = "Unique Transactions"
  )
```

And here we have the total checks for every file.

```{r check_print, echo=FALSE}
all_checks %>% 
  group_by(file) %>% 
  arrange(desc(nrow)) %>% 
  slice(1) %>% 
  ungroup() %>% 
  mutate(across(file, ~md_code(str_replace(., "us_contracts_", "FY")))) %>% 
  select(-types, -fiscal, -ncol) %>% 
  mutate(zero = zero/nrow) %>% 
  mutate(across(c("city", "state", "zip", "zero"), scales::percent, 0.1)) %>% 
  mutate(across(nrow, comma)) %>% 
  kable()
```

## Upload

```{r}
s3 <- get_bucket_df(bucket = "publicaccountability", prefix = "csv")
s3 <- as_tibble(s3) %>% 
  transmute(
    path = as_fs_path(Key),
    size = as_fs_bytes(Size),
    modification_time = parse_datetime(LastModified)
  )
```

```{r results='asis'}
md_order(str_subset(s3$path, "us_assist_2020"))
```

```{r results='asis'}
fin_rx <- "FY(\\d{4}|\\(All\\))_All_Assistance_\\w+_\\d+_(\\d+).csv"
s3_paths <- basename(all_paths) %>% 
    str_replace(fin_rx, "us_assist_\\1-\\2.csv") %>% 
    str_replace("\\(All\\)", "delta")
s3_paths <- path("csv", s3_paths)
md_order(s3_paths)
```

```{r upload}
for (i in seq_along(all_paths)[12:20]) {
  put_object(
    file = all_paths[i],
    object = s3_paths[i], 
    bucket = "publicaccountability",
    acl = "public-read",
    multipart = TRUE,
    show_progress = TRUE
  )
}
```
