---
title: "Illinois Contributions"
author: "Kiernan Nicholls"
date: "`r date()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 3
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
if (!interactive()) {
  options(width = 120)
  set.seed(5)
}
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
doc_dir <- fs::dir_create(here::here("il", "contribs", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give
journalists, policy professionals, activists, and the public at large a simple
way to search across huge volumes of public data about people and organizations.

Our goal is to standardize public data on a few key fields by thinking of each
dataset row as a transaction. For each transaction there should be (at least) 3
variables:

1. All **parties** to a transaction.
2. The **date** of the transaction.
3. The **amount** of money involved.

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for entirely duplicated records.
1. Check ranges of continuous variables.
1. Is there anything blank or missing?
1. Check for consistency issues.
1. Create a five-digit ZIP Code called `zip`.
1. Create a `year` field from the transaction date.
1. Make sure there is data on both parties to a transaction.

## Packages

The following packages are needed to collect, manipulate, visualize, analyze,
and communicate these results. The `pacman` package will facilitate their
installation and attachment.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) {
  install.packages("pacman")
}
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  gluedown, # printing markdown
  janitor, # clean data frames
  campfin, # custom irw tools
  aws.s3, # aws cloud storage
  refinr, # cluster & merge
  scales, # format strings
  knitr, # knit documents
  vroom, # fast reading
  rvest, # scrape html
  glue, # code strings
  here, # project paths
  httr, # http requests
  fs # local storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a
sub-directory of the more general, language-agnostic
[`irworkshop/accountability_datacleaning`][tap] GitHub repository.

The `R_campfin` project uses the [RStudio projects][rproj] feature and should be
run as such. The project also uses the dynamic `here::here()` tool for file
paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::i_am("il/contribs/docs/il_contribs_diary.Rmd")
```

[tap]: https://github.com/irworkshop/accountability_datacleaning
[rproj]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

## Data

Contribution data is available from the [Illinois State Board of Elections][ib].
The ILSBOE operates a [contributions search portal][search] where users can find
and export data on certain contributions. Data is also available in [bulk] from
the ILSBOE.

> ### Frequently Asked Questions
> #### What about campaign disclosure?**
> The Campaign Financing Act covers the public's right to know certain financial
information about candidates, elected officials and those individuals and groups
who are financially involved in political campaigns. The State Board of
Elections supervises the administration of the Illinois act and closely monitors
campaign expenditures which appear on reports submitted by candidates and
committees as required by law. These reports, detailing contributions and
expenditures, give the media and the public information on where candidates
received their campaign money and where it is being spent. Board hearings are
held if suspected or actual violations of the Campaign Financing Act occur. The
Board is authorized to levy fines and turn over evidence of wrongdoing to local
prosecutors.
>
> #### Is electronically filed data available on the Board of Elections website?
> Yes, the data is available in a searchable format. It may be accessed in a
number of ways by selecting from the different search options available. Search
tips are provided for each type of search. In addition, all itemized receipts
for statewide candidates, legislative candidates, and legislative leadership
committees for the period from July 1, 1994 through December 31, 1998, may be
searched.
>
> #### Is electronically filed data downloadable from the Board website?
> Contribution and expenditure data may be downloaded in either a Tab-Delimited
Text File or XML file. The data is also available at no cost from the Board on
cdrom.
>
> #### When is a political committee required to file electronically?
> Electronic filing is required for all political committees that during a
reporting period (i) had at any time a balance or an accumulation of
contributions of $10,000 or more, (ii) made aggregate expenditures of $10,000 or
more, or (iii) received loans of an aggregate of $10,000 or more. Once a
committee exceeds the threshold that requires it to report electronically, it
must continue thereafter to report electronically until it dissolves, whether or
not its accumulation, receipts or expenditures fall beneath the levels set by
statute for mandatory electronic filing.
>
> #### Who must file campaign disclosure reports?
> Any individual, trust, partnership, committee, association, corporation, or
any other organization or group of persons which receives or spends more than
$5,000 on behalf of or in opposition to a candidate or question of public
policy, meets the definition of a political committee and must comply with all
provisions of the Illinois Campaign Financing Act, including the filing of
campaign disclosure reports. The $5,000 threshold does not apply to political
party committees. In addition, any entity other than a natural person that makes
expenditures of any kind in an aggregate amount of more than $3,000 during any
12-month period supporting or opposing a public official or candidate must
organize as a political committee.

[ib]: https://elections.il.gov/Default.aspx
[search]: https://elections.il.gov/CampaignDisclosure/ContributionSearchByAllContributions.aspx

## Download

The campaign finance database is hosted at `/campaigndisclosuredatafiles`.
There, we can see the 13 files available for download.

```{r raw_html}
il_home <- read_html("https://elections.il.gov/campaigndisclosuredatafiles/")
```

```{r raw_ls, echo=FALSE}
il_ls <- read_fwf(
  file = I(html_text2(html_element(il_home, "pre"))),
  skip = 1,
  col_positions = fwf_widths(
    c(19, 13, NA), 
    c("date", "length", "name")
  ),
  col_types = cols(
    date = col_datetime("%m/%d/%Y  %H:%M %p")
  )
)
```

This table of files includes the date, size, name and URL.

```{r raw_href, echo=FALSE}
raw_href <- html_attr(html_elements(il_home, "a"), "href")
raw_url <- paste0("https://elections.il.gov", raw_href)[-1]
```

```{r echo=FALSE, results='asis'}
il_ls %>% 
  mutate(
    across(length, fs_bytes),
    across(name, md_code),
    across(name, md_link, raw_url)
  ) %>% 
  kable()
```

### Dictionary

The [`CampaignDisclosureDataDictionary.txt`][readme] file contains the columns
within each of the available files. We are interested in the `Receipts.txt` file
that contains all campaign contributions.

```{r readme, echo=FALSE}
readme <- read_lines(str_subset(raw_url, "Dictionary"))
readme[238] <- str_remove(readme[238], "\\s\\((.*)\\)")
receipts_readme <- read_tsv(
  file = I(str_replace(readme[221:249], "\t+", "\t")),
  col_names = c("column", "description"),
  col_types = cols(.default = col_character())
)
receipts_readme %>% 
  mutate(across(column, md_code)) %>% 
  kable()
```

[readme]: https://elections.il.gov/campaigndisclosuredatafiles/CampaignDisclosureDataDictionary.txt

### Receipts

```{r raw_dir}
raw_dir <- dir_create(here("il", "contribs", "data", "raw"))
raw_txt <- path(raw_dir, str_subset(il_ls$name, "Receipts"))
```

```{r raw_download}
if (!file_exists(raw_txt)) {
  download.file(
    url = str_subset(raw_url, "Receipts"),
    destfile = raw_txt,
    method = "curl"
  )
}
```

## Fix

There are 3 problems within the `Receipts.txt` text file:
1. There are two instances of a line being erroneously split in the middle and
spread across 7 lines with information repeated in the middle 5 lines.
2. There are two instances of a name ending with `\n` causing that line to be
split across two lines.
3. There is one address with two extra `\t` delimiters between the street number
and street type.

Presuming the `Receipts.txt` file is the same one (or at least in the same
order) as the one downloaded today (September 16, 2021), then we can fix these
issues manually, removing the bad lines, and saving the fixed lines to a new
text file.

```{r fix_txt}
fix_txt <- here("il", "contribs", "data", "Receipts-fix.txt")
fixes <- rep(FALSE, 4)
if (!file_exists(fix_txt)) {
  x <- read_lines(raw_txt)
  Sys.sleep(5)
  
  # middle newline, repeated column in split middle
  if (str_starts(x[4170845], "\\d", negate = TRUE)) {
    x[4170839] <- paste(x[4170839], x[4170845], sep = "\t")
    fixes[1] <- TRUE
  }
  
  if (str_starts(x[4193501], "\\d", negate = TRUE)) {
    x[4193495] <- paste(x[4193495], x[4193501], sep = "\t")
    fixes[2] <- TRUE
  }

  # newline in name
  if (all(str_starts(x[c(4351744, 4377250)], "\\d", negate = TRUE))) {
    x[4351743] <- paste0(x[4351743], x[4351744])
    x[4377249] <- paste0(x[4377249], x[4377250])
    fixes[3] <- TRUE
  }

  # two tabs within address
  if (str_count(x[5452831], "\t") > 28) {
    x[5452831] <- str_replace(
      string = x[5452831],
      pattern = "672\tS Lincoln\tAve",
      replacement = "672 S Lincoln Ave"
    )
    fixes[4] <- TRUE
  }
  
  
  # remove bad lines
  if (all(fixes)) {
    bad_rows <- c(
      4170840:4170845, # fix one
      4193496:4193501, # fix two
      4351744,         # fix three
      4377250          # fix four
    )
    x <- x[-bad_rows]
  }
  write_lines(x, fix_txt)
}
```

```{r include=FALSE}
if (exists("x")) rm(x)
Sys.sleep(3)
flush_memory()
```

## Read

We can read the manually fixed tab-delimited file.

```{r raw_read}
ilc <- read_delim(
  file = fix_txt,
  delim = "\t",
  quote = "",
  escape_backslash = FALSE,
  escape_double = FALSE,
  trim_ws = TRUE,
  col_types = cols(
    .default = col_character(),
    ID = col_integer(),
    CommitteeID = col_integer(),
    FiledDocID = col_integer(),
    RcvDate = col_datetime(),
    Amount = col_double(),
    AggregateAmount = col_double(),
    LoanAmount = col_double(),
    Archived = col_logical(),
    RedactionRequested = col_logical()
  )
)
```

```{r}
problems(ilc)
```

To ensure the file was properly read, we should count the distinct values of a
discrete variable like the logical `RedactionRequested` column, which should
only contain `TRUE` and `FALSE` values.

```{r read_check}
count(ilc, RedactionRequested)
```

```{r clean_names}
ilc <- clean_names(ilc, case = "snake")
```

## Committees

The contribution records contain the name and address of the contributor but
only identify the receiving committee with an `committee_id` variable. We can
use that `committee_id` to join against the `Committees.txt` database.

```{r cmt_download}
cmt_txt <- path(raw_dir, str_subset(il_ls$name, "Committees"))
if (!file_exists(cmt_txt)) {
  download.file(
    url = str_subset(raw_url, "Committees"),
    destfile = cmt_txt
  )
}
```

```{r cmt_read}
committees <- read_delim(
  file = cmt_txt,
  delim = "\t",
  quote = "",
  escape_backslash = FALSE,
  escape_double = FALSE,
  trim_ws = TRUE,
  col_types = cols(
    .default = col_character(),
    ID = col_integer(),
    StateCommittee = col_logical(),
    StateID = col_integer(),
    LocalCommittee = col_logical(),
    LocalID = col_integer(),
    StatusDate = col_datetime(),
    CreationDate = col_datetime(),
    CreationAmount = col_double(),
    DispFundsReturn = col_logical(),
    DispFundsPolComm = col_logical(),
    DispFundsCharity = col_logical(),
    DispFunds95 = col_logical()
  )
)
```

```{r cmt_snake}
committees <- clean_names(committees, case = "snake")
```

We are only interested in the columns which identify the receiving committee by
name and address so they can be easily searched.

```{r cmt_select}
committees <- committees %>% 
  select(id, name, starts_with("address"), city, state, zip)
```

#### Wrangle

Before we join the tables, we are going to normalize the geographic variables of
the address. The explanation for this process is detailed in the `Wrangle`
section below.

```{r cmte_addr_norm}
committees <- committees %>% 
  unite(
    col = address,
    starts_with("address"),
    sep = " ",
    remove = TRUE,
    na.rm = TRUE
  ) %>% 
  mutate(
    address = normal_address(
      address = address,
      abbs = usps_street,
      na_rep = TRUE
    )
  )
```

```{r cmte_zip_norm}
committees$zip <- normal_zip(
  zip = committees$zip,
  na_rep = TRUE,
  na = ""
)
```

```{r cmte_st_norm}
il_zip <- zipcodes$zip[zipcodes$state == "IL"]
committees$state[committees$state == "L" & committees$zip %in% il_zip] <- "IL"
committees$state[committees$state == "O:" & committees$zip %in% il_zip] <- "IL"
committees$state <- normal_state(
  state = committees$state,
  valid = valid_state
)
```

```{r cmte_city_swap}
committees <- committees %>% 
  mutate(
    city = normal_city(
      city = city, 
      abbs = usps_city,
      states = c("IL", "DC", "ILLINOIS"),
      na = invalid_city,
      na_rep = TRUE
    )
  ) %>% 
  left_join(
    y = zipcodes,
    by = c("state", "zip"),
    suffix = c("_norm", "_match")
  ) %>% 
  mutate(
    match_abb = is_abbrev(city_norm, city_match),
    match_dist = str_dist(city_norm, city_match),
    city_swap = if_else(
      condition = !is.na(match_dist) & (match_abb | match_dist == 1),
      true = city_match,
      false = city_norm
    ),
    .after = city_norm
  ) %>% 
  select(
    -match_abb,
    -match_dist,
    -city_norm,
    -city_match
  ) %>% 
  rename(city = city_swap)
```

#### Join

These columns can be joined with every contribution. We will identify columns
from the `Committees.txt` column with the `cmte_` prefix.

```{r cmte_rename}
committees <- rename_with(
  .data = committees,
  .fn = ~paste0("cmte_", .),
  .cols = -id
)
```

```{r cmte_join}
ilc <- left_join(
  x = ilc,
  y = committees,
  by = c("committee_id" = "id")
)
```

## Explore

There are `r comma(nrow(ilc))` rows of `r ncol(ilc)` columns. Each record
represents a single contribution from an individual or business to a campaign
or committee.

```{r glimpse}
glimpse(ilc)
tail(ilc)
```

### Missing

Columns vary in their degree of missing values.

```{r na_count}
col_stats(ilc, count_na)
```

We can flag any record missing a key variable needed to identify a transaction.

```{r na_flag}
key_vars <- c("rcv_date", "last_only_name", "amount", "cmte_name")
ilc <- flag_na(ilc, all_of(key_vars))
sum(ilc$na_flag)
```

`r percent(mean(ilc$na_flag), 0.1)` rows are missing a key variable.

```{r}
if (sum(ilc$na_flag) == 0) {
  ilc <- select(ilc, -na_flag)
}
```

### Duplicates

We can also flag any entirely duplicate rows. To keep memory usage low with such
a large data frame, we will split our data into a list and check each element of
the list. For each chunk, we will write the duplicate `id` to a text file.

```{r}
prop_distinct(ilc$id)
```

```{r dupe_write}
dupe_file <- here("ny", "contribs", "data", "dupe_ids.txt")
if (!file_exists(dupe_file)) {
  tmp <- file_temp(ext = "rds")
  write_rds(ilc, file = tmp)
  file_size(tmp)
  il_id <- split(ilc$id, ilc$rcv_date)
  ils <- ilc %>%
    select(-id) %>% 
    group_split(rcv_date)
  if (file_exists(tmp)) {
    rm(ilc)
    Sys.sleep(5)
    flush_memory(2)
  }
  ils <- ils[map_lgl(ils, function(x) nrow(x) > 1)]
  pb <- txtProgressBar(max = length(ils), style = 3)
  for (i in seq_along(ils)) {
    d1 <- duplicated(ils[[i]], fromLast = FALSE)
    d2 <- duplicated(ils[[i]], fromLast = TRUE)
    dupe_vec <- d1 | d2
    rm(d1, d2)
    if (any(dupe_vec)) {
      write_lines(
        x = il_id[[i]][dupe_vec], 
        file = dupe_file, 
        append = file_exists(dupe_file),
        na = ""
      )
    }
    rm(dupe_vec)
    ils[[i]] <- NA
    il_id[[i]] <- NA
    if (i %% 100 == 0) {
      Sys.sleep(2)
      flush_memory()
    }
    setTxtProgressBar(pb, i)
  }
  rm(ils, il_id)
  Sys.sleep(5)
  flush_memory()
  ilc <- read_rds(tmp)
}
```

```{r dupe_add}
dupe_id <- tibble(
  id = as.integer(read_lines(dupe_file, skip_empty_rows = TRUE)),
  dupe_flag = TRUE
)
ilc <- left_join(ilc, dupe_id, by = "id")
ilc <- mutate(ilc, across(dupe_flag, Negate(is.na)))
```

`r percent(mean(ilc$dupe_flag), 0.1)` of rows are duplicates.

```{r dupe_view}
ilc %>% 
  filter(dupe_flag) %>% 
  count(rcv_date, last_only_name, amount, cmte_name, sort = TRUE)
```

### Categorical

```{r distinct_count}
col_stats(ilc, n_distinct)
```

```{r distinct_plots, echo=FALSE}
explore_plot(ilc, d2part)
explore_plot(ilc, archived)
```

### Amounts

```{r amount_summary}
summary(ilc$amount)
mean(ilc$amount <= 0)
```

These are the records with the minimum and maximum amounts.

```{r amount_minmax}
glimpse(ilc[c(which.max(ilc$amount), which.min(ilc$amount)), ])
```

```{r hist_amount, echo=FALSE}
ilc %>%
  filter(amount >= 1, amount <= 1e6) %>% 
  ggplot(aes(amount)) +
  geom_histogram(fill = dark2["purple"]) +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Illinois Contributions Amount Distribution",
    caption = "Source: Illinois State Board of Elections ",
    x = "Amount",
    y = "Count"
  )
```

### Dates

We can add the calendar year from `date` with `lubridate::year()`

```{r date_year}
ilc <- mutate(ilc, rcv_year = year(rcv_date))
```

```{r date_range}
min(ilc$rcv_date)
sum(ilc$rcv_year < 1994)
max(ilc$rcv_date)
sum(ilc$rcv_date > today())
```

```{r bar_year, echo=FALSE}
ilc %>% 
  count(rcv_year) %>% 
  filter(rcv_year >= 1994) %>% 
  mutate(even = is_even(rcv_year)) %>% 
  ggplot(aes(x = rcv_year, y = n)) +
  geom_col(aes(fill = even)) + 
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(breaks = seq(1994, 2020, by = 2)) +
  theme(legend.position = "bottom") +
  labs(
    title = "Illinois Contributions by Year",
    caption = "Source: Illinois State Board of Elections ",
    fill = "Election Year",
    x = "Year Made",
    y = "Count"
  )
```

## Wrangle

To improve the searchability of the database, we will perform some consistent,
confident string normalization. For geographic variables like city names and
ZIP codes, the corresponding `campfin::normal_*()` functions are tailor made to 
facilitate this process.

### Address

For the street `addresss` variable, the `campfin::normal_address()` function
will force consistence case, remove punctuation, and abbreviate official 
USPS suffixes.

```{r address_norm}
norm_addr <- ilc %>% 
  distinct(address1, address2) %>% 
  unite(
    col = address_full,
    starts_with("address"),
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(
    address_norm = normal_address(
      address = address_full,
      abbs = usps_street,
      na_rep = TRUE
    )
  ) %>% 
  select(-address_full)
```

```{r address_view}
sample_n(norm_addr, 10)
```

```{r address_join}
ilc <- left_join(ilc, norm_addr, by = c("address1", "address2"))
```

```{r include=FALSE}
rm(norm_addr)
flush_memory()
```

### ZIP

For ZIP codes, the `campfin::normal_zip()` function will attempt to create
valid _five_ digit codes by removing the ZIP+4 suffix and returning leading
zeroes dropped by other programs like Microsoft Excel.

```{r zip_norm}
ilc <- ilc %>% 
  mutate(
    zip_norm = normal_zip(
      zip = zip,
      na_rep = TRUE
    )
  )
```

```{r zip_progress}
progress_table(
  ilc$zip,
  ilc$zip_norm,
  compare = valid_zip
)
```

### State

Valid two digit state abbreviations can be made using the 
`campfin::normal_state()` function.

```{r state_norm}
st_norm <- ilc %>% 
  distinct(state) %>% 
  mutate(
    state_norm = normal_state(
      state = state,
      abbreviate = TRUE,
      na_rep = TRUE,
      valid = valid_state
    )
  )
```

```{r state_count}
st_norm %>% 
  filter(state != state_norm)
```

```{r state_join}
ilc <- left_join(ilc, st_norm, by = "state")
```

```{r state_progress}
progress_table(
  ilc$state,
  ilc$state_norm,
  compare = valid_state
)
```

### City

Cities are the most difficult geographic variable to normalize, simply due to
the wide variety of valid cities and formats.

#### Normal

The `campfin::normal_city()` function is a good start, again converting case,
removing punctuation, but _expanding_ USPS abbreviations. We can also remove
`invalid_city` values.

```{r city_norm}
norm_city <- ilc %>% 
  distinct(city, state_norm, zip_norm) %>% 
  mutate(
    city_norm = normal_city(
      city = city, 
      abbs = usps_city,
      states = c("IL", "DC", "ILLINOIS"),
      na = invalid_city,
      na_rep = TRUE
    )
  )
```

#### Swap

We can further improve normalization by comparing our normalized value
against the _expected_ value for that record's state abbreviation and ZIP code.
If the normalized value is either an abbreviation for or very similar to the
expected value, we can confidently swap those two.

```{r city_swap}
norm_city <- norm_city %>% 
  rename(city_raw = city) %>% 
  left_join(
    y = zipcodes,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(city_norm, city_match),
    match_dist = str_dist(city_norm, city_match),
    city_swap = if_else(
      condition = !is.na(match_dist) & (match_abb | match_dist == 1),
      true = city_match,
      false = city_norm
    )
  ) %>% 
  select(
    -city_match,
    -match_dist,
    -match_abb
  )
```

```{r city_rejoin}
ilc <- left_join(
  x = ilc,
  y = norm_city,
  by = c(
    "city" = "city_raw", 
    "state_norm", 
    "zip_norm"
  )
)
```

#### Refine

The [OpenRefine][or] algorithms can be used to group similar strings and replace
the less common versions with their most common counterpart. This can greatly
reduce inconsistency, but with low confidence; we will only keep any refined
strings that have a valid city/state/zip combination.

[or]: https://openrefine.org/

```{r city_refine}
good_refine <- ilc %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>% 
  filter(city_refine != city_swap) %>% 
  inner_join(
    y = zipcodes,
    by = c(
      "city_refine" = "city",
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  )
```

```{r city_count, echo=FALSE}
good_refine %>%
  count(
    state_norm, 
    zip_norm, 
    city_swap, 
    city_refine,
    sort = TRUE
  )
```

Then we can join the refined values back to the database.

```{r city_join}
ilc <- ilc %>% 
  left_join(good_refine, by = names(.)) %>% 
  mutate(city_refine = coalesce(city_refine, city_swap))
```

#### Progress

Our goal for normalization was to increase the proportion of city values known
to be valid and reduce the total distinct values by correcting misspellings.

```{r city_progress, echo=FALSE}
many_city <- c(valid_city, extra_city)
progress <- progress_table(
  str_to_upper(ilc$city),
  ilc$city_norm,
  ilc$city_swap,
  ilc$city_refine,
  compare = many_city
) %>% mutate(stage = as_factor(stage))
kable(progress, digits = 3)
```

You can see how the percentage of valid values increased with each stage.

```{r bar_progress, echo=FALSE}
raw_in <- percent(prop_in(ilc$city, valid_city))
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = dark2["purple"]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Illinois City Normalization Progress",
    subtitle = glue("Raw at {raw_in} before conversion to uppercase"),
    x = "Stage",
    y = "Percent Valid"
  )
```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivalent.

```{r bar_distinct, echo=FALSE}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Illinois City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Distinct Values",
    fill = "Valid"
  )
```

Before exporting, we can remove the intermediary normalization columns and
rename all added variables with the `_clean` suffix.

```{r clean_select}
ilc <- ilc %>% 
  select(
    -city_norm,
    -city_swap,
    city_clean = city_refine
  ) %>% 
  rename_all(~str_replace(., "_norm", "_clean")) %>% 
  rename_all(~str_remove(., "_raw")) %>% 
  relocate(city_clean, state_clean, zip_clean, .after = address_clean)
```

```{r clean_glimpse}
glimpse(sample_n(ilc, 50))
```

## Conclude

1. There are `r comma(nrow(ilc))` records in the database.
1. There are `r comma(sum(ilc$dupe_flag))` duplicate records in the database.
1. The range and distribution of `amount` and `date` seem reasonable.
1. There are `r comma(sum(ilc$na_flag))` records missing key variables.
1. Consistency in geographic data has been improved with `campfin::normal_*()`.
1. The 4-digit `year` variable has been created with `lubridate::year()`.

## Export

Now the file can be saved on disk for upload to the Accountability server.

```{r clean_dir}
clean_dir <- dir_create(here("il", "contribs", "data", "clean"))
clean_path <- path(clean_dir, "il_contribs_19940701-20210915.csv")
write_csv(ilc, clean_path, na = "")
(clean_size <- file_size(clean_path))
```

## Upload

We can use the `aws.s3::put_object()` to upload the text file to the IRW server.

```{r aws_upload, eval=FALSE}
aws_path <- path("csv", basename(clean_path))
if (!object_exists(aws_path, "publicaccountability")) {
  put_object(
    file = clean_path,
    object = aws_path, 
    bucket = "publicaccountability",
    acl = "public-read",
    show_progress = TRUE,
    multipart = TRUE
  )
}
aws_head <- head_object(aws_path, "publicaccountability")
(aws_size <- as_fs_bytes(attr(aws_head, "content-length")))
unname(aws_size == clean_size)
```
