---
title: "Michigan Expenditures"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 9,
  fig.height = 5,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, warning=FALSE, error=FALSE}
pacman::p_load_gh("VerbalExpressions/RVerbalExpressions")
pacman::p_load_current_gh("irworkshop/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # text mining tools
  magrittr, # pipe opperators
  janitor, # dataframe clean
  zipcode, # clean & databse
  batman, # parse logicals
  refinr, # cluster & merge
  scales, #format strings
  rvest, # scrape website
  skimr, # summary stats
  vroom, # quickly read
  glue, # combine strings
  gluedown, #markdown
  here, # locate storage
  fs # search storage 
)
```

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

The data is obtained from the [Michigan Secretary of State's website][03].

[03]: https://miboecfr.nictusa.com/cfr/dumpall/cfrdetail/

### Variables

The [`cfrdetail/ReadMe_EXPENDITURES.html`][04] file provides a table of variable descriptions.

[04]: https://miboecfr.nictusa.com/cfr/dumpall/cfrdetail/ReadMe_EXPENDITURES.html

```{r readme, results='asis', echo=FALSE}
download.file(
  url = "https://miboecfr.nictusa.com/cfr/dumpall/cfrdetail/ReadMe_EXPENDITURES.html",
  destfile = here("state","mi", "docs", "ReadMe_EXPENDITURES.html")
)

read_html("https://miboecfr.nictusa.com/cfr/dumpall/cfrdetail/ReadMe_EXPENDITURES.html") %>%
  html_node("table") %>% 
  html_table() %>% 
  as_tibble() %>%
  mutate(X1 = glue("`{X1}`")) %>% 
  kable(
    format = "markdown",
    col.names = c("Variable", "Description")
  )
```

## Import

As the [`cfrdetail/ReadMe_EXPENDITURES.html`][04] file also explains:

> Record layout of expenditures. Files are named by statement year. Larger files are split and
numbered to make them easier to work with. In these cases the column header row will only exist in
the first (00) file.

No expenditure files are large enough to be split, so we can simply create a vector of URLs by
`glue()`-ing the standard format with each year.

```{r make_urls}
urls <- glue("https://miboecfr.nictusa.com/cfr/dumpall/cfrdetail/{2008:2022}_mi_cfr_expenditures.zip")
```

### Download

Then, if the files haven't already been downloaded we can download each file to the raw directory. The files were downloaded Nov 6, 2022. 

```{r create_raw}
raw_dir <- here("state","mi", "expends", "data", "raw")
dir_create(raw_dir)
```

```{r download_raw}

if (!all_files_new(raw_dir, "*.zip$")) {
  for (year_url in urls) {
    download.file(
      url = year_url,
      destfile = glue("{raw_dir}/{basename(year_url)}")
    ) 
  }
}
```

### Read

Since the `readr::read_delim()` function can read ZIP files, we don't need to unzip. We can read
each file into a list using `purrr::map()` and then bind them into a single list after removing
empty columns.

```{r}
mi_names <- str_split(read_lines(dir_ls(raw_dir)[1])[1], "\t")[[1]]
mi_names <- mi_names[-length(mi_names)]
mi_names[length(mi_names)] <- "runtime"

mi_names[29] <- "office"
```


```{r read_raw}
mi <-  vroom(
  file = dir_ls(raw_dir),
  delim = "\t",
  skip = 1, # header
  col_names = mi_names,
  col_types = cols(
    .default = col_character(),
    page_no = col_integer(),
    doc_stmnt_year = col_integer(),
    received_date = col_date_mdy(),
    amount = col_double(),
    aggregate = col_double(),
    runtime = col_skip()
  )
)
```

```{r fix address line reading errors}
read_errors <- mi %>% filter(is.na(as.numeric(amount)))

wrong_posit <- which(mi_names =="address"):(length(mi_names)-2)
read_errors[wrong_posit] <- read_errors[wrong_posit+1] 

read_errors <- read_errors %>% mutate(unique_id = paste0(expense_id,doc_stmnt_year), exp_date = NA_character_,
                                      amount = as.numeric(amount))

#read_errors[read_errors$expense_id == "9280",wrong_posit] <- read_errors[read_errors$expense_id == "9280",wrong_posit+1]
mi <- mi %>% mutate(unique_id = paste0(expense_id,doc_stmnt_year))

mi[match(read_errors$unique_id,mi$unique_id),wrong_posit] <- read_errors[,wrong_posit]
mi <- mi %>% select(-unique_id)
```


## Explore

```{r glimpse}
head(mi)
tail(mi)
glimpse(sample_frac(mi))
```

### Update
For next update: Since the data contains a unique document sequence number, we can filter out all the rows with a sequence number that already appeared in our last update.

### Missing

As we know from the README, the `com_legal_name` variable represents who is making the expenditure
and has `r percent(mean(is.na(mi$com_legal_name)))` missing values. The `l_name` variable
represents the "Last name of the individual OR the organization name receiving the expenditure;"
this variable, on the other hand, is `r percent(mean(is.na(mi$lname_or_org)))` missing 
(`r sum(is.na(mi$lname_or_org))` records).

```{r count_na}
col_stats(mi, count_na)
```

Any record missing the variables needed to identify both parties of the transaction can be flagged
with a new `na_flag` variable.

```{r flag_na, collapse=TRUE}
mi <- mutate(mi, na_flag = is.na(com_legal_name) | is.na(lname_or_org) | is.na(amount))
sum(mi$na_flag)
```

### Duplicates

While there are zero completely duplicated records, there are a number that are duplicated save
for the `expense_id` variable.

```{r distinct_records}
nrow(mi) - nrow(distinct(select(mi, -expense_id)))
flag_dupes(mi, setdiff(names(mi),"expense_id"))
```

### Categorical

For categorical variables, we can explore the degree of distinctness and the distribution of these
variables.

```{r n_distinct}
col_stats(mi, n_distinct)
```

```{r exp_type_bar, echo=FALSE}
explore_plot(
  data = mi,
  var = expenditure_type,
) + 
  ggtitle("Michigan Expenditures Count by Type")
```

```{r com_type_bar, echo=FALSE}
explore_plot(
  data = mi,
  var = com_type,
) + ggtitle("Michigan Expenditures Count by Committee Type")
```

```{r sched_bar, echo=FALSE}
explore_plot(
  data = mi,
  var = schedule_desc,
) + ggtitle("Michigan Expenditures Count by Schedule")
```

```{r desc_bar, echo=FALSE}
angle_axis <- function(angle = 20, hjust = 1, ...) {
  theme(axis.text.x = element_text(angle = angle, hjust = hjust, ...))
}
top <- head(pull(drop_na(count(mi, exp_desc, sort = TRUE)), exp_desc), 7)
mi %>% 
  mutate(exp_desc = if_else(exp_desc %in% c(NA, top), exp_desc, "OTHER")) %>% 
  count(exp_desc, sort = TRUE) %>% 
  mutate(p = n/sum(n)) %>% 
  drop_na() %>% 
  ggplot(aes(x = reorder(exp_desc, p), y = p)) +
  geom_col(aes(fill = exp_desc)) +
  scale_y_continuous(labels = percent) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = "none"
  ) +
  labs(
    title = "Michigan Expenditures Count by Description",
    x = "Expenditure Description",
    y = "Percent"
  ) +
  angle_axis()
```

### Continuous

For continuous variables, we can explore the range and distribution of values.

#### Amounts

```{r range_amount}
mi$amount <- as.numeric(mi$amount)
summary(mi$amount)
```

```{r amount_hist, echo=FALSE}
mi %>% 
  ggplot(aes(amount)) +
  geom_histogram() +
  scale_x_continuous(
    trans = "log10",
    labels = dollar
  ) +
  labs(
    title = "Michigan Expenditure Amount Distribution",
    x = "Expenditure Amount",
    y = "Count"
  )
```

```{r amount_box_exp_type, echo=FALSE}
mi %>% 
  ggplot(
    mapping = aes(
      x = expenditure_type, 
      y = amount
    )
  ) +
  geom_boxplot(
    mapping  = aes(fill = expenditure_type), 
    varwidth = TRUE,
    outlier.alpha = 0.01
  ) +
  scale_fill_brewer(
    type    = "qual",
    palette = "Set1",
    guide   = FALSE
  ) +
  scale_y_continuous(
    trans  = "log10",
    labels = dollar
  ) +
  labs(
    title = "Michigan Expenditure Amount Ranges",
    x     = "Expenditure Type",
    y     = "Count"
  )
```

#### Dates

From the minimum and maximum, we can see that the `date` variable is not exactly clean.

```{r date_minmax, collapse=TRUE}
mi$exp_date <- as.Date(mi$exp_date, format = "%m/%d/%Y")

min(mi$exp_date, na.rm = TRUE)
max(mi$exp_date, na.rm = TRUE)
```

We can create a `exp_year` variable from `exp_date` using `lubridate::year()`.

```{r add_year, collapse=TRUE}
mi <- mutate(mi, exp_year = year(exp_date))
sum(mi$exp_year < 2006, na.rm = TRUE)
sum(mi$exp_date > today(), na.rm = TRUE)
```

We can nullify these few invalid dates in a new `date_clean` variable and flag those changed
records with `data_flag`

```{r clean_date, collapse=TRUE}
mi <- mi %>% 
  mutate(
    date_clean = if_else(
      condition = exp_year > 2019 | exp_year < 2006,
      true = as.Date("1970-01-01"),
      false = exp_date
    ) %>% na_if("1970-01-01")
  )

sum(is.na(mi$exp_date))
sum(is.na(mi$date_clean))
```

Then we'll have to go back and fix the `exp_year`.

```{r fix_year}
mi <- mutate(mi, exp_year = year(date_clean))
```

There is a `doc_stmnt_year` variable, which lists "The calendar year that this statement was
required by the BOE."

```{r stmnt_year_bar, echo=FALSE}
mi %>% 
  mutate(on_year = is_even(doc_stmnt_year)) %>%
  count(on_year, doc_stmnt_year, sort = TRUE) %>%
  mutate(p = n/sum(n)) %>% 
  ggplot(aes(x = doc_stmnt_year, y = p)) +
  geom_col(aes(fill = on_year)) +
  scale_x_continuous(breaks = 2008:2022) +
  scale_y_continuous(labels = percent) +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  labs(
    title = "Michigan Expenditure Counts by Statement Year",
    x     = "Statement Year",
    y     = "Count",
    fill  = "Election Year"
  )
```

Most of the time, these are the same but they can't be equated.

```{r years_same, collapse=TRUE}
mean(mi$doc_stmnt_year == mi$exp_year, na.rm = TRUE)
```

We can also use `date_clean` to explore the intersection of `amount` and time.

```{r month_amount_line}
mi %>% 
  mutate(
    month = month(date_clean),
    on_year = is_even(exp_year)
  ) %>% 
  group_by(month, on_year) %>% 
  summarize(mean = mean(amount)) %>% 
  drop_na() %>% 
  ggplot(aes(month, mean)) +
  geom_line(aes(color = on_year), size = 2) +
  scale_y_continuous(labels = dollar) +
  scale_color_brewer(type = "qual", palette = "Dark2") +
  labs(
    title = "Michigan Expenditure Amount by Month",
    subtitle = "On Election Years and Off",
    x      = "Month",
    y      = "Mean Amount",
    color  = "Election Year"
  )
```

```{r year_amount_bar, echo=FALSE}
mi %>% 
  mutate(on_year = is_even(exp_year)) %>%
  group_by(on_year, exp_year) %>% 
  summarize(mean = mean(amount)) %>%
  ggplot(aes(x = exp_year, y = mean)) +
  geom_col(aes(fill = on_year)) +
  scale_x_continuous(breaks = 2006:2022) +
  scale_y_continuous(labels = dollar) +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  labs(
    title = "Michigan Expenditure Amount by Year",
    x     = "Statement Year",
    y     = "Count",
    fill  = "Election Year"
  )
```

```{r year_amount_box, echo=FALSE}
mi %>% 
  mutate(on_year = is_even(exp_year)) %>%
  filter(!is.na(on_year)) %>% 
  ggplot(aes(x = on_year, y = amount)) +
  geom_boxplot(aes(fill = on_year), varwidth = TRUE, outlier.alpha = 0.01) +
  scale_y_continuous(labels = dollar, trans = "log10") +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  labs(
    title = "Michigan Expenditure Amount by Year",
    x     = "Statement Year",
    y     = "Count",
    fill  = "Election Year"
  )
```

## Wrangle

### Address

```{r normal_address}
mi <- mi %>% 
  mutate(
    address_norm = normal_address(
      address = address,
      abbs = usps_street,
      na_rep = TRUE
    )
  )
```

```{r view_address, echo=FALSE}
mi %>% 
  select(address, address_norm) %>% 
  sample_n(10)
```

### ZIP

```{r zip_pre, collapse=TRUE}
n_distinct(mi$zip)
prop_in(mi$zip, valid_zip, na.rm = TRUE)
sample(mi$zip, 10)
```

```{r}
mi <- mi %>% 
  mutate(
    zip_norm = normal_zip(
      zip = zip,
      na_rep = TRUE
    )
  )
```

```{r zip_post, collapse=TRUE}
n_distinct(mi$zip_norm)
prop_in(mi$zip_norm, valid_zip, na.rm = TRUE)
sum(unique(mi$zip_norm) %out% valid_zip)
```

### State

```{r state_pre, collapse=TRUE}
n_distinct(mi$state)
prop_in(mi$state, valid_state, na.rm = TRUE)
setdiff(mi$state, valid_state)
length(setdiff(mi$state, valid_state))
```

```{r normal_state}
can_prov <- c("ON", "QC", "NS", "NB", "MB", "BC", "PE", "SK", "AB", "NL")

mi <- mi %>% 
  mutate(
    state_norm = normal_state(
      state = state,
      abbreviate = TRUE,
      na_rep = TRUE,
      valid = c(valid_state, can_prov)
    )
  )
```

```{r state_post, collapse=TRUE}
# changes made
sum(mi$state != str_replace_na(mi$state_norm), na.rm = T)
n_distinct(mi$state_norm)
# only NA remains
prop_in(mi$state_norm, valid_state, na.rm = TRUE)
sum(unique(mi$state_norm) %out% valid_state)
```

### City

#### Normalize

```{r city_norm, collapse=TRUE}
n_distinct(mi$city)
prop_in(mi$city, valid_city, na.rm = TRUE)
length(setdiff(mi$city, valid_city))
```

```{r nomal_city}
mi <- mi %>% 
  mutate(
    city_norm = normal_city(
      city = str_replace(city, "\\bTWP\\b", "TOWNSHIP"),
      abbs = usps_city,
      states = c("MI", "DC", "MICHIGAN"),
      na = invalid_city,
      na_rep = TRUE
    )
  )
```

```{r changed_city, echo=FALSE}
mi %>% 
  select(city, city_norm) %>% 
  filter(city != city_norm) %>% 
  sample_frac()
```

```{r city_post_norm, collapse=TRUE}
n_distinct(mi$city_norm)
prop_in(mi$city_norm, valid_city, na.rm = TRUE)
length(setdiff(mi$city_norm, valid_city))
```

#### Swap

```{r swap_city}
mi <- mi %>% 
  rename(city_raw = city) %>% 
  left_join(
    y = zipcodes,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city)

mi$city_raw <- iconv(mi$city_raw, to="UTF-8")
mi$city_match <- iconv(mi$city_raw, to="UTF-8")

mi <- mi %>% 
  mutate(
    match_dist = stringdist(city_raw, city_match),
    city_swap = if_else(
      condition = match_dist == 1,
      true = city_match,
      false = city_norm
    )
  )
```

```{r dist_check, collapse=TRUE}
mean(mi$match_dist == 0, na.rm = TRUE)
sum(mi$match_dist == 1, na.rm = TRUE)
```

```{r city_post_swap, collapse=TRUE}
n_distinct(mi$city_swap)
prop_in(mi$city_swap, valid_city, na.rm = TRUE)
length(setdiff(mi$city_swap, valid_city))
```

#### Refine

The [OpenRefine][or] algorithms can be used to group similar strings and replace
the less common versions with their most common counterpart. This can greatly
reduce inconsistency, but with low confidence; we will only keep any refined
strings that have a valid city/state/zip combination.

[or]: https://openrefine.org/

```{r city_refine}
good_refine <- mi %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>% 
  filter(city_refine != city_swap) %>% 
  inner_join(
    y = zipcodes,
    by = c(
      "city_refine" = "city",
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  )
```

```{r city_count, echo=FALSE}
good_refine %>%
  count(
    state_norm, 
    zip_norm, 
    city_swap, 
    city_refine,
    sort = TRUE
  )
```

Then we can join the refined values back to the database.

```{r city_join}
mi <- mi %>% 
  left_join(good_refine) %>% 
  mutate(city_refine = coalesce(city_refine, city_swap))

mi$city_refine <- mi$city_refine %>% str_replace("ELANSING", "EAST LANSING")
```

#### Check

We can use the `campfin::check_city()` function to pass the remaining unknown
`city_refine` values (and their `state_norm`) to the Google Geocode API. The
function returns the name of the city or locality which most associated with
those values.

This is an easy way to both check for typos and check whether an unknown
`city_refine` value is actually a completely acceptable neighborhood, census
designated place, or some other locality not found in our `valid_city` vector
from our `zipcodes` database.

First, we'll filter out any known valid city and aggregate the remaining records
by their city and state. Then, we will only query those unknown cities which
appear at least ten times.

```{r check_filter}
many_city <- c(valid_city, extra_city)
mi_out <- mi %>% 
  filter(city_refine %out% many_city) %>% 
  count(city_refine, state_norm, sort = TRUE) %>% 
  drop_na() %>% 
  filter(n > 1) %>% 
  head(200)
```

Passing these values to `campfin::check_city()` with `purrr::pmap_dfr()` will
return a single tibble of the rows returned by each city/state combination.

First, we'll check to see if the API query has already been done and a file
exist on disk. If such a file exists, we can read it using `readr::read_csv()`.
If not, the query will be sent and the file will be written using
`readr::write_csv()`.

```{r check_send}
check_file <- here("state", "mi", "expends", "data", "api_check.csv")
if (file_exists(check_file)) {
  check <- read_csv(
    file = check_file
  )
} else {
  check <- pmap_dfr(
    .l = list(
      mi_out$city_refine, 
      mi_out$state_norm
    ), 
    .f = check_city, 
    key = Sys.getenv("GEOCODING_API"), 
    guess = TRUE
  ) %>% 
    mutate(guess = coalesce(guess_city, guess_place)) %>% 
    select(-guess_city, -guess_place)
  write_csv(
    x = check,
    path = check_file
  )
}
```

Any city/state combination with a `check_city_flag` equal to `TRUE` returned a
matching city string from the API, indicating this combination is valid enough
to be ignored.

```{r check_accept}
valid_locality <- check$guess[check$check_city_flag]
```

Then we can perform some simple comparisons between the queried city and the
returned city. If they are extremely similar, we can accept those returned
locality strings and add them to our list of accepted additional localities.

```{r check_compare}
valid_locality <- check %>% 
  filter(!check_city_flag) %>% 
  mutate(
    abb = is_abbrev(original_city, guess),
    dist = str_dist(original_city, guess)
  ) %>%
  filter(abb | dist <= 3) %>% 
  pull(guess) %>% 
  append(valid_locality)
```

#### Progress

Our goal for normalization was to increase the proportion of city values known
to be valid and reduce the total distinct values by correcting misspellings.

```{r city_recount}
many_city <- c(many_city,valid_locality)
mi %>% 
  filter(city_refine %out% many_city) %>% 
  count(city_refine, sort = TRUE)
```

```{r city_add}
many_city <- c(
  many_city,
  "FARMINGTON HILLS",
  "SHELBY TOWNSHIP",
  "MACOMB TOWNSHIP",
  "GROSSE POINTE WOODS",
  "GROSSE POINTE FARMS",
  "GROSSE POINTE PARK"
)
```

```{r city_progress, echo=FALSE}
progress <- progress_table(
  str_to_upper(mi$city_raw),
  mi$city_norm,
  mi$city_swap,
  mi$city_refine,
  compare = many_city
) %>% mutate(stage = as_factor(stage))
kable(progress, digits = 3)
```

You can see how the percentage of valid values increased with each stage.

```{r bar_progress, echo=FALSE}
raw_in <- percent(prop_in(mi$city, valid_city))
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = dark2["purple"]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Michigan City Normalization Progress",
    subtitle = glue("Raw at {raw_in} before conversion to uppercase"),
    x = "Stage",
    y = "Percent Valid"
  )
```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivalent.

```{r bar_distinct, echo=FALSE}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Michigan City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Distinct Values",
    fill = "Valid"
  )
```

## Conclude

```{r conclude, echo=FALSE}
sum_dupe <- nrow(mi) - nrow(distinct(select(mi, -expense_id)))
```

1. There are `r nrow(mi)` records in the database.
1. There are `r nrow(sum_dupe)` duplicate records in the database, ignoring `expense_id`.
1. The range and distribution of `amount` and `date` seem reasonable.
1. There are `r sum(mi$na_flag)` records missing either recipient or date.
1. Consistency in goegraphic data has been improved with `campfin::normal_*()`.
1. The 5-digit `zip_norm` variable has been created with `campfin::normal_zip(mi$zip)`.
1. The 4-digit `exp_year` variable has been created with `lubridate::year(mi$date_clean)`.

## Export

```{r proc_dir}
proc_dir <- here("state","mi", "expends", "data", "processed")
dir_create(proc_dir)
clean_path <- glue("{proc_dir}/mi_expends_clean.csv")
```

```{r write_csv}
mi %>% 
  select(
    -exp_date,
    -address,
    -state,
    -city_raw,
    -zip,
    -city_match,
    -city_norm,
    -match_dist
  ) %>% 
  write_csv(
    path = clean_path,
    na = ""
  )

(clean_size <- file_size(clean_path))
file_encoding(clean_path) %>% 
  mutate(across(path, path.abbrev))
```

## Upload

We can use the `aws.s3::put_object()` to upload the text file to the IRW server.

```{r aws_upload, eval=FALSE}
aws_path <- path("csv", basename(clean_path))
if (!object_exists(aws_path, "publicaccountability")) {
  put_object(
    file = clean_path,
    object = aws_path, 
    bucket = "publicaccountability",
    acl = "public-read",
    show_progress = TRUE,
    multipart = TRUE
  )
}
aws_head <- head_object(aws_path, "publicaccountability")
(aws_size <- as_fs_bytes(attr(aws_head, "content-length")))
unname(aws_size == clean_size)
```
