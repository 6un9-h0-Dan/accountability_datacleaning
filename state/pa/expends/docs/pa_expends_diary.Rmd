---
title: "Pennsylvania Campaign Expenditures Data Diary"
author: "Yanqi Xu"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
Sys.setenv("http_proxy"="")
Sys.setenv("no_proxy"=TRUE)
Sys.setenv("no_proxy"=1)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  readxl, # read excel files
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  knitr, # knit documents
  glue, # combine strings
  scales, #format strings
  here, # relative storage
  fs, # search storage 
  vroom #read deliminated files
)
```

```{r fix_fun, echo=FALSE, collapse = TRUE}
# fix conflict
here <- here::here
# custom utility functions
print_all <- function(df) df %>% print(n = nrow(.)) 
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data
Data is from the [Pennsylvania Election and Campaign Finance System (ECF)][ecf].

[ecf]: https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Pages/default.aspx

The ECF provides a [Full Campaign Finance Export][data]. From this page,
files are organized as annual directories containing files for contributions,
debt, expenditures, filer information, and receipts.

[data]: https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx

The ECF also provides a `readme.txt` file, which we can read to better
understand the data we will be downloading.

In this update, we include all years up to 2022. The next update should start with 2023.
```{r}
pa_host <- "https://www.dos.pa.gov/VotingElections"
pa_dir <- "CandidatesCommittees/CampaignFinance/Resources/Documents"
readme_file <- "readmepriorto2022.txt"
readme_url <- paste(pa_host, pa_dir, readme_file, sep = "/")
```

This text file provides the column names and types for the each of the data
files included in the raw download.

```{r vars_print, echo=FALSE}
readme <- readme_url %>% 
  read_lines(skip = 4) %>% 
  str_c(collapse = "\r\n") %>% 
  str_split("\\W(?=\\w+\r\n-+)") %>% 
  pluck(1) %>%
  map(
    ~read_lines(.) %>% 
      str_trim() %>%
      str_remove("(?<=\\))(.*)$") %>% 
      enframe(name = NULL) %>% 
      separate(
        col = value, 
        into = c("col", "type"), 
        sep = "\t+",
        extra = "merge",
        fill = "right"
      ) %>% 
      #na_if("") %>% 
      drop_na() %>% 
      mutate(col = str_to_lower(col))
  ) %>% 
  set_names(c("contribs", "debt", "expense", "filer", "receipt"))
```

[03]: https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx "source"

### About

Data layout for 2022 is different from previous years and the record layout can be found [here](https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/readme2022.txt).

For data prior to 2022, see https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/readmepriorto2022.txt

## Import

### Download

Download raw, **immutable** data file. Go to https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx. We'll download the files from 2015 to 2019 (file format: zip file) with the script.

```{r raw_dir}
# create a directory for the raw data
raw_dir <- here("state","pa", "expends", "data", "raw")
dir_create(raw_dir)
```

Download all the file packages containing all campaign-finance-related files. 
```{r download to raw_dir, eval = FALSE}
#download the files into the directory
pa_exp_urls <- glue("https://www.dos.pa.gov//VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/{2015:2022}.zip")

if (!all_files_new(raw_dir)) {
  for (url in pa_exp_urls) {
    download.file(
      url = url,
      destfile = glue("{raw_dir}/{basename(url)}")
    )
  }
}
```
### Read
Read individual csv files from the downloaded zip files
```{r read_many, eval = FALSE}

zip_files <- dir_ls(raw_dir, glob = "*.zip")

if (all_files_new(path = raw_dir, glob = "*.txt")) {
  for (i in seq_along(zip_files)) {
    unzip(
      zipfile = zip_files[i],
      #Matches the csv files that starts with expense, and trim the "./ " from directory names
      files = grep("expense.+", unzip(zip_files[i]), value = TRUE) %>% substring(3,),
      exdir = raw_dir
    )
  }
}
```

## Fix
To properly read the file into R, we first have to do some simple string processing to the text file.

```{r fix_dir}
fix_dir <- dir_create(path(dirname(raw_dir), "fix"))
pa_expends_paths <- dir_ls(raw_dir, regexp = "expense.+", recurse = FALSE)

raw_dir %>% dir_info() %>% filter(path %in% pa_expends_paths)

#fix_eval <- length(dir_ls(fix_dir)) != length(expense_files)
```

```{r fix_loop_old, eval=FALSE}
# for old format files
for (f in pa_expends_paths) {
  n <- path(fix_dir, str_c("FIX", basename(f), sep = "_"))
  x <- read_lines(f, skip = 1)
  for (i in rev(seq_along(x))) {
    y <- i - 1
    if (y == 0) {
      next() # skip first
    } else if (str_starts(x[i], "\"\\d+\",") | str_ends(x[y], "\"(Y|N)\"")) {
      next() # skip if good
    } else { # merge if bad
      x[y] <- str_c(x[y], x[i])
      x <- x[-i] # remove bad
    }
  }
  x <- str_remove(x, '(?<=")"(?!,)')
  write_lines(x, n)
  message(basename(n))
}
```

```{r fix_loop_new, eval=FALSE}
# new format files
for (f in pa_expends_paths) {
  n <- path(fix_dir, str_c("FIX", basename(f), sep = "_"))
  x <- read_lines(f, skip = 1)
  for (i in rev(seq_along(x))) {
    if (str_starts(x[i], "\\d+,\\d+,")) {
      next() # skip if good
    } else { # merge if bad
      x[i - 1] <- str_c(x[i - 1], x[i])
      x <- x[-i] # remove bad
    }
  }
  write_lines(x, n)
  message(basename(n))
}
```

```{r fix_info,eval=FALSE}
fix_info <- as_tibble(dir_info(fix_dir))
sum(fix_info$size)
fix_info %>% 
  select(path, size, modification_time) %>% 
  mutate(across(path, basename))
```



Read multiple csvs into R
```{r read multiple files}
exp_22 <- glue(raw_dir, "/expense_2022.txt")
#pa_lines <- list.files(raw_dir, pattern = ".txt", recursive = TRUE) %>% map(read_lines) %>% unlist()

pa_expends_paths <- setdiff(pa_expends_paths, exp_22)
col_names_2022 <- c("FILERID", 
"REPORTID", 
"EYEAR",
"TIMESTAMP", 
"CYCLE", 
"EXPNAME", 
"ADDRESS1", 
"ADDRESS2", 
"CITY", 
"STATE", 
"ZIPCODE", 
"EXPDATE", 
"EXPAMT", 
"EXPDESC")

col_names_earlier <- readme$expense$col
```

There are some double quotes in the files that cause errors when reading in. So we will manually change the double quotes into single quotes. The regex used to process files up to 2022 and since 2022 are very similar, just a little different.

```{r format}
y <- read_lines(exp_22)
#find quotes not at the end that don't contain commas
y <- y %>% str_replace_all('(?<!,)\\"(?!$|,)',"'")

read_pa <- function(file){
  x <- read_lines(file)
  #find quotes not at the beginning and not at the end that don't contain commas
  x <- x %>% str_replace_all('(?<!^|,)\\"(?!$|,)',"'") %>% 
    str_replace_all("&amp;", "&")
  df <- read_delim(I(x),
          delim = ",", escape_double = FALSE,
      escape_backslash = FALSE, col_names = col_names_earlier, 
      col_types = cols(.default = col_character()))
  return(df)
}


pa <- pa_expends_paths %>% 
  map_dfr(read_pa)

pa22 <- I(y) %>% read_delim(",", escape_double = FALSE,
      escape_backslash = FALSE, col_names = col_names_2022,
      name_repair = make_clean_names,
      col_types = cols(.default = col_character()))
```

We can see that the two files' record layouts are similar, with the 2022 file having two more columns `reportid` and `timestamp`. We can just combine them into one. 

```{r bind rows}
pae <- pa %>% bind_rows(pa22)
```

We also need to pull up the processed filer table to join back to the `filerid`, and `eyear` fields. 

Then we can read the fixed filer files to describe the recipients.

```{r fil_check, eval}
fil_paths <- dir_ls(
  path = raw_dir, 
  recurse = TRUE, 
  regexp = "(F|f)iler[\\.|_]"
)

fix_check <- here("state","pa", "expends", "data", "fixed.txt")
```

```{r eval=FALSE}
# do not repeat if done
#if (!file_exists(fix_check)) {
  # for all filer files
  for (f in c(fil_paths)) {
    # read raw file
    read_file(f) %>% 
      # force conversion to simple
      iconv(to = "ASCII", sub = "") %>% 
      # replace non-carriage newline
      str_replace_all("(?<!\r)\n", " ") %>%
      # replace not-field double quotes
      str_replace_all("(?<!^|,|\r\n)\"(?!,|\r\n|$)", "\'") %>% 
      # replace non-delim commas
      str_remove_all(",(?!\"|\\d|\\.\\d+|-(\\d|\\.))") %>% 
      # overwrite raw file
      write_file(f)
    # check progress
    message(paste(basename(f), "done"))
    # clean garbage memory
    flush_memory()
  }
  # note this has done
  file_create(fix_check)
#}
```

```{r fil_read}
fil_path_2022 <- glue(raw_dir, "/filer_2022.txt")
#pa_lines <- list.files(raw_dir, pattern = ".txt", recursive = TRUE) %>% map(read_lines) %>% unlist()
fil_cols_2022 <- c("filerid", "reportid", "eyear", "timestamp", "cycle", "ammend", "terminate", "filertype","filername",
 "office", "district", "party", "address1", "address2", "city", "state",    
 "zipcode", "county", "phone", "beginning","monetary", "inkind")
fil_paths <- setdiff(fil_paths, fil_path_2022)

filers2022 <- read_delim(fil_path_2022,
                          delim = ",",
  escape_backslash = FALSE, 
  escape_double = FALSE,
  col_names = fil_cols_2022,
  col_types = cols(
    .default = col_character()
  ))

filers <- map_df(
  .x = fil_paths,
  .f = read_delim,
  delim = ",",
  escape_backslash = FALSE, 
  escape_double = FALSE,
  col_names = readme$filer$col,
  col_types = cols(
    .default = col_character()
  )
)

filers <- filers2022 %>% bind_rows(filers)
```

We only want to join a single filer to each contribution listed in the data.
We can group by the unique filer ID and a filing year and select only one
copy of the data.

```{r fil_slice}
nrow(filers)
filers <- filers %>% 
  group_by(filerid, eyear) %>% 
  slice(1) %>% 
  ungroup()
nrow(filers)
```

```{r eval=FALSE}
filers %>% write_csv(glue("{raw_dir}/join_filers_2022.csv"))
```


Now the filer information can be added to the contribution data with a 
`dplyr::left_join()` along the unique filer ID and election year.


```{r raw_join}
# 18,386,163
pae <- left_join(
  x = pae,
  y = filers,
  by = c("filerid", "eyear"),
  suffix = c("_exp", "_fil")
)
rm(filers)

pae <- rename_prefix(
  df = pae,
  suffix = c("_exp", "_fil"),
  punct = TRUE
)
```

```{r raw_rename}
pae <- pae %>% 
  rename_with(~str_replace(., "address", "addr")) %>% 
  rename(
    exp_zip = exp_zipcode,
    fil_type = filertype,
    filer = filername,
    fil_zip = fil_zipcode,
    fil_phone = phone
  )
```


```{r include=FALSE}
flush_memory()
```

The text fields contain both lower-case and upper-case letters. The for loop converts them to all upper-case letters unifies the encoding to "UTF-8", replaces the "&amp;", the HTML expression of "An ampersand". These strings are invalid and cannot be converted Converting the encoding may result in some NA values. But there're not too many of them based on counts of NAs before and after the encoding conversion.

```{r fix encoding}
col_stats(pae, count_na)

pae <- pae %>% mutate_all(.funs = iconv, to = "UTF-8") 
# After the encoding, we'll see how many entries have NA fields for each column.
col_stats(pae, count_na)
```

```{r column types}
#All the fields are converted to strings. Convert to date and double.
pae$expdate <- as.Date(pae$expdate, "%Y%m%d")
pae$expamt <- as.double(pae$expamt)
```

## Explore

There are `nrow(pae)` records of `length(pae)` variables in the full database.

```{r glimpse}
head(pae)
tail(pae)
glimpse(pae)
```

### Distinct

The variables range in their degree of distinctness.
Checking the number of distinct values of a discrete variable is another good
way to ensure the file was read properly.

```{r n_distinct}
pae %>% col_stats(n_distinct)
```

### Missing

The variables also vary in their degree of values that are `NA` (missing).

```{r count_na}
pae %>% col_stats(count_na)
```

We will flag any records with missing values in the key variables used to identify an expenditure.
There are `r sum(pae$na_flag)` elements that are flagged as missing at least one value.
```{r na_flag}
pae <- pae %>% flag_na(expname, expdate, expdesc, expamt, exp_city,filer)

percent(mean(pae$na_flag), 0.01)
```

### Duplicates
We can see there're not complete duplicates. 

```{r get_dupes, collapse=TRUE}
pa <- flag_dupes(pa, dplyr::everything())
sum(pae$dupe_flag)
```

### Ranges

#### Amounts

```{r, collapse = TRUE}
summary(pae$expamt)
sum(pae$expamt < 0 , na.rm = TRUE)
```

See how the campaign expenditures were distributed

```{r amount distribution, eval = TRUE}
pae %>% 
  ggplot(aes(x = expamt)) + 
  geom_histogram() +
  scale_x_continuous(
    trans = "log10", labels = dollar)
```

Expenditures out of state
```{r}
sum(pae$state != "PA", na.rm = TRUE)
```

Top spending purposes
```{r eval = TRUE, echo = FALSE}
pae %>%   drop_na(expdesc) %>% 
  group_by(expdesc) %>% 
  summarize(total_spent = sum(expamt)) %>% 
  arrange(desc(total_spent)) %>% 
  head(10) %>% 
  ggplot(aes(x = expdesc, y = total_spent)) +
  geom_col() +
  labs(title = "Pennsylvania Campaign Expenditures by Total Spending",
       caption = "Source: Pennsylvania Dept. of State") +
  scale_y_continuous(labels = scales::dollar) +
  coord_flip() +
  theme_minimal()

```

### Dates
Some of the dates are too far back and some are past the current dates. 
```{r}
summary(pae$expdate)
```

### Year

Add a `year` variable from `date` after `col_date()` using `lubridate::year()`.

```{r add_year}
pae <- pae %>% mutate(year = year(expdate), on_year = is_even(year))
```

```{r year_count_bar, eval = TRUE, echo=FALSE}
pae %>% 
  filter( 2014 < year & year < 2024) %>% 
  count(on_year, year) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(aes(fill=on_year)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  labs(
    title = "Pennsylvania Expenditure Counts per Year",
    caption = "Source: Pennsylvania Dept. of State",
    x = "Year",
    y = "Count"
  )
  
```

```{r amount_year_bar, eval = TRUE, echo=FALSE}
pae %>% 
  filter( 2014 < year & year < 2024) %>% 
  group_by(year, on_year) %>% 
  summarize(mean = mean(expamt)) %>% 
  ggplot(aes(x = year, y = mean)) +
  geom_col(aes(fill = on_year)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Pennsylvania Expenditure Mean Amount per Year",
    caption = "Source: Pennsylvania Dept. of State",
    x = "Year",
    y = "Amount"
  ) 
```

```{r amount_month_line, echo = FALSE}
pae %>% 
  mutate(month = month(expdate)) %>% 
  drop_na(month) %>% 
  group_by(on_year, month) %>% 
  summarize(mean = mean(expamt)) %>% 
  ggplot(aes(month, mean)) +
  geom_line(aes(color = on_year), size = 2) +
  scale_y_continuous(labels = dollar) +
  scale_x_continuous(labels = month.abb, breaks = 1:12) +
  scale_color_brewer(
    type = "qual",
    palette = "Dark2"
  ) +
  labs(
    title = "Pennsylvania Expenditure Amount by Month",
    caption = "Source: Pennsylvania Dept. of State",
    color = "Election Year",
    x = "Month",
    y = "Amount"
  )
```

```{r}
pae %>% group_by(expdesc) %>% summarize(total = sum(expamt)) %>% arrange(desc(total))
```

## Wrangle
To improve the searchability of the database, we will perform some consistent,
confident string normalization. For geographic variables like city names and
ZIP codes, the corresponding `campfin::normal_*()` functions are tailor made to 
facilitate this process.

### Address

For the street `*_addresss` variables, the `campfin::normal_address()` function
will force consistence case, remove punctuation, and abbreviate official 
USPS suffixes.

```{r con_addr_norm}
norm_addr <- pae %>%
  count(exp_addr1, exp_addr2, sort = TRUE) %>% 
  select(-n) %>% 
  unite(
    col = exp_addr_full,
    starts_with("exp_addr"),
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(
    exp_addr_norm = normal_address(
      address = exp_addr_full,
      abbs = usps_street,
      na_rep = TRUE
    )
  ) %>% 
  select(-exp_addr_full)
```

```{r addr_join}
norm_addr
pae <- left_join(pae, norm_addr)
rm(norm_addr); flush_memory(1)
```

We will repeat the process for filer addresses.

```{r fil_addr_norm}
norm_addr <- pae %>% 
  count(fil_addr1, fil_addr2, sort = TRUE) %>% 
  select(-n) %>% 
  unite(
    col = fil_addr_full,
    starts_with("fil_addr"),
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(
    fil_addr_norm = normal_address(
      address = fil_addr_full,
      abbs = usps_street,
      na_rep = TRUE
    )
  ) %>% 
  select(-fil_addr_full)
```

```{r fil_addr_join}
norm_addr
pae <- left_join(pae, norm_addr)
rm(norm_addr); flush_memory(1)
```

### ZIP

```{r collapse = TRUE}
table(nchar(pae$fil_zip))
table(nchar(pae$exp_zip))
```

For ZIP codes, the `campfin::normal_zip()` function will attempt to create
valid _five_ digit codes by removing the ZIP+4 suffix and returning leading
zeroes dropped by other programs like Microsoft Excel.

```{r zip_norm}
pae <- mutate_at(
  .tbl = pae,
  .vars = vars(ends_with("zip")),
  .funs = list(norm = normal_zip),
  na_rep = TRUE
)
```

```{r zip_progress}
progress_table(
  pae$exp_zip,
  pae$exp_zip_norm,
  pae$fil_zip,
  pae$fil_zip_norm,
  compare = valid_zip
)
```

### State

There is no need to clean the two state variables.

```{r state_check}
prop_in(pae$exp_state, valid_state)
prop_in(pae$fil_state, valid_state)
```

### City

Cleaning city values is the most complicated. This process involves four steps:

1. Prepare raw city values by removing invalid data and reducing inconsistencies
1. Match prepared city values with the _actual_ city name of that record's ZIP code
1. swap prepared city values with the ZIP code match _if_ only 1 edit is needed
1. Refine swapped city values with key collision and n-gram fingerprints

#### Normal

`r sum(!is.na(pae$city))` distinct cities were in the original dataset in column 
```{r city_norm_con}
exp_norm_city <- pae %>% 
  count(exp_city, exp_state, exp_zip_norm, sort = TRUE) %>% 
  select(-n) %>% 
  mutate(
    across(
      .cols = exp_city, 
      .fns = list(norm = normal_city), 
      abbs = usps_city,
      states = c("PA", "DC", "PENNSYLVANIA"),
      na = invalid_city,
      na_rep = TRUE
    )
  )
```

```{r city_norm}
fil_norm_city <- pae %>% 
  count(fil_city, fil_state, fil_zip_norm, sort = TRUE) %>% 
  select(-n) %>% 
  mutate(
    across(
      .cols = fil_city, 
      .fns = list(norm = normal_city), 
      abbs = usps_city,
      states = c("PA", "DC", "PENNSYLVANIA"),
      na = invalid_city,
      na_rep = TRUE
    )
  )
```

#### Swap

We can further improve normalization by comparing our normalized value
against the _expected_ value for that record's state abbreviation and ZIP code.
If the normalized value is either an abbreviation for or very similar to the
expected value, we can confidently swap those two.

```{r con_city_swap}
exp_norm_city <- exp_norm_city %>% 
  left_join(
    y = zipcodes,
    by = c(
      "exp_state" = "state",
      "exp_zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(exp_city_norm, city_match),
    match_dist = str_dist(exp_city_norm, city_match),
    exp_city_swap = if_else(
      condition = !is.na(match_dist) & (match_abb | match_dist == 1),
      true = city_match,
      false = exp_city_norm
    )
  ) %>% 
  select(
    -city_match,
    -match_dist,
    -match_abb
  )
```

```{r fil_city_swap}
fil_norm_city <- fil_norm_city %>% 
  left_join(
    y = zipcodes,
    by = c(
      "fil_state" = "state",
      "fil_zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(fil_city_norm, city_match),
    match_dist = str_dist(fil_city_norm, city_match),
    fil_city_swap = if_else(
      condition = !is.na(match_dist) & (match_abb | match_dist == 1),
      true = city_match,
      false = fil_city_norm
    )
  ) %>% 
  select(
    -city_match,
    -match_dist,
    -match_abb
  )
```

```{r include=FALSE}
flush_memory()
```
#### Refine

The [OpenRefine][or] algorithms can be used to group similar strings and replace
the less common versions with their most common counterpart. This can greatly
reduce inconsistency, but with low confidence; we will only keep any refined
strings that have a valid city/state/zip combination.

[or]: https://openrefine.org/

```{r city_refine}
good_refine <- exp_norm_city %>% 
  mutate(
    exp_city_refine = exp_city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>% 
  filter(exp_city_refine != exp_city_swap) %>% 
  inner_join(
    y = zipcodes,
    by = c(
      "exp_city_refine" = "city",
      "exp_state" = "state",
      "exp_zip_norm" = "zip"
    )
  )
```

```{r city_count, echo=FALSE}
good_refine %>%
  count(
    exp_state, 
    exp_zip_norm, 
    exp_city_swap, 
    exp_city_refine,
    sort = TRUE
  )
```

Then we can join the refined values back to the database.

```{r city_join}
exp_norm_city <- exp_norm_city %>% 
  left_join(good_refine, by = names(.)) %>% 
  mutate(exp_city_refine = coalesce(exp_city_refine, exp_city_swap))
```

```{r include=FALSE}
flush_memory()
```

Manually change the city_refine fields due to overcorrection.


```{r revert overcorrected refine}
exp_norm_city <- exp_norm_city %>% 
  mutate(
    exp_city_refine = exp_city_refine %>% 
  na_if("ILLEGIBLE") %>% 
  str_replace("^PHILA$", "PHILADELPHIA") %>% 
  str_replace("^PGH$", "PITTSBURGH") %>% 
  str_replace("^NEW YORK CITY$", "NEW YORK") %>% 
  str_replace("^H\\sBURG$", "HARRISBURG") %>% 
  str_replace("^HBG$", "HARRISBURG") %>% 
  str_replace("^NYC$", "NEW YORK") %>% 
  str_replace("^DU BOIS$", "DUBOIS") %>% 
  str_replace("^PIT$", "PITTSBURGH") %>% 
  str_replace("^MCCBG$", "MCCONNELLSBURG") %>% 
  str_replace("^PLUM BORO$", "PLUM") %>% 
  str_replace("^GREENVILLE$", "EAST GREENVILLE") %>% 
  str_replace("^NON$", "ONO") %>% 
  str_replace("^FORD CLIFF$", "CLIFFORD") %>% 
  str_replace("^W\\sB$", "WILKES BARRE")) 
```
  

#### Check

We can use the `campfin::check_city()` function to pass the remaining unknown
`city_refine` values (and their `state_norm`) to the Google Geocode API. The
function returns the name of the city or locality which most associated with
those values.

This is an easy way to both check for typos and check whether an unknown
`city_refine` value is actually a completely acceptable neighborhood, census
designated place, or some other locality not found in our `valid_city` vector
from our `zipcodes` database.

First, we'll filter out any known valid city and aggregate the remaining records
by their city and state. Then, we will only query those unknown cities which
appear at least ten times.

```{r check_filter}
pae_out <- exp_norm_city %>% 
  filter(exp_city_refine %out% c(valid_city, extra_city)) %>% 
  count(exp_city_refine, exp_state, sort = TRUE) %>% 
  drop_na() %>% 
  head(1000)
```

Passing these values to `campfin::check_city()` with `purrr::pmap_dfr()` will
return a single tibble of the rows returned by each city/state combination.

First, we'll check to see if the API query has already been done and a file
exist on disk. If such a file exists, we can read it using `readr::read_csv()`.
If not, the query will be sent and the file will be written using
`readr::write_csv()`.

```{r check_send}
check_file <- here("state","pa", "expends", "data", "api_check.csv")
if (file_exists(check_file)) {
  check <- read_csv(
    file = check_file,
    col_types = cols(
      .default = col_character(),
      check_city_flag = col_logical()
    )
  )
} else {
  check <- pmap_dfr(
    .l = list(
      pae_out$exp_city_refine, 
      pae_out$exp_state
    ), 
    .f = check_city, 
    key = Sys.getenv("GEOCODE_KEY"), 
    guess = TRUE
  ) %>% 
    mutate(guess = coalesce(guess_city, guess_place)) %>% 
    select(-guess_city, -guess_place)
  write_csv(
    x = check,
    path = check_file
  )
}
```

Any city/state combination with a `check_city_flag` equal to `TRUE` returned a
matching city string from the API, indicating this combination is valid enough
to be ignored.

```{r check_accept}
valid_locality <- check$guess[check$check_city_flag]
```

Then we can perform some simple comparisons between the queried city and the
returned city. If they are extremely similar, we can accept those returned
locality strings and add them to our list of accepted additional localities.

```{r check_compare}
valid_locality <- check %>% 
  filter(!check_city_flag) %>% 
  mutate(
    abb = is_abbrev(original_city, guess),
    dist = str_dist(original_city, guess)
  ) %>%
  filter(abb | dist <= 3) %>% 
  pull(guess) %>% 
  c(valid_locality)
```

```{r}
valid_locality <- c(valid_locality, "ABBOTT PARK", "RESEARCH TRIANGLE PARK")
```

```{r manual lookup}
# Manually change overcorrected city names to original 
exp_norm_city$exp_city_refine <- exp_norm_city$exp_city_refine %>% 
  str_replace("^FEASTERVILLE\\sTREVOSE", "FEASTERVILLE") %>% 
  str_replace("LEES SUMMIT", "LAKE LOTAWANA") %>% 
  str_replace("HAZLETON", "HAZLE TOWNSHIP") %>% 
  str_replace("DANIA", "DANIA BEACH") %>% 
  str_replace("CRANBERRY TWP", "CRANBERRY TOWNSHIP")

exp_norm_city[which(exp_norm_city$exp_city == "HOLLIDASBURG"), "exp_city_refine"] <- "HOLLIDAYSBURG"
exp_norm_city[which(exp_norm_city$exp_city == "PENN HELLE"), "exp_city_refine"] <- "PENN HILLS"
exp_norm_city[which(exp_norm_city$exp_city == "PHUM"), "exp_city_refine"] <- "PLUM"
exp_norm_city[which(exp_norm_city$exp_city == "CLARKSGREEN"), "exp_city_refine"] <- "CLARKS GREEN"
exp_norm_city[which(exp_norm_city$exp_city == "SANFRANCISCO"), "exp_city_refine"] <- "SAN FRANCISCO"
exp_norm_city[which(exp_norm_city$exp_city == "RIEFFTON"), "exp_city_refine"] <- "REIFFTON"
exp_norm_city[which(exp_norm_city$exp_city == "SHOREVILLE"), "exp_city_refine"] <- "SHOREVIEW"
exp_norm_city[which(exp_norm_city$exp_city == "PITTSBURGH PLUM"), "exp_city_refine"] <- "PLUM"
exp_norm_city[which(exp_norm_city$exp_city == "MOUNTVIEW"), "exp_city_refine"] <- "MOUNT VIEW"
exp_norm_city[which(exp_norm_city$exp_city == "PLUM BORO"), "exp_city_refine"] <- "PLUM"
exp_norm_city[which(exp_norm_city$exp_city == "HAZELTON CITY"), "exp_city_refine"] <- "HAZLE TOWNSHIP"
exp_norm_city[which(exp_norm_city$exp_city == "BARNSVILLE"), "exp_city_refine"] <- "BARNESVILLE"
```

```{r city_rejoin}
pae <- left_join(pae, exp_norm_city)
pae <- left_join(pae, fil_norm_city)
```


```{r}
many_city <- c(valid_city, extra_city, valid_locality)

pae_out <- exp_norm_city %>% 
  filter(exp_city_refine %out% many_city) %>% 
  count(exp_city,exp_city_refine, exp_state, sort = TRUE) %>% 
  drop_na()

pa_city_lookup <- read_csv(file = here("state","pa", "expends", "data", "raw", "pa_city_lookup.csv"), col_names = c("city", "city_lookup", "changed", "count"), skip = 1)

pae_out <- pae_out %>% left_join(pa_city_lookup, by = c("exp_city_refine" = "city")) %>% filter(exp_city_refine != city_lookup | is.na(city_lookup)) %>% drop_na(exp_city_refine) %>% select(-c(n,changed, count))

pae <- pae %>% left_join(pae_out, by = c("exp_city", "exp_state","exp_city_refine")) %>% mutate(exp_city_clean = ifelse(
  is.na(city_lookup) & exp_city_refine %out% valid_city,
  NA,
  coalesce(city_lookup, exp_city_refine)))
```

```{r city_progress, echo=FALSE}
progress <- progress_table(
  str_to_upper(pae$exp_city),
  pae$exp_city_norm,
  pae$exp_city_swap,
  pae$exp_city_refine,
  pae$exp_city_clean,
  compare = many_city
) %>% mutate(stage = as_factor(stage))
kable(progress, digits = 3)
```


```{r bar_progress, echo=FALSE}
raw_in <- percent(prop_in(pae$exp_city, valid_city))
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = dark2["purple"]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Pennsylvania City Normalization Progress",
    subtitle = glue("Raw at {raw_in} before conversion to uppercase"),
    x = "Stage",
    y = "Percent Valid"
  )
```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivalent.

```{r bar_distinct, echo=FALSE}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Pennsylvania City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Distinct Values",
    fill = "Valid"
  )
```

```{r include=FALSE}
flush_memory()
```

## Conclude

1. There are `r nrow(pae)` records in the database
1. There are `r sum(pae$dupe_flag)` records with suspected duplicate filerID, recipient, date, _and_ amount
(flagged with `dupe_flag`)
1. The ranges for dates and amounts are reasonable
1. Consistency has been improved with `stringr` package and custom `normal_*()` functions.
1. The five-digit `zip_clean` variable has been created with `zipcode::clean.zipcode()`
1. The `year` variable has been created with `lubridate::year()`
1. There are `r count_na(pae$city_raw)` records with missing `city` values and `r count_na(pae$expamt)` records with missing `payee` values (both flagged with the `na_flag`).

## Export

```{r write_clean}
clean_dir <- here("state","pa", "expends", "data", "processed")
dir_create(clean_dir)
pae %>% 
  select(
    -exp_city_norm,
    -on_year,
    -exp_city_swap,
    -exp_city_refine,
    -city_lookup,
    -fil_city_norm
  ) %>% 
    rename (exp_zip_clean = exp_zip_norm,
          fil_zip_clean = fil_zip_norm,
          fil_city_clean =fil_city_swap) %>% 
  write_csv(
    path = glue("{clean_dir}/pa_expends_clean.csv"),
    na = ""
  )
```

