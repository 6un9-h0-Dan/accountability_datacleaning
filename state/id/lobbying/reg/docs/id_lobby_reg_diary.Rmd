---
title: "Idaho Lobbyists"
author: "Kiernan Nicholls & Yanqi Xu"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
set.seed(5)
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
fs::dir_create(here::here("id", "lobbying", "reg", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  scales, # format strings
  readxl, # read excel files
  knitr, # knit documents
  glue, # combine strings
  gluedown, #format markdown
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

> All registered lobbyists must file financial reports showing the totals of all expenditures made
by the lobbyist or incurred on behalf of such lobbyistâ€˜s employer (not including payments made
directly to the lobbyist) during the period covered by the report, the totals are segregated
according to financial category i.e., entertainment, food and refreshment, advertising, living
accommodations, travel, telephone, office expenses, and other expenses or services.

### Download
The lobbying registration data can be downloaded from the [Idaho Secretary of State's website](https://elections.sos.idaho.gov/TED/PublicPortal/Launchpad.aspx). Registartion data for 2019 onward can be accessed through direct download. The website has two views by lobbyist and by employer, but they are essentially powered by the same data backend. Data prior to 2018 can also be downloaded in a separate tab.

This data is downloaded on July 4, 2023 to include registration data for 2019 through 2022.


### Read

```{r read_raw}
raw_dir <- here("state","id", "lobbying", "reg", "data", "raw")

idlr <- map_dfr(
  .x = dir_ls(raw_dir),
  .f = read_excel,
  col_types = "guess"
)

idlr <- idlr %>% clean_names()
```

## Explore

```{r glimpse}
head(idlr)
tail(idlr)
glimpse(idlr)
```

### Year
We will create a `year` field out of the `date_filed` field.
```{r}
idlr <- idlr %>% mutate(year = year(date_filed))

min(idlr$year)
max(idlr$year)
```


### Distinct

```{r n_distinct}
col_stats(idlr, n_distinct)
```

### Missing

There are no missing fields. 
```{r glimpse_na}
col_stats(idlr, count_na)
```

### Duplicates

There are no duplicate records in the database.

```{r flag_dupes}
idlr <- flag_dupes(idlr, everything())
sum(idlr$dupe_flag)
mean(idlr$dupe_flag)
```

## Wrangle

For each of three entity types (lobbyists, client, and filer/account owner), they all have corresponding`*_address*` variables, with city, state and zipcode all in the same fields. We first need to separate those fields. 

### Addresses

We will begin with address normalization. First, we can use `tidyr::unite()` to combine each 
separate variable into a single string for each registrant.

```{r results='asis'}
md_bullet(head(idlr$permanent_address))
```
Because there's only a whitespace or two separating the the address and state, this is our best attempt to extract the city field. 
```{r wrangle address}
idlr <- idlr %>% 
  mutate(across(.cols = ends_with("address"),
                .fns = str_to_upper)) %>% 
  mutate(
         zip = str_extract(permanent_address, "(?<=, )\\d{5}$"),
         state = str_extract(permanent_address, "[:alpha:]+(?=,\\s+\\d{5}(?:-\\d{4})?$)"),
         #city_sep is everything at tne end of but before the double white space
         city_sep = str_extract(permanent_address %>% str_remove(paste0(state,", ",zip)) %>% 
                                  str_remove("\\s+$"), "(?<=\\s{2}).+$"),
         #city_alt is everything at tne end of but before the single white space
         city_alt = str_extract(permanent_address %>% str_remove(paste0(state,", ",zip)) %>% 
                                  str_remove("\\s+$"), "(?<=\\s{1})[^\\s]+$"))

idlr <- idlr %>% 
  mutate(city = case_when(nchar(city_sep) > nchar(city_alt) & !str_detect(city_sep,"STE|SUITE|FLOOR|DRIVE|\\d+")  ~ city_sep,
                          TRUE ~ city_alt) %>% str_to_upper() %>% str_squish()  %>% str_remove("STE|SUITE\\s[^\\s]+\\s"),
         # If city_sep is longer than city_alt, use city_sep, unless it contains words like Suite/Floor or contains numbers
                                  city =case_when(
                                    #city == "FALLS" ~ "TWIN FALLS",
                                  city == "D'ALENE" ~ "COEUR D'ALENE", 
                                  city_sep == "6TH ST  BAKERS CITY" ~ "BAKERS CITY",
                                  city == "VEGAS" ~ "LAS VEGAS",
                                  city == "SEGUNDO" ~ "EL SEGUNDO",
                                  str_detect(permanent_address, "MENDOTA HEIGHTS") ~ "MENDOTA HEIGHTS",
                                  str_detect(permanent_address, "SALT\\s+LAKE\\s+CITY") ~ "SALT LAKE CITY",
                                  str_detect(permanent_address, "[^\\s]+\\s+FALLS") ~ str_extract(permanent_address, "[^\\s]+\\s+FALLS"),
                                  TRUE ~ city)) %>% 
  select(-c(city_sep, city_alt))

idlr <- idlr %>% 
  mutate(permanenet_address_sep = permanent_address %>% str_remove(paste0(state,", ",zip)) %>% str_remove(paste0(city,"\\s")) %>% str_squish())
```

```{r wrangle emp_address}
idlr <- idlr %>% 
  mutate(emp_zip = str_extract(employer_address, "(?<=, )\\d{5}$"),
         emp_state = str_extract(employer_address, "[:alpha:]+(?=,\\s+\\d{5}(?:-\\d{4})?$)"),
         #city_sep is everything at tne end of but before the double white space
         city_sep = str_extract(employer_address %>% str_remove(paste0(emp_state,", ",emp_zip)) %>% 
                                  str_remove("\\s+$"), "(?<=\\s{2}).+$"),
         #city_alt is everything at tne end of but before the single white space
         city_alt = str_extract(employer_address %>% str_remove(paste0(emp_state,", ",emp_zip)) %>% 
                                  str_remove("\\s+$"), "(?<=\\s{1})[^\\s]+$"))

idlr <- idlr %>% 
  mutate(emp_city = case_when(nchar(city_sep) > nchar(city_alt) & !str_detect(city_sep,"\\sSte|Suite|Floor|\\d+")  ~ city_sep,
                              city_sep == city_alt ~ city_alt,
                          TRUE ~ city_alt) %>% str_squish() %>% str_remove(".+DRIVE\\s") %>% str_remove("\\s+STE|SUITE\\s[^\\s]+\\s"),
         # If city_sep is longer than city_alt, use city_sep, unless it contains words like Suite/Floor or contains numbers
                                  emp_city =case_when(
                                    #emp_city == "FALLS" ~ "TWIN FALLS",
                                    emp_city == "D'ALENE" ~ "COEUR D'ALENE", 
                                    emp_city == "VEGAS" ~ "LAS VEGAS",
                                  emp_city == "SEGUNDO" ~ "EL SEGUNDO",
                                    str_detect(employer_address, "MENDOTA HEIGHTS") ~ "MENDOTA HEIGHTS",
                                    str_detect(employer_address, "SALT\\s+LAKE\\s+CITY") ~ "SALT LAKE CITY",
                                    str_detect(employer_address, "FOSTER\\s+CITY") ~ "FOSTER CITY",
                                    str_detect(employer_address, "[^\\s]+\\s+FALLS") ~ str_extract(employer_address, "[^\\s]+\\s+FALLS"),
                                  TRUE ~ emp_city)) %>% select(-c(city_sep,city_alt))


idlr <- idlr %>% 
  mutate(employer_address_sep = employer_address %>% str_remove(paste0(emp_state,", ",emp_zip)) %>% str_remove(paste0(emp_city,"\\s")) %>% str_squish())
```
Next, we will normalize both addresses. For the street `addresss` variables, the `campfin::normal_address()`
function will force consistence case, remove punctuation, and
abbreviation official USPS suffixes.
```{r}
idlr <- idlr %>% 
mutate_at(.vars = vars(ends_with('address_sep')), .funs = list(norm = ~ normal_address(.,abbs = usps_street,
      na_rep = TRUE)))
```

### ZIP
We can see that the zip fields are clean and don't need to be cleaned. 
```{r zip_progress, collapse=TRUE}
progress_table(
  idlr$zip,
  idlr$emp_zip,
  compare = valid_zip
)
```

### State
The state fields are also clean. 
```{r state_progress, collapse=TRUE}
progress_table(
  idlr$state,
  idlr$emp_state,
  compare = valid_state
)
```

### City 
```{r}
many_city <- c(valid_city, extra_city,"COEUR D'ALENE")
```

```{r city_normal}
idlr <- idlr %>% 
  mutate_at(
    .vars = vars(ends_with("city")),
    .funs = list(norm = normal_city),
      abbs = usps_city,
      states = c("ID", "DC", "IDAHO"),
      na = invalid_city,
      na_rep = TRUE
    )
```


```{r city_swap}
idlr <- idlr %>%
  rename(city_raw = city) %>% 
  left_join(
    y = zipcodes,
    by = c(
      "state" = "state",
      "zip" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(city_norm, city_match),
    match_exp = is_abbrev(city_match, city_norm),
    match_dist = str_dist(city_norm, city_match),
    city_swap = if_else(
      condition = match_abb | match_exp | match_dist <= 2 | state == city_norm,
      true = city_match,
      false = city_norm
    )
  ) %>% 
  select(
    -city_match,
    -match_dist,
    -match_abb
  )

idlr <- idlr %>%
  rename(emp_city_raw = emp_city) %>% 
  left_join(
    y = zipcodes,
    by = c(
      "emp_state" = "state",
      "emp_zip" = "zip"
    )
  ) %>% 
  rename(emp_city_match = city) %>% 
  mutate(
    match_exp = is_abbrev(emp_city_norm, emp_city_match),
    match_abb = is_abbrev(emp_city_match, emp_city_norm),
    match_dist = str_dist(emp_city_norm, emp_city_match),
    emp_city_swap = if_else(
      condition = match_abb | match_exp | match_dist <= 2 | state == emp_city_norm,
      true = emp_city_match,
      false = emp_city_norm
    )
  ) %>% 
  select(
    -emp_city_match,
    -match_dist,
    -match_abb
  )
```

```{r city_count_out}
idlr %>% 
  filter(city_swap %out% many_city) %>% 
  count(state, city_norm, city_swap, sort = TRUE)

idlr %>% 
  filter(emp_city_swap %out% many_city) %>% 
  count(emp_state, emp_city_norm, emp_city_swap, sort = TRUE)
```

#### Refine

The [OpenRefine] algorithms can be used to group similar strings and replace the
less common versions with their most common counterpart. This can greatly 
reduce inconsistency, but with low confidence; we will only keep any refined
strings that have a valid city/state/zip combination.

[or]: https://openrefine.org/
  
```{r city_refine}
good_refine <- idlr %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>% 
  filter(city_refine != city_swap) %>% 
  inner_join(
    y = zipcodes,
    by = c(
      "city_refine" = "city",
      "state" = "state",
      "zip" = "zip"
    )
  )
```

```{r city_count, echo=FALSE}
good_refine %>%
  count(
    state, 
    zip, 
    city_swap, 
    city_refine,
    sort = TRUE
  )
```

Then we can join the refined values back to the database.

```{r city_join}
idlr <- idlr %>% 
  left_join(good_refine) %>% 
  mutate(city_refine = coalesce(city_refine, city_swap,city_raw))
```

```{r}
idlr <- idlr %>% mutate(city_refine = case_when(
  city_raw == "COEUR D'ALENE" ~ "COEUR D'ALENE",
  TRUE ~ city_refine
))
```

We can repeat the process for employer cities. 
```{r emp_city_refine}
good_refine <- idlr %>% 
  mutate(
    emp_city_refine = emp_city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>% 
  filter(emp_city_refine != emp_city_swap) %>% 
  inner_join(
    y = zipcodes,
    by = c(
      "emp_city_refine" = "city",
      "emp_state" = "state",
      "emp_zip" = "zip"
    )
  )
```

```{r emp_city_join}
idlr <- idlr %>% 
  left_join(good_refine) %>% 
  mutate(emp_city_refine = coalesce(emp_city_refine, emp_city_swap, emp_city_raw))
```

```{r}
idlr <- idlr %>% mutate(emp_city_refine = case_when(
  emp_city_raw == "COEUR D'ALENE" ~ "COEUR D'ALENE",
  TRUE ~ emp_city_refine
))
```

### Progress

```{r}
progress <- progress_table(
  idlr$city_raw,
  idlr$city_norm,
  idlr$city_swap,
  idlr$city_refine,
  idlr$emp_city_raw,
  idlr$emp_city_norm,
  idlr$emp_city_swap,
  idlr$emp_city_refine,
  compare = many_city
) %>% mutate(stage = as_factor(stage))
kable(progress, digits = 3)
```

You can see how the percentage of valid values increased with each stage.

```{r bar_progress, echo=FALSE}
raw_in <- percent(prop_in(idlr$city_raw, valid_city))
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = dark2["purple"]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Montana City Normalization Progress",
    subtitle = glue("Raw at {raw_in} before conversion to uppercase"),
    x = "Stage",
    y = "Percent Valid"
  )
```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivalent.

```{r bar_distinct, echo=FALSE}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Idaho City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Distinct Values",
    fill = "Valid"
  )
```

## Conclude

1. There are `r comma(nrow(idlr))` records in the database.
1. There are no duplicate records in the database.
1. There are no records missing any pertinent information.
1. Consistency in goegraphic data has been improved with `campfin::normal_*()`.
1. The 5-digit `zip_norm` variable has been created with `campfin::normal_zip()`.

## Export

```{r}
idlr <- idlr %>% 
  select(
    -city_norm,
    -emp_city_norm,
    -city_swap,
    -emp_city_swap
  ) %>% 
  rename(
    city_clean = city_refine,
    emp_city_clean = emp_city_refine
  ) %>% 
  rename_all(~str_replace(., "_norm", "_clean"))
```


```{r create_proc_dir}
proc_dir <- here("state","id", "lobbying", "reg", "data", "processed")
dir_create(proc_dir)
```

```{r write_clean}
idlr %>% 
  write_csv(
    path = glue("{proc_dir}/id_lobby_reg_2019-2022.csv"),
    na = ""
  )
```



